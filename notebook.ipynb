{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pennylane as qml \n",
    "import torch\n",
    "\n",
    "def create_circuit(n_qubits,n_layers=None,circ = \"simplified_two_design\",fim=False, shots=None):\n",
    "\n",
    "    dev = qml.device(\"default.qubit.torch\", wires=n_qubits, shots=shots)\n",
    "\n",
    "    def RZRY(params):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        #qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        #qml.AngleEmbedding(params,wires=range(n_qubits))\n",
    "        for q in range(n_qubits):\n",
    "            qml.Hadamard(wires=q)\n",
    "\n",
    "        for w in range(n_layers): \n",
    "            for q in range(n_qubits):\n",
    "                index = w * (2*n_qubits) + q * 2\n",
    "                qml.RZ(params[index],wires=q)\n",
    "                qml.RY(params[index + 1],wires=q)\n",
    "        \n",
    "        qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        return qml.probs(wires=range(n_qubits))\n",
    "\n",
    "    def S2D(init_params,params,measurement_qubits=0,prod_approx=False):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        \n",
    "        #qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        if not prod_approx:\n",
    "            return qml.probs(wires=list(range(measurement_qubits)))\n",
    "        else:\n",
    "            return [qml.probs(i) for i in range(measurement_qubits)]\n",
    "\n",
    "    def SU(params):\n",
    "        qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        \n",
    "        ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "        for i in range(2,n_qubits):\n",
    "            ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "        return qml.expval(ZZ)\n",
    "    \n",
    "    def simmpleRZRY(params,cnots=True):\n",
    "        qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RZ, wires=range(n_qubits), pattern=\"single\", parameters=params[0])\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params[1])\n",
    "        if cnots:\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "            return qml.expval(qml.PauliZ(n_qubits-1))\n",
    "        else:\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "            return qml.expval(ZZ)\n",
    "        \n",
    "    def RY(params,y=True,probs=False,prod=False, entanglement=None):\n",
    "        #qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params)\n",
    "        #qml.broadcast(qml.CZ, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "\n",
    "        if entanglement==\"all_to_all\":\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        if y==True:\n",
    "            #YY = qml.operation.Tensor(qml.PauliY(0), qml.PauliY(1))\n",
    "            YY = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                #YY = qml.operation.Tensor(YY, qml.PauliY(i))\n",
    "                YY.append(qml.PauliZ(i))\n",
    "            \n",
    "            #return [qml.expval(i) for i in YY]\n",
    "            return qml.expval(YY)\n",
    "\n",
    "        elif probs==False:\n",
    "\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            #ZZ = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))        \n",
    "                #ZZ.append(qml.PauliZ(i))        \n",
    "\n",
    "            #return [qml.expval(i) for i in ZZ]\n",
    "            return qml.expval(ZZ)\n",
    "\n",
    "        else:\n",
    "            if prod:\n",
    "                return [qml.probs(i) for i in range(n_qubits)]\n",
    "            else:\n",
    "                return qml.probs(wires=range(n_qubits))\n",
    "            \n",
    "        \n",
    "        \n",
    "    def GHZ(params,measurement_qubits=0):\n",
    "        qml.RY(params,wires=0)\n",
    "        qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "\n",
    "    def random_product_state(params,gate_sequence=None):\n",
    "                \n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(np.pi / 4, wires=i)\n",
    "\n",
    "        for ll in range(len(params)):\n",
    "\n",
    "            for i in range(n_qubits):\n",
    "                gate_sequence[\"{}{}\".format(ll,i)](params[ll][i], wires=i)\n",
    "\n",
    "            #for i in range(n_qubits - 1):\n",
    "                #qml.CZ(wires=[i, i + 1])\n",
    "    def SEL(params, measurement_qubits=0):\n",
    "        qml.StronglyEntanglingLayers(params, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    def RL(params, measurement_qubits=0):\n",
    "        qml.RandomLayers(params, ratio_imprim=0.8 ,imprimitive=qml.CZ, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    if circ == \"rzry\":\n",
    "        qcircuit = RZRY\n",
    "    elif circ == \"simplified_two_design\":\n",
    "        qcircuit = S2D\n",
    "    elif circ == \"special_unitary\":\n",
    "        qcircuit = SU\n",
    "    elif circ == \"simpleRZRY\":\n",
    "        qcircuit = simmpleRZRY\n",
    "    elif circ == \"RY\":\n",
    "        qcircuit = RY\n",
    "    elif circ == \"ghz\":\n",
    "        qcircuit = GHZ\n",
    "    elif circ == \"random_product_state\":\n",
    "        qcircuit = random_product_state\n",
    "    elif circ == \"SEL\":\n",
    "        qcircuit = SEL\n",
    "    elif circ == \"RL\":\n",
    "        qcircuit = RL\n",
    "    if not fim:\n",
    "        circuit = qml.QNode(qcircuit, dev,interface=\"torch\", diff_method=\"backprop\")\n",
    "    else:\n",
    "        circuit = qml.QNode(qcircuit, dev)\n",
    "\n",
    "    return circuit\n",
    "\n",
    "def compute_gradient(log_prob, w):\n",
    "    \"\"\"Compute gradient of the log probability with respect to weights.\n",
    "    \n",
    "    Args:\n",
    "    - log_prob (torch.Tensor): The log probability tensor.\n",
    "    - w (torch.Tensor): The weights tensor, with requires_grad=True.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The gradient of log_prob with respect to w, flattened.\n",
    "    \"\"\"\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()\n",
    "    log_prob.backward(retain_graph=True)\n",
    "    \n",
    "    if w.grad is None:\n",
    "        raise ValueError(\"The gradient for the given log_prob with respect to w is None.\")\n",
    "    \n",
    "    return w.grad.view(-1).detach().numpy()\n",
    "\n",
    "def policy(probs, policy_type=\"contiguous-like\", n_actions=2, n_qubits=1):\n",
    "\n",
    "    if policy_type == \"contiguous-like\":\n",
    "        return probs\n",
    "    elif policy_type == \"parity-like\":\n",
    "        policy = torch.zeros(n_actions)\n",
    "        for i in range(len(probs)):\n",
    "            a=[]\n",
    "            for m in range(int(np.log2(n_actions))):\n",
    "                if m==0:    \n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)\n",
    "                else:\n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)[:-m]\n",
    "                \n",
    "                a.append(bitstring.count(\"1\") % 2)\n",
    "            policy[int(\"\".join(str(x) for x in a),2)] += probs[i]\n",
    "\n",
    "        return policy    \n",
    "    \n",
    "def compute_policy_and_gradient(args):\n",
    "    n_qubits, shapes, type , n_actions, policy_type, clamp = args\n",
    "\n",
    "    if policy_type == \"parity-like\":\n",
    "        measure_qubits = n_qubits\n",
    "    else:\n",
    "        measure_qubits = int(np.log2(n_actions))\n",
    "\n",
    "    qc = create_circuit(n_qubits, circ=type, fim=False, shots=None)\n",
    "\n",
    "    if type == \"simplified_two_design\":\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_init = torch.tensor(weights[0], requires_grad=False)\n",
    "        weights_tensor_params = torch.tensor(weights[1], requires_grad=True)\n",
    "        \n",
    "        probs = qc(weights_tensor_init,weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    else:\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_params = torch.tensor(weights, requires_grad=True)\n",
    "\n",
    "        probs = qc(weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    pi = policy(probs, policy_type=policy_type, n_actions=n_actions, n_qubits=n_qubits)\n",
    "    if clamp is not None:\n",
    "        pi = torch.clamp(pi, clamp, 1)\n",
    "\n",
    "    dist = torch.distributions.Categorical(probs=pi)\n",
    "    \n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    gradient_no_clamp = np.linalg.norm(compute_gradient(log_prob, weights_tensor_params), 2)\n",
    "    return gradient_no_clamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, env, n_episodes=1000, max_t=1000, gamma=1.0, print_every=5):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    average_scores = []\n",
    "    runtime_sum = 0\n",
    "    for e in range(1, n_episodes):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Sample the action from current policy\n",
    "            if t==0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = policy.sample(state_tensor)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # Total expected reward\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "    #standardized returns\n",
    "        R=0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0,R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + np.finfo(np.float32).eps)\n",
    "\n",
    "        for log_prob, R in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        policy_sum = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "    # Backpropagation\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        policy_sum.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        runtime = end_time-start_time\n",
    "        \n",
    "        runtime_sum += runtime\n",
    "        if e % print_every == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tLast {}\\tEpisodes average reward: {:.2f}\\tRuntime: {:.2f}'.format(e, scores_deque[-1], print_every, np.mean(scores_deque), runtime_sum))\n",
    "            runtime_sum = 0\n",
    "        if np.mean(scores_deque) == 500:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores, policy.gradient_list, average_scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def save_training_data(self):\n",
    "        ''' \n",
    "        Saves training data into json files\n",
    "        '''\n",
    "        current_directory = os.path.dirname(__file__)\n",
    "        folder_name = f\"{str(self.env_name)}_{self.pqc.policy.post_processing}_{self.pqc.circuit.n_layers}\"\n",
    "        folder_path = os.path.join(current_directory, folder_name)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        episode_data = [self.scores_deque, \n",
    "                        self.runtime, \n",
    "                        self.loss.item(), \n",
    "                        tensor_to_list(self.pqc.get_gradients()[0]), \n",
    "                        tensor_to_list(self.pqc.get_gradients()[1])]\n",
    "\n",
    "        if folder_path is not None:\n",
    "            file_path = os.path.join(self.folder_path, f\"{self.file_name}.json\")\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, 'r') as f:\n",
    "                    existing_data = json.load(f)\n",
    "                existing_data.append(episode_data)\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump(existing_data, f, indent=4)\n",
    "            else:\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump([episode_data], f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ../../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "\n",
    "def tensor_to_list(tensor):\n",
    "    \"\"\"\n",
    "    Convert a tensor or numpy array to a nested list.\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, list):\n",
    "        return [tensor_to_list(t) for t in tensor]\n",
    "    elif isinstance(tensor, dict):\n",
    "        return {key: tensor_to_list(value) for key, value in tensor.items()}\n",
    "    elif isinstance(tensor, np.ndarray):\n",
    "        return tensor.tolist()\n",
    "    elif isinstance(tensor, torch.Tensor):\n",
    "        return tensor.tolist()\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "def measure_probs(qubits):\n",
    "    return qml.probs(wires=range(qubits)) \n",
    "\n",
    "def measure_expval_pairs(qubits):\n",
    "    expvals = []\n",
    "    for i in range(qubits // 2):\n",
    "        expvals.append(qml.expval(qml.PauliZ(2*i) @ qml.PauliZ(2*i + 1)))\n",
    "    return expvals\n",
    "    \n",
    "def create_optimizer_with_lr(params, lr_list, use_amsgrad=False):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': p, 'lr': lr} for p, lr in zip(params, lr_list)\n",
    "    ], amsgrad=use_amsgrad)\n",
    "    return optimizer\n",
    "\n",
    "def get_function_representation(func):\n",
    "    if callable(func):\n",
    "        # Check if the function is a lambda\n",
    "        if func.__name__ == \"<lambda>\":\n",
    "            # Optionally, check if the function has a custom description attribute\n",
    "            return f\"{func.__module__}.<lambda>\" + (getattr(func, 'description', ''))\n",
    "        else:\n",
    "            return f\"{func.__module__}.{func.__name__}\"\n",
    "    return \"Unknown Function Type\"\n",
    "\n",
    "def jerbi_circuit(n_qubits, n_layers, shots, input_scaling, diff_method, weight_init, input_init, measure, measure_qubits):\n",
    "\n",
    "    if shots is None:\n",
    "        dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "    else:\n",
    "        dev = qml.device(\"default.qubit\", wires=n_qubits, shots=shots)\n",
    "    \n",
    "    if n_layers < 1:\n",
    "        raise ValueError(\"Number of layers can't take values below 1\")\n",
    "    \n",
    "    weight_shapes = {\"params\": (n_layers + 1, n_qubits, 2),\n",
    "                    \"input_params\": (n_layers, n_qubits, 2)}\n",
    "    init_method   = {\"params\": weight_init,\n",
    "                    \"input_params\": input_init}\n",
    "    \n",
    "    @qml.qnode(dev, interface='torch', diff_method=diff_method)\n",
    "    def qnode(inputs, params, input_params):\n",
    "    #in case n_qubits != input length\n",
    "        if n_qubits > len(inputs) and n_qubits % len(inputs) == 0:\n",
    "            multiplier = n_qubits // len(inputs)\n",
    "            inputs = torch.cat([inputs] * multiplier)\n",
    "        elif n_qubits != len(inputs) and n_qubits % len(inputs) != 0:\n",
    "            raise ValueError('Number of qubits cannot be divided by input lenght')\n",
    "\n",
    "    #hadamard\n",
    "        qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        \n",
    "        for layer in range(n_layers):\n",
    "            for wire in range(n_qubits):\n",
    "                qml.RZ(params[layer][wire][0], wires=wire)\n",
    "                qml.RY(params[layer][wire][1], wires=wire)\n",
    "\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "            if input_scaling:\n",
    "                for wire in range(n_qubits):\n",
    "                    qml.RY(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                    qml.RZ(input_params[layer][wire][1] * inputs[wire], wires=wire)\n",
    "            else:\n",
    "                for wire in range(n_qubits):\n",
    "                    qml.RY(inputs[wire], wires=wire)\n",
    "                    qml.RZ(inputs[wire], wires=wire)\n",
    "\n",
    "        for wire in range(n_qubits):\n",
    "            qml.RZ(params[-1][wire][0], wires=wire)\n",
    "            qml.RY(params[-1][wire][1], wires=wire)\n",
    "            \n",
    "        return measure(measure_qubits)\n",
    "\n",
    "    model = qml.qnn.TorchLayer(qnode, weight_shapes=weight_shapes, init_method=init_method)  \n",
    "    \n",
    "    return model\n",
    "    \n",
    "def S2D(n_qubits, n_layers, shots, input_scaling, diff_method, weight_init, input_init, measure_type, observables, measure_qubits):\n",
    "\n",
    "    dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "    observables = observables if observables is not None else None\n",
    "    \n",
    "    shapes = qml.SimplifiedTwoDesign.shape(n_layers=n_layers, n_wires=n_qubits)\n",
    "\n",
    "    weight_shapes = {\"params\": shapes[1],\n",
    "                     \"input_params\": shapes[0]}\n",
    "    \n",
    "    init_method   = {\"params\": weight_init,\n",
    "                     \"input_params\": input_init}\n",
    "\n",
    "    @qml.qnode(dev, interface='torch', diff_method='parameter-shift')\n",
    "    def qnode(inputs, params, input_params):\n",
    "\n",
    "        return measure_selection(measure_type, observables, measure_qubits)\n",
    "\n",
    "    model = qml.qnn.TorchLayer(qnode, weight_shapes=weight_shapes, init_method=init_method)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers,  shots= None, input_scaling=True, design='jerbi_circuit', diff_method = 'backprop', weight_init=torch.nn.init.uniform_, input_init = torch.nn.init.ones_, measure = None, measure_qubits = None):\n",
    "        super(CircuitGenerator, self).__init__()\n",
    "        '''\n",
    "\n",
    "        Creates a parameterized quantum circuit based on the arguments:\n",
    "\n",
    "            n_qubits(int) = Number of qubits\n",
    "            n_layers(int) = Number of layers (0 if no data re-uploading)\n",
    "            shots(int) = Number of times the circuit gets executed\n",
    "            input_scaling(bool) = Input parameters are used if True (input*input_params)\n",
    "            design(str) = The PQC ansatz design ('jerbi_circuit')\n",
    "            diff_method(str) = Differentiation method ('best', 'backprop', 'parameter-shift', ...)\n",
    "            weight_init (torch.nn.init) = How PQC weights are initialized (.uniform_, .ones_, ...)\n",
    "            input_init (torch.nn.init) = How input weights are initialized (.uniform_, .ones_, ...)\n",
    "            measure (function) = Measure function (measure_probs, measure_expval_pairs)\n",
    "            measure_qubits (int) = Number of qubits to be measured (in some cases might be equal to the number of qubits)\n",
    "            \n",
    "        '''\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.shots = shots\n",
    "        self.input_scaling = input_scaling\n",
    "        self.design = design\n",
    "        self.diff_method = diff_method\n",
    "        self.weight_init = weight_init\n",
    "        self.input_init = input_init\n",
    "        if measure is None:\n",
    "            self.measure = measure_probs\n",
    "        else:\n",
    "            self.measure = measure\n",
    "\n",
    "        if measure_qubits is None:\n",
    "            self.measure_qubits = n_qubits\n",
    "        else:\n",
    "            self.measure_qubits = measure_qubits\n",
    "\n",
    "        if self.design == 'jerbi_circuit':\n",
    "            self.circuit = jerbi_circuit(n_qubits = self.n_qubits,\n",
    "                                        n_layers = self.n_layers,\n",
    "                                        shots = self.shots,\n",
    "                                        input_scaling = self.input_scaling,\n",
    "                                        diff_method = self.diff_method,\n",
    "                                        weight_init = self.weight_init,\n",
    "                                        input_init = self.input_init,\n",
    "                                        measure = self.measure,\n",
    "                                        measure_qubits = self.measure_qubits)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported circuit type\")\n",
    "\n",
    "    def input(self,inputs):\n",
    "\n",
    "        outputs = self.circuit(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyType(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_qubits, n_actions, post_processing = 'raw_contiguous', beta = 0.5, increase_rate = 0.003):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_qubits = n_qubits\n",
    "        self.beta = beta\n",
    "        self.increase_rate = increase_rate\n",
    "        self.post_processing = post_processing\n",
    "        self.reward_threshold = None\n",
    "\n",
    "    def input(self,probs):\n",
    "        if self.post_processing == 'raw_contiguous':\n",
    "            policy = self.raw_contiguous(probs)\n",
    "        elif self.post_processing == 'raw_parity':\n",
    "            policy = self.raw_parity(probs)\n",
    "        elif self.post_processing == 'softmax':\n",
    "            policy = self.softmax(probs)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid post-processing method specified.\")\n",
    "        return policy\n",
    "\n",
    "    def raw_contiguous(self,probs):\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        chunk_size = len(probs_flatten) // self.n_actions\n",
    "        remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "        policy = []\n",
    "\n",
    "        for i in range(self.n_actions):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size\n",
    "\n",
    "            if i < remainder:\n",
    "                end += 1\n",
    "\n",
    "            # Update the original policy list instead of creating a new one\n",
    "            policy.append(sum(probs_flatten[start:end]))\n",
    "\n",
    "        policy_tensor = torch.stack(policy)\n",
    "        return policy_tensor\n",
    "        \n",
    "    def raw_parity(self,probs):\n",
    "\n",
    "        if self.n_actions % 2 != 0:\n",
    "            raise ValueError('For parity-like policy, n_actions must be an even number')\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        policy = torch.zeros(self.n_actions)\n",
    "        counter = 0\n",
    "        for prob in probs_flatten:\n",
    "            policy[counter] += prob\n",
    "            counter += 1\n",
    "            if counter == self.n_actions:\n",
    "                counter = 0\n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    def softmax(self, probs):\n",
    "        if len(probs) == self.n_actions:\n",
    "            scaled_output = probs * self.beta\n",
    "            softmax_output = F.softmax(scaled_output, dim=0)\n",
    "            return softmax_output\n",
    "        else:\n",
    "            probs_flatten = probs.flatten()\n",
    "            chunk_size = len(probs_flatten) // self.n_actions\n",
    "            remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "            policy = []\n",
    "\n",
    "            for i in range(self.n_actions):\n",
    "                start = i * chunk_size\n",
    "                end = (i + 1) * chunk_size\n",
    "\n",
    "                if i < remainder:\n",
    "                    end += 1\n",
    "\n",
    "                # Update the original policy list instead of creating a new one\n",
    "                policy.append(sum(probs_flatten[start:end]))\n",
    "            policy_tensor = torch.stack(policy)\n",
    "            softmax_output = F.softmax(policy_tensor * self.beta, dim=0)\n",
    "            return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumPolicyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, circuit, policy):\n",
    "        super(QuantumPolicyModel, self).__init__()\n",
    "        self.circuit = circuit\n",
    "        self.policy = policy\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Input state is fed to the circuit - its output is then fed to the post processing \n",
    "        '''\n",
    "        probs = self.circuit.input(inputs)\n",
    "        probs_processed = self.policy.input(probs)\n",
    "        return probs_processed\n",
    "    \n",
    "    def sample(self, inputs):\n",
    "        '''\n",
    "        Samples an action from the action probability distribution aka policy\n",
    "        '''\n",
    "        policy = self.forward(inputs)\n",
    "        dist = torch.distributions.Categorical(policy)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), policy\n",
    "    \n",
    "    def beta_schedule(self):\n",
    "\n",
    "        print(self.policy.beta)\n",
    "        if self.policy.post_processing == 'softmax':\n",
    "            self.policy.beta += self.policy.increase_rate\n",
    "    \n",
    "    def beta_schedule_smart(self, reward):\n",
    "\n",
    "        print(self.policy.beta)\n",
    "        if self.policy.reward_threshold is None:\n",
    "            self.policy.reward_threshold = reward\n",
    "        else:\n",
    "            self.policy.reward_threshold = 0.95 * self.policy.reward_threshold + 0.05 * reward\n",
    "\n",
    "        if reward > self.policy.reward_threshold:\n",
    "            self.policy.beta *= 1.015\n",
    "        else:\n",
    "            self.policy.beta *= 0.99\n",
    "\n",
    "        self.policy.beta = max(0.3, min(5, self.policy.beta))\n",
    "\n",
    "    def get_parameters(self):\n",
    "        '''\n",
    "        Returns the values of each set of parameters\n",
    "        '''\n",
    "        parameter_values = [param.clone().detach().numpy().flatten() for param in self.circuit.parameters()]\n",
    "        return parameter_values\n",
    "    \n",
    "    def get_gradients(self):\n",
    "        '''\n",
    "        Returns the gradient values of each set of parameters\n",
    "        '''\n",
    "        gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for name, param in self.circuit.named_parameters()]\n",
    "        return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceUpdate():\n",
    "\n",
    "    def __init__(self, pqc, optimizer, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name = None, rundate = None):\n",
    "        \n",
    "        self.pqc = pqc\n",
    "        self.optimizer = optimizer\n",
    "        self.env = env\n",
    "        self.env_name = env_name\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.scores_deque = deque(maxlen=print_every)\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "        self.file_name = file_name\n",
    "        self.rundate = rundate\n",
    "        self.running_reward = 10\n",
    "\n",
    "    def get_trajectory(self):\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(self.max_t):\n",
    "            if t == 0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = self.pqc.sample(state_tensor)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            \n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                self.scores_deque.append(sum(self.rewards))\n",
    "                break\n",
    "\n",
    "    def update_policy(self):\n",
    "\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        for log_prob, ret in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * ret)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        self.loss = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()    \n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def save_agent_data(self,main_path):\n",
    "\n",
    "        agent_variables = {\n",
    "            \"Number of Qubits\": self.pqc.circuit.n_qubits,\n",
    "            \"Number of Layers\": self.pqc.circuit.n_layers,\n",
    "            \"Shots\": self.pqc.circuit.shots,\n",
    "            \"Input Scaling\": self.pqc.circuit.input_scaling,\n",
    "            \"Design\": self.pqc.circuit.design,\n",
    "            \"Differentiation Method\": self.pqc.circuit.diff_method,\n",
    "            \"Weight Initiation\": \"lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\",\n",
    "            \"Input_init\": get_function_representation(self.pqc.circuit.input_init),\n",
    "            \"Measure\": get_function_representation(self.pqc.circuit.measure),\n",
    "            \"Measure Qubits\": self.pqc.circuit.measure_qubits,\n",
    "            \"Policy Type\": self.pqc.policy.post_processing,\n",
    "            \"Softmax scheduling (in case policy is softmax)\": (\"Starting beta: \" + str(self.pqc.policy.beta) + \". Increase rate: \" + str(self.pqc.policy.increase_rate)),\n",
    "            \"Optimizers\": str(self.optimizer),\n",
    "            \"Envinronment Name\": str(self.env_name),\n",
    "            \"Gamma (discounting factor)\": self.gamma,\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(main_path, \"agent_characteristics.json\"), \"w\") as f:\n",
    "            json.dump(agent_variables, f, indent=4)\n",
    "\n",
    "    def save_data(self,main_path):\n",
    "\n",
    "        run= os.path.join(main_path,str(self.file_name)+'_data.npz')\n",
    "\n",
    "        if not os.path.exists(main_path):\n",
    "            os.makedirs(main_path)\n",
    "\n",
    "        if os.path.exists(run):\n",
    "            data = np.load(run, allow_pickle=True)\n",
    "            old_episode_reward = data['episode_reward'].tolist()\n",
    "            old_loss = data['loss'].tolist()\n",
    "            old_runtime = data['runtime'].tolist()\n",
    "            old_params_gradients = data['params_gradients'].tolist()\n",
    "            old_input_params_gradients = data['input_params_gradients'].tolist()\n",
    "        else:\n",
    "            old_episode_reward = []\n",
    "            old_loss = []\n",
    "            old_runtime = []\n",
    "            old_params_gradients = []\n",
    "            old_input_params_gradients = []\n",
    "\n",
    "\n",
    "        old_episode_reward.append(self.scores_deque[-1])\n",
    "        old_loss.append(self.loss.item())\n",
    "        old_runtime.append(self.runtime)\n",
    "        old_params_gradients.append(tensor_to_list(self.pqc.get_gradients()[0]))\n",
    "        old_input_params_gradients.append(tensor_to_list(self.pqc.get_gradients()[1]))\n",
    "\n",
    "        np.savez_compressed(run, episode_reward = np.array(old_episode_reward),\n",
    "                                 loss = np.array(old_loss),\n",
    "                                 runtime = np.array(old_runtime),\n",
    "                                 params_gradients = np.array(old_params_gradients), \n",
    "                                 input_params_gradients = np.array(old_input_params_gradients))\n",
    "        \n",
    "        del old_episode_reward[:]\n",
    "        del old_loss[:]\n",
    "        del old_runtime[:]\n",
    "        del old_params_gradients[:]\n",
    "        del old_input_params_gradients[:]\n",
    "\n",
    "    def writer_function(self, writer, iteration):\n",
    "\n",
    "        writer.add_scalar(\"Episode Reward\", np.mean(self.scores_deque), global_step=iteration)\n",
    "        writer.add_scalar(\"Running Reward\", self.running_reward, global_step=iteration)\n",
    "        writer.add_scalar(\"Runtime\", self.runtime, global_step=iteration)\n",
    "        writer.add_scalar(\"Loss\", self.loss.item(), global_step=iteration)\n",
    "        writer.add_scalar(\"Beta\", self.pqc.policy.beta, global_step=iteration)\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        logs_dir = \"../../data\"\n",
    "        os.makedirs(logs_dir, exist_ok=True)\n",
    "        envinronment_folder = os.path.join(logs_dir, self.env_name)\n",
    "        os.makedirs(envinronment_folder, exist_ok=True)\n",
    "        experiment_folder = f\"{self.pqc.policy.post_processing}_{self.pqc.circuit.n_layers}layer_{self.rundate}\"\n",
    "        experiment_path = os.path.join(envinronment_folder, experiment_folder)\n",
    "        os.makedirs(experiment_path, exist_ok=True)\n",
    "        run = os.path.join(experiment_path,str(self.file_name))\n",
    "        os.makedirs(run, exist_ok=True)\n",
    "        writer = SummaryWriter(log_dir=run)\n",
    "        self.save_agent_data(experiment_path)\n",
    "        \n",
    "        for i in range(1, self.n_episodes):\n",
    "            start_time = time.time()\n",
    "            self.get_trajectory()\n",
    "            self.pqc.beta_schedule_smart(self.scores_deque[-1])\n",
    "            #self.pqc.beta_schedule()\n",
    "            self.update_policy()\n",
    "            end_time = time.time()\n",
    "            self.runtime = end_time - start_time\n",
    "            self.writer_function(writer,i)\n",
    "            self.save_data(run)\n",
    "            self.running_reward = (self.running_reward * 0.99) + (len(self.rewards) * 0.01)\n",
    "            \n",
    "            if self.running_reward > self.env.spec.reward_threshold:\n",
    "                print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i, np.mean(self.scores_deque)))\n",
    "                break\n",
    "            elif i % self.print_every == 0 and self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tLast {} Episodes average reward: {:.2f}\\tRuntime: {:.2f}\\t {:.2f}\\t'.format(i, self.scores_deque[-1], self.print_every, np.mean(self.scores_deque), self.runtime, self.running_reward))\n",
    "\n",
    "        #self.event_acumulator(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single agent runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 4      #set to 1 if data_reuploading is off\n",
    "n_actions = 2\n",
    "shots = None\n",
    "input_scaling = True\n",
    "design = 'jerbi_circuit' \n",
    "diff_method = 'backprop' \n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "input_init = torch.nn.init.ones_\n",
    "measure = measure_expval_pairs\n",
    "measure_qubits = None\n",
    "circuit = CircuitGenerator( n_qubits, \n",
    "                            n_layers,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure,\n",
    "                            measure_qubits)\n",
    "\n",
    "beta = 0.5\n",
    "increase_rate = 0.0025\n",
    "post_processing = 'softmax'\n",
    "policy_type = PolicyType(n_qubits, n_actions, post_processing, beta, increase_rate)\n",
    "\n",
    "\n",
    "pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "\n",
    "pqc = pqc\n",
    "lr_list= [0.01,0.08]\n",
    "params= circuit.parameters()\n",
    "optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)\n",
    "n_episodes = 1000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 5\n",
    "verbose = 1\n",
    "\n",
    "reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple agent runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Contiguous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 1      #set to 1 if data_reuploading is off\n",
    "    n_actions = 2\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_probs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator( n_qubits, \n",
    "                                n_layers,\n",
    "                                shots,\n",
    "                                input_scaling,\n",
    "                                design,\n",
    "                                diff_method,\n",
    "                                weight_init,\n",
    "                                input_init,\n",
    "                                measure,\n",
    "                                measure_qubits)\n",
    "\n",
    "\n",
    "    post_processing = 'raw_contiguous'\n",
    "    policy_type = PolicyType(n_qubits, n_actions)\n",
    "\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "    \n",
    "\n",
    "    pqc = pqc\n",
    "    lr_list= [0.025,0.075]\n",
    "    params= circuit.parameters()\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "        \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, i) for i in range(num_agents))\n",
    "    print(results)\n",
    "    time.sleep(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 1      #set to 1 if data_reuploading is off\n",
    "    n_actions = 2\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_probs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator( n_qubits, \n",
    "                                n_layers,\n",
    "                                shots,\n",
    "                                input_scaling,\n",
    "                                design,\n",
    "                                diff_method,\n",
    "                                weight_init,\n",
    "                                input_init,\n",
    "                                measure,\n",
    "                                measure_qubits)\n",
    "\n",
    "\n",
    "    post_processing = 'raw_parity'\n",
    "    policy_type = PolicyType(n_qubits, n_actions, post_processing)\n",
    "\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "    \n",
    "\n",
    "    pqc = pqc\n",
    "    lr_list= [0.025,0.075]\n",
    "    params= circuit.parameters()\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')  \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, i+10, rundate) for i in range(num_agents))\n",
    "    print(results)\n",
    "    time.sleep(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    n_actions = 2\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_expval_pairs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator( n_qubits, \n",
    "                                n_layers,\n",
    "                                shots,\n",
    "                                input_scaling,\n",
    "                                design,\n",
    "                                diff_method,\n",
    "                                weight_init,\n",
    "                                input_init,\n",
    "                                measure,\n",
    "                                measure_qubits)\n",
    "\n",
    "    post_processing = 'softmax'\n",
    "    beta = .5\n",
    "    increase_rate = 0.003\n",
    "    policy_type = PolicyType(n_qubits, n_actions, post_processing, beta, increase_rate)\n",
    "\n",
    "    pqc_list = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr_list= [0.025,0.075]\n",
    "    params= circuit.parameters()\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc_list[i], optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studying different inverse temperature schedulings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    n_actions = 2\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_expval_pairs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator( n_qubits, \n",
    "                                n_layers,\n",
    "                                shots,\n",
    "                                input_scaling,\n",
    "                                design,\n",
    "                                diff_method,\n",
    "                                weight_init,\n",
    "                                input_init,\n",
    "                                measure,\n",
    "                                measure_qubits)\n",
    "\n",
    "    post_processing = 'softmax'\n",
    "    beta_list = [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.]\n",
    "    increase_rate = 0.003\n",
    "    policy_type_list = [PolicyType(n_qubits, n_actions, post_processing, i, increase_rate) for i in beta_list]\n",
    "\n",
    "    pqc_list = [QuantumPolicyModel(circuit,policy_type) for policy_type in policy_type_list]\n",
    "\n",
    "    lr_list= [0.025,0.075]\n",
    "    params= circuit.parameters()\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc_list[i], optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, beta_list[i], rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    n_actions = 2\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_expval_pairs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator( n_qubits, \n",
    "                                n_layers,\n",
    "                                shots,\n",
    "                                input_scaling,\n",
    "                                design,\n",
    "                                diff_method,\n",
    "                                weight_init,\n",
    "                                input_init,\n",
    "                                measure,\n",
    "                                measure_qubits)\n",
    "\n",
    "    post_processing = 'softmax'\n",
    "    beta = 0.35\n",
    "    increase_rate_list = [.0005, .001, .002, .0035, .005, .0065, .008, .0095, .01, .02]\n",
    "    policy_type_list = [PolicyType(n_qubits, n_actions, post_processing, beta, i) for i in increase_rate_list]\n",
    "\n",
    "    pqc_list = [QuantumPolicyModel(circuit,policy_type) for policy_type in policy_type_list]\n",
    "\n",
    "    lr_list= [0.025,0.075]\n",
    "    params= circuit.parameters()\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc_list[i], optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, increase_rate_list[i], rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studying different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):\n",
    " \n",
    "    # initialize an empty string\n",
    "    str1 = \"\"\n",
    " \n",
    "    # traverse in the string\n",
    "    for ele in s:\n",
    "        str1 += ele\n",
    " \n",
    "    # return string\n",
    "    return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    n_actions = 2\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_probs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator( n_qubits, \n",
    "                                n_layers,\n",
    "                                shots,\n",
    "                                input_scaling,\n",
    "                                design,\n",
    "                                diff_method,\n",
    "                                weight_init,\n",
    "                                input_init,\n",
    "                                measure,\n",
    "                                measure_qubits)\n",
    "\n",
    "\n",
    "    post_processing = 'raw_parity'\n",
    "    policy_type = PolicyType(n_qubits, n_actions, post_processing)\n",
    "\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "    \n",
    "\n",
    "    pqc = pqc\n",
    "    lr_list= [[0.01,0.01],[0.025,0.01],[0.05,0.01],[0.1,0.01],[0.01,0.25],[0.01,0.05],[0.01,0.1],[0.25,0.25],[0.50,0.50],[0.1,0.1]]\n",
    "    params= [circuit.parameters() for i in range(len(lr_list))]\n",
    "    optimizers= [create_optimizer_with_lr(param,lr) for param,lr in zip(params,lr_list)]\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')  \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers[i], env, env_name, n_episodes, max_t, gamma, print_every, verbose, lr_list[i], rundate) for i in range(num_agents))\n",
    "    print(results)\n",
    "    time.sleep(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {file_name}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    n_actions = 2\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_probs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator( n_qubits, \n",
    "                                n_layers,\n",
    "                                shots,\n",
    "                                input_scaling,\n",
    "                                design,\n",
    "                                diff_method,\n",
    "                                weight_init,\n",
    "                                input_init,\n",
    "                                measure,\n",
    "                                measure_qubits)\n",
    "\n",
    "\n",
    "    post_processing = 'raw_parity'\n",
    "    policy_type = PolicyType(n_qubits, n_actions, post_processing)\n",
    "\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "    \n",
    "\n",
    "    pqc = pqc\n",
    "    lr_list= [[0.01,0.01],[0.025,0.01],[0.05,0.01],[0.1,0.01],[0.01,0.25],[0.01,0.05],[0.01,0.1],[0.25,0.25],[0.50,0.50],[0.1,0.1]]\n",
    "    params= [circuit.parameters() for i in range(len(lr_list))]\n",
    "    optimizers= [create_optimizer_with_lr(param, lr, use_amsgrad=True) for param,lr in zip(params,lr_list)]\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')  \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(pqc, optimizers[i], env, env_name, n_episodes, max_t, gamma, print_every, verbose, lr_list[i], rundate) for i in range(num_agents))\n",
    "    print(results)\n",
    "    time.sleep(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
