{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "import ray\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "def create_directory(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def tensor_to_list(tensor):\n",
    "    \"\"\"\n",
    "    Convert a tensor or numpy array to a nested list.\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, list):\n",
    "        return [tensor_to_list(t) for t in tensor]\n",
    "    elif isinstance(tensor, dict):\n",
    "        return {key: tensor_to_list(value) for key, value in tensor.items()}\n",
    "    elif isinstance(tensor, np.ndarray):\n",
    "        return tensor.tolist()\n",
    "    elif isinstance(tensor, torch.Tensor):\n",
    "        return tensor.tolist()\n",
    "    else:\n",
    "        return tensor\n",
    "    \n",
    "def create_optimizer_with_lr(params, lr_list, use_amsgrad=False):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': p, 'lr': lr} for p, lr in zip(params, lr_list)\n",
    "    ], amsgrad=use_amsgrad)\n",
    "    return optimizer\n",
    "\n",
    "def get_function_representation(func):\n",
    "    # Check if the object is callable\n",
    "    if callable(func):\n",
    "        # Handle functools.partial objects separately\n",
    "        if isinstance(func, partial):\n",
    "            # Get the original function wrapped by functools.partial\n",
    "            original_func = func.func\n",
    "            # Represent the original function with its module and name\n",
    "            original_func_rep = f\"{original_func.__module__}.{original_func.__name__}\"\n",
    "            # Optionally, include the arguments set by partial\n",
    "            return f\"partial({original_func_rep}, args={func.args}, keywords={func.keywords})\"\n",
    "        # Handle lambda functions\n",
    "        elif func.__name__ == \"<lambda>\":\n",
    "            return f\"{func.__module__}.<lambda>\" + (getattr(func, 'description', ''))\n",
    "        else:\n",
    "            # Handle regular functions\n",
    "            return f\"{func.__module__}.{func.__name__}\"\n",
    "    return \"Unknown Function Type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measures\n",
    "\n",
    "def measure_probs(qubits):\n",
    "    return qml.probs(wires=range(qubits)) \n",
    "\n",
    "def two_measure_expval(qubits):\n",
    "\n",
    "    pauli_string = qml.PauliZ(0)\n",
    "    for i in range(1, qubits):\n",
    "        pauli_string = pauli_string @ qml.PauliZ(i)\n",
    "    \n",
    "    expvals = []\n",
    "    expvals.append(qml.expval(pauli_string))\n",
    "    expvals.append(qml.expval(-pauli_string))\n",
    "\n",
    "    return expvals\n",
    "\n",
    "def three_measure_expval(qubits):\n",
    "    expvals = []\n",
    "\n",
    "    if qubits == 1:\n",
    "        first_observable = qml.PauliZ(0)\n",
    "        middle_observable = qml.PauliX(0)\n",
    "        last_observable = -qml.PauliZ(0)\n",
    "    elif qubits == 2:\n",
    "        first_observable = qml.PauliZ(0)\n",
    "        middle_observable = qml.PauliZ(0) @ qml.PauliZ(1) \n",
    "        last_observable = qml.PauliZ(1)       \n",
    "    elif qubits >= 4:\n",
    "        first_observable = qml.PauliZ(0)\n",
    "        middle_observable = qml.PauliZ(1)\n",
    "        for i in range(2, qubits - 1):\n",
    "            middle_observable = middle_observable @ qml.PauliZ(i)\n",
    "        last_observable = qml.PauliZ(qubits - 1)                    \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported number of qubits: only 1, 3, or 4 qubits are supported\")\n",
    "\n",
    "    expvals.append(qml.expval(first_observable))\n",
    "    expvals.append(qml.expval(middle_observable))\n",
    "    expvals.append(qml.expval(last_observable))\n",
    "\n",
    "    return expvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JerbiModel(nn.Module):\n",
    "    '''\n",
    "    Creates a parametrized quantum circuit based on the 'Parametrized quantum policies for reinforcement learning' paper by Sofiene Jerbi.\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, \n",
    "                n_qubits,\n",
    "                n_layers, \n",
    "                device = \"default_qubit\",\n",
    "                shots = None,\n",
    "                diff_method = 'best', \n",
    "                entanglement = True,\n",
    "                entanglement_pattern = \"all_to_all\", \n",
    "                entanglement_gate = qml.CZ, \n",
    "                input_scaling = True, \n",
    "                input_init = None, \n",
    "                weight_init = None, \n",
    "                measure = two_measure_expval):\n",
    "        super(JerbiModel, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_scaling = input_scaling\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        # Call the error handling function\n",
    "        self.handle_errors_and_warnings()\n",
    "\n",
    "        # Initialize the device\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(self.device, wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(self.device, wires=self.n_qubits, shots=self.shots)\n",
    "\n",
    "        # Weight initialization\n",
    "        self.weight_shapes = {\n",
    "            \"input_params\": (self.n_layers, self.n_qubits, 2),\n",
    "            \"params\": (self.n_layers + 1, self.n_qubits, 2)\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"input_params\": self.input_init,\n",
    "            \"params\": self.weight_init\n",
    "        }\n",
    "\n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, params, input_params):\n",
    "\n",
    "            # Apply Hadamard gates to all qubits\n",
    "            qml.broadcast(qml.Hadamard, wires=range(self.n_qubits), pattern=\"single\")\n",
    "\n",
    "            # Apply layers and entanglement\n",
    "            for layer in range(self.n_layers):\n",
    "                for wire in range(self.n_qubits):\n",
    "                    qml.RZ(params[layer][wire][0], wires=wire)\n",
    "                    qml.RY(params[layer][wire][1], wires=wire)\n",
    "\n",
    "                if self.entanglement:\n",
    "                    qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "                # Input scaling\n",
    "                if self.input_scaling is True:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RY(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                        qml.RZ(input_params[layer][wire][1] * inputs[wire], wires=wire)\n",
    "                else:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RY(inputs[wire], wires=wire)\n",
    "                        qml.RZ(inputs[wire], wires=wire)\n",
    "\n",
    "            # Final layer\n",
    "            for wire in range(self.n_qubits):\n",
    "                qml.RZ(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "        model = qml.qnn.TorchLayer(qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)\n",
    "\n",
    "        return model\n",
    "\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        Gives inputs to the circuit and outputs the respective output\n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        '''\n",
    "        Draws the circuit\n",
    "        '''\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        circuit = qml.draw_mpl(self.qnode)(inputs, \n",
    "                                        initialized_params[\"params\"], \n",
    "                                        initialized_params[\"input_params\"])\n",
    "\n",
    "\n",
    "    def handle_errors_and_warnings(self):\n",
    "        ''' \n",
    "        Handles the errors and warnings\n",
    "        '''\n",
    "        # Check if the number of layers is valid\n",
    "        if self.n_layers < 1:\n",
    "            raise ValueError(\"Number of layers must be at least 1.\")\n",
    "\n",
    "        # Check if the number of qubits is compatible with the input length\n",
    "        if not isinstance(self.n_qubits, int) or self.n_qubits <= 0:\n",
    "            raise ValueError(\"Number of qubits must be a positive integer.\")\n",
    "\n",
    "        if not isinstance(self.entanglement, bool):\n",
    "            raise TypeError(\"Entanglement flag must be a boolean value.\")\n",
    "        \n",
    "        if self.device not in [\"default_qubit\", \"default_mixed\", \"lightning.qubit\", \"lightning.gpu\"]:\n",
    "            raise Warning(f\"Unknown device '{self.device}' specified. Defaulting to 'default_qubit'.\")\n",
    "            self.device = \"default_qubit\"\n",
    "\n",
    "        if self.diff_method not in [\"best\", \"parameter-shift\", \"adjoint\", \"backprop\"]:\n",
    "            raise Warning(f\"Unknown differentiation method '{self.diff_method}'. Using 'best' method.\")\n",
    "            self.diff_method = \"best\"\n",
    " \n",
    "       \n",
    "    def info(cls):\n",
    "        '''\n",
    "        Provides a summary of the JerbiModel class, including its parameters and methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        JerbiModel: A Quantum Neural Network model that creates a Parameterized Quantum Circuit based on the design in https://doi.org/10.48550/arXiv.2103.05577.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_qubits (int): \n",
    "            Number of qubits used in the quantum circuit.\n",
    "        \n",
    "        n_layers (int): \n",
    "            Number of layers in the quantum circuit. Each layer typically consists of parameterized rotations followed by entanglement gates.\n",
    "        \n",
    "        device (str): \n",
    "            The quantum device used for simulation or execution (e.g., 'default_qubit', 'lightning.qubit', 'lightning.gpu').\n",
    "\n",
    "        shots (int, optional): \n",
    "            Number of times the circuit gets executed (repeated measurements). If None, the circuit is executed with analytic calculations (no shot noise).\n",
    "        \n",
    "        diff_method (str): \n",
    "            Differentiation method used for training the model. Common options are 'best', 'parameter-shift', 'backprop', etc.\n",
    "\n",
    "        entanglement (bool):\n",
    "            If True, entanglement between qubits is implemented. The entanglement pattern and gate are defined by `entanglement_pattern` and `entanglement_gate`, respectively.\n",
    "        \n",
    "        entanglement_pattern (str): \n",
    "            Entanglement pattern used in the circuit, such as 'chain', 'ring', 'all_to_all', etc., as defined by qml.broadcast patterns.\n",
    "        \n",
    "        entanglement_gate (function): \n",
    "            Quantum gate used for entanglement, such as qml.CZ or qml.CNOT. This gate is applied between qubits according to the specified entanglement pattern.\n",
    "        \n",
    "        input_scaling (bool): \n",
    "            If True, input parameters are scaled by additional learnable parameters (input_params). The input is multiplied by these parameters before being applied to the qubits.\n",
    "        \n",
    "        input_init (function): \n",
    "            Function to initialize the input scaling parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, or any function defined by the user.\n",
    "        \n",
    "        weight_init (function): \n",
    "            Function to initialize the weights of the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.normal_, or any function defined by the user.\n",
    "        \n",
    "        measure (function): \n",
    "            Measurement function that takes the number of qubits as an argument and returns the measurement result. Common choices are `measure_probs`, `two_measure_expval`, or any user-defined measurement function.\n",
    "\n",
    "        Methods:\n",
    "        --------\n",
    "        generate_circuit(self): \n",
    "            Generates and initializes the quantum circuit based on the parameters.\n",
    "        \n",
    "        forward(self, inputs): \n",
    "            Takes inputs and passes them through the quantum circuit to get the output.\n",
    "\n",
    "        visualize_circuit(self): \n",
    "            Visualizes the generated quantum circuit for the given number of qubits using the initial parameters. Useful for debugging or analyzing the circuit design.\n",
    "\n",
    "        handle_errors_and_warnings(self): \n",
    "            Handles common errors and warnings, such as invalid parameter values, unsupported devices, and incompatible differentiation methods.\n",
    "            \n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UQC(nn.Module):\n",
    "    '''\n",
    "    Creates a parameterized quantum circuit based on the 'Data re-uploading for a universal quantum classifier' paper by Adrián Pérez-Salinas.\n",
    "\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, \n",
    "                n_qubits, \n",
    "                n_layers, \n",
    "                state_dim,\n",
    "                device,\n",
    "                shots = None, \n",
    "                diff_method = 'best',\n",
    "                encoding_type = 'full',\n",
    "                entanglement = True,\n",
    "                entanglement_pattern = \"all_to_all\", \n",
    "                entanglement_gate = qml.CZ, \n",
    "                input_init = None,\n",
    "                weight_init = None,\n",
    "                bias_init = None,\n",
    "                measure = two_measure_expval):\n",
    "        super(UQC, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.state_dim = state_dim\n",
    "        self.device = device\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.encoding_type = encoding_type\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.bias_init = bias_init\n",
    "        self.measure = measure\n",
    "        self.input_scaling = None\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "\n",
    "    def generate_circuit(self):\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(self.device, wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(self.device, wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        if self.encoding_type is 'full':\n",
    "            self.weight_shapes = {\n",
    "                \"weights\": (self.n_layers, self.n_qubits, self.state_dim),\n",
    "                \"params\": (self.n_layers, self.n_qubits, 1),\n",
    "                \"bias\": (self.n_layers, self.n_qubits)\n",
    "            }\n",
    "        elif self.encoding_type is 'partial':\n",
    "            self.weight_shapes = {\n",
    "            \"weights\": (self.n_layers, self.n_qubits, int(self.state_dim/self.n_qubits)),\n",
    "            \"params\": (self.n_layers, self.n_qubits, 1),\n",
    "            \"bias\": (self.n_layers, self.n_qubits)\n",
    "            }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"weights\": self.input_init,\n",
    "            \"params\": self.weight_init,\n",
    "            \"bias\": self.bias_init\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, weights, params, bias):\n",
    "\n",
    "            for layer in range(self.n_layers):\n",
    "                for wire in range(self.n_qubits):\n",
    "                    if self.encoding_type is 'full':\n",
    "                        hadamard_product = torch.dot(inputs.clone().detach(), weights[layer][wire])\n",
    "                        angle = hadamard_product + bias[layer][wire]\n",
    "                    elif self.encoding_type is 'partial':\n",
    "                        separate_inputs = np.array_split(inputs,self.n_qubits)\n",
    "                        hadamard_product = torch.dot(separate_inputs[wire], weights[layer][wire])\n",
    "                        angle = hadamard_product + bias[layer][wire]\n",
    "\n",
    "                    qml.RZ(angle, wires=wire)\n",
    "                    \n",
    "                    qml.RY(params[layer][wire][0], wires=wire)\n",
    "                    \n",
    "                if self.entanglement:\n",
    "                    qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(self.qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        Gives inputs to the circuit and outputs the respective output\n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "\n",
    "    def visualize_circuit(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.state_dim)], dtype=torch.float32)\n",
    "        \n",
    "        # Initialize all parameters using the provided initialization methods\n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        circuit = qml.draw_mpl(self.qnode)(inputs, \n",
    "                                        initialized_params[\"weights\"], \n",
    "                                        initialized_params[\"params\"], \n",
    "                                        initialized_params[\"bias\"])\n",
    "\n",
    "\n",
    "    def handle_errors_and_warnings(self):\n",
    "        ''' \n",
    "        Handles the errors and warnings\n",
    "        '''\n",
    "        # Check if the number of layers is valid\n",
    "        if self.n_layers < 1:\n",
    "            raise ValueError(\"Number of layers must be at least 1.\")\n",
    "\n",
    "        # Check if the number of qubits is compatible with the input length\n",
    "        if not isinstance(self.n_qubits, int) or self.n_qubits <= 0:\n",
    "            raise ValueError(\"Number of qubits must be a positive integer.\")\n",
    "\n",
    "        if not isinstance(self.entanglement, bool):\n",
    "            raise TypeError(\"Entanglement flag must be a boolean value.\")\n",
    "        \n",
    "        if self.device not in [\"default_qubit\", \"default_mixed\", \"lightning.qubit\", \"lightning.gpu\"]:\n",
    "            raise Warning(f\"Unknown device '{self.device}' specified. Defaulting to 'default_qubit'.\")\n",
    "            self.device = \"default_qubit\"\n",
    "\n",
    "        if self.diff_method not in [\"best\", \"parameter-shift\", \"adjoint\", \"backprop\"]:\n",
    "            raise Warning(f\"Unknown differentiation method '{self.diff_method}'. Using 'best' method.\")\n",
    "            self.diff_method = \"best\" \n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def info(cls):        \n",
    "        '''\n",
    "        Provides a summary of the UQC class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        Creates a parameterized quantum circuit based on the work in 'Data re-uploading for a universal quantum classifier' by Adrián Pérez-Salinas.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_qubits (int): \n",
    "            Number of qubits used in the quantum circuit.\n",
    "\n",
    "        n_layers (int): \n",
    "            Number of layers in the quantum circuit. Each layer consists of parameterized rotations and entanglement gates.\n",
    "\n",
    "        state_dim (int): \n",
    "            Dimensionality of the state space, determining the size of the weights associated with each qubit.\n",
    "\n",
    "        device (str): \n",
    "            The quantum device to be used for execution, such as 'default.qubit', 'lightning.qubit', etc.\n",
    "\n",
    "        shots (int, optional): \n",
    "            Number of times the circuit gets executed (repeated measurements). If None, the circuit is executed with analytic calculations (no shot noise).\n",
    "\n",
    "        diff_method (str): \n",
    "            Differentiation method used for training the model. Common options include 'best', 'parameter-shift', 'adjoint', and 'backprop'.\n",
    "\n",
    "        encoding_type (str): \n",
    "            Type of encoding used for the input data. Can be 'full' for complete encoding or 'partial' for partial encoding, which changes the shape of weights.\n",
    "\n",
    "        entanglement (bool): \n",
    "            If True, entanglement between qubits is implemented. The entanglement pattern and gate are defined in entanglement_pattern and entanglement_gate, respectively.\n",
    "\n",
    "        entanglement_pattern (str): \n",
    "            Entanglement pattern used in the circuit, such as 'chain', 'ring', 'all_to_all', etc., as defined by qml.broadcast patterns.\n",
    "\n",
    "        entanglement_gate (function): \n",
    "            Quantum gate used for entanglement, such as qml.CZ or qml.CNOT. This gate is applied between qubits according to the specified entanglement pattern.\n",
    "\n",
    "        input_init (function): \n",
    "            Function to initialize the weights of the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.normal_, or a user-defined function.\n",
    "\n",
    "        weight_init (function): \n",
    "            Function to initialize the parameters of the quantum circuit, similar to input_init.\n",
    "\n",
    "        bias_init (function): \n",
    "            Function to initialize the bias terms in the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.zeros_, or a user-defined function.\n",
    "\n",
    "        measure (function): \n",
    "            Measurement function that takes the number of qubits as an argument and returns the measurement result. Common choices are measure_probs, two_measure_expval, or any user-defined measurement function.\n",
    "        \n",
    "        Methods:\n",
    "        --------\n",
    "        generate_circuit(self): \n",
    "            Generates and initializes the quantum circuit based on the parameters.\n",
    "        \n",
    "        forward(self, inputs): \n",
    "            Takes inputs and passes them through the quantum circuit to get the output.\n",
    "\n",
    "        visualize_circuit(self): \n",
    "            Visualizes the generated quantum circuit for the given number of qubits using the initial parameters. Useful for debugging or analyzing the circuit design.\n",
    "\n",
    "        handle_errors_and_warnings(self): \n",
    "            Handles common errors and warnings, such as invalid parameter values, unsupported devices, and incompatible differentiation methods.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfqTutorial(nn.Module):\n",
    "    '''\n",
    "    Creates a parameterized quantum circuit based on the TensorFlow Quantum tutorial in https://www.tensorflow.org/quantum/tutorials/quantum_reinforcement_learning\n",
    "\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, \n",
    "                n_qubits,\n",
    "                n_layers, \n",
    "                device,\n",
    "                shots = None, \n",
    "                diff_method = 'best', \n",
    "                entanglement = True,\n",
    "                entanglement_pattern = \"all_to_all\", \n",
    "                entanglement_gate = qml.CZ, \n",
    "                input_scaling = True, \n",
    "                input_init = None, \n",
    "                weight_init = None, \n",
    "                measure = two_measure_expval):\n",
    "        super(TfqTutorial, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_scaling = input_scaling\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "\n",
    "    def generate_circuit(self):\n",
    "        # Call the error handling function\n",
    "        self.handle_errors_and_warnings()\n",
    "\n",
    "        # Initialize the device\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(self.device, wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(self.device, wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        # Weight initialization\n",
    "        self.weight_shapes = {\n",
    "            \"input_params\": (self.n_layers, self.n_qubits, 1),\n",
    "            \"params\": (self.n_layers + 1, self.n_qubits, 3)\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"input_params\": self.input_init,\n",
    "            \"params\": self.weight_init,\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, params, input_params):\n",
    "            \n",
    "            # Apply layers and entanglement\n",
    "            for layer in range(self.n_layers):\n",
    "                for wire in range(self.n_qubits):\n",
    "                    qml.RX(params[layer][wire][0], wires=wire)\n",
    "                    qml.RY(params[layer][wire][1], wires=wire)\n",
    "                    qml.RZ(params[layer][wire][2], wires=wire)\n",
    "\n",
    "                if self.entanglement:\n",
    "                    qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "                # Input scaling\n",
    "                if self.input_scaling:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RX(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                else:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RX(inputs[wire], wires=wire)\n",
    "                \n",
    "            # Final layer\n",
    "            for wire in range(self.n_qubits):\n",
    "                qml.RX(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "                qml.RZ(params[-1][wire][2], wires=wire)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)  \n",
    "        \n",
    "        return model\n",
    "    \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        Gives inputs to the circuit and outputs the respective output\n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        '''\n",
    "        Draws the circuit\n",
    "        '''\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        circuit = qml.draw_mpl(self.qnode)(inputs, \n",
    "                                        initialized_params[\"params\"], \n",
    "                                        initialized_params[\"input_params\"])\n",
    "\n",
    "\n",
    "    def handle_errors_and_warnings(self):\n",
    "        ''' \n",
    "        Handles the errors and warnings\n",
    "        '''\n",
    "        # Check if the number of layers is valid\n",
    "        if self.n_layers < 1:\n",
    "            raise ValueError(\"Number of layers must be at least 1.\")\n",
    "\n",
    "        # Check if the number of qubits is compatible with the input length\n",
    "        if not isinstance(self.n_qubits, int) or self.n_qubits <= 0:\n",
    "            raise ValueError(\"Number of qubits must be a positive integer.\")\n",
    "\n",
    "        if not isinstance(self.entanglement, bool):\n",
    "            raise TypeError(\"Entanglement flag must be a boolean value.\")\n",
    "        \n",
    "        if self.device not in [\"default_qubit\", \"default_mixed\", \"lightning.qubit\", \"lightning.gpu\"]:\n",
    "            raise Warning(f\"Unknown device '{self.device}' specified. Defaulting to 'default_qubit'.\")\n",
    "            self.device = \"default_qubit\"\n",
    "\n",
    "        if self.diff_method not in [\"best\", \"parameter-shift\", \"adjoint\", \"backprop\"]:\n",
    "            raise Warning(f\"Unknown differentiation method '{self.diff_method}'. Using 'best' method.\")\n",
    "            self.diff_method = \"best\"\n",
    " \n",
    "       \n",
    "    def info(cls):\n",
    "        '''\n",
    "        Provides a summary of the TFQ class, including its parameters and methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        TFQ circuit: A Parameterized Quantum Circuit based on the design in the TensorFlow Quantum tutorial in https://www.tensorflow.org/quantum/tutorials/quantum_reinforcement_learning\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_qubits (int): \n",
    "            Number of qubits used in the quantum circuit.\n",
    "        \n",
    "        n_layers (int): \n",
    "            Number of layers in the quantum circuit. Each layer typically consists of parameterized rotations followed by entanglement gates.\n",
    "        \n",
    "        device (str): \n",
    "            The quantum device used for simulation or execution (e.g., 'default_qubit', 'lightning.qubit').\n",
    "\n",
    "        shots (int, optional): \n",
    "            Number of times the circuit gets executed (repeated measurements). If None, the circuit is executed with analytic calculations (no shot noise).\n",
    "        \n",
    "        diff_method (str): \n",
    "            Differentiation method used for training the model. Common options are 'best', 'parameter-shift', 'backprop', etc.\n",
    "\n",
    "        entanglement (bool):\n",
    "            If True, entanglement between qubits is implemented. The entanglement pattern and gate are defined by `entanglement_pattern` and `entanglement_gate`, respectively.\n",
    "        \n",
    "        entanglement_pattern (str): \n",
    "            Entanglement pattern used in the circuit, such as 'chain', 'ring', 'all_to_all', etc., as defined by qml.broadcast patterns.\n",
    "        \n",
    "        entanglement_gate (function): \n",
    "            Quantum gate used for entanglement, such as qml.CZ or qml.CNOT. This gate is applied between qubits according to the specified entanglement pattern.\n",
    "        \n",
    "        input_scaling (bool): \n",
    "            If True, input parameters are scaled by additional learnable parameters (input_params). The input is multiplied by these parameters before being applied to the qubits.\n",
    "        \n",
    "        input_init (function): \n",
    "            Function to initialize the input scaling parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, or any function defined by the user.\n",
    "        \n",
    "        weight_init (function): \n",
    "            Function to initialize the weights of the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.normal_, or any function defined by the user.\n",
    "        \n",
    "        measure (function): \n",
    "            Measurement function that takes the number of qubits as an argument and returns the measurement result. Common choices are `measure_probs`, `two_measure_expval`, `three_measure_expval`, or any user-defined measurement function.\n",
    "\n",
    "        Methods:\n",
    "        --------\n",
    "        generate_circuit(self): \n",
    "            Generates and initializes the quantum circuit based on the parameters.\n",
    "        \n",
    "        forward(self, inputs): \n",
    "            Takes inputs and passes them through the quantum circuit to get the output.\n",
    "\n",
    "        visualize_circuit(self): \n",
    "            Visualizes the generated quantum circuit for the given number of qubits using the initial parameters. Useful for debugging or analyzing the circuit design.\n",
    "\n",
    "        handle_errors_and_warnings(self): \n",
    "            Handles common errors and warnings, such as invalid parameter values, unsupported devices, and incompatible differentiation methods.\n",
    "            \n",
    "        \"\"\"\n",
    "        print(info_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyPostProcessing(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_qubits, n_actions, policy_type = 'softmax', \n",
    "                 beta_scheduling = False, beta = 1, increase_rate = 0.001, \n",
    "                 output_scaling = False, output_init = torch.nn.init.ones_):\n",
    "        super(PolicyPostProcessing, self).__init__()\n",
    "\n",
    "        '''\n",
    "        Determines the type of policy used based on the arguments:\n",
    "\n",
    "            n_actions(int) = Number of actions\n",
    "            post_processing(str) = Type of policy ('raw_contiguous', 'raw_parity', 'softmax')\n",
    "            beta_scheduling(bool) = Inverse temperature parameter used in softmax (used if set to True)\n",
    "            beta(float) = Beta parameter or inverse temperature (used only for softmax)\n",
    "            increase_rate(float) = Amount added to beta at the end of each episode (used only for softmax)\n",
    "            output_scaling(bool) = Output parameters are used if True\n",
    "            output_init(function) = How the output parameters are initialized\n",
    "            \n",
    "        '''\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_actions = n_actions\n",
    "        self.policy_type = policy_type\n",
    "        self.beta_scheduling = beta_scheduling\n",
    "        self.beta = beta\n",
    "        self.increase_rate = increase_rate\n",
    "        self.output_scaling = output_scaling\n",
    "        self.output_init = output_init\n",
    "\n",
    "        if self.output_scaling == True:\n",
    "            self.output_params = nn.parameter.Parameter(torch.Tensor(self.n_actions), requires_grad=True)\n",
    "            self.output_init(self.output_params)\n",
    "        else:\n",
    "            self.register_parameter('w_input', None)\n",
    "\n",
    "    def forward(self,probs):\n",
    "        if self.policy_type == 'raw_contiguous':\n",
    "            policy = self.raw_contiguous(probs)\n",
    "        elif self.policy_type == 'raw_parity':\n",
    "            policy = self.raw_parity(probs)\n",
    "        elif self.policy_type == 'softmax':\n",
    "            policy = self.softmax(probs)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid post-processing method specified.\")\n",
    "        return policy\n",
    "\n",
    "\n",
    "    def raw_contiguous(self,probs):\n",
    "\n",
    "        log_n_actions = int(np.log2(self.n_actions))\n",
    "        \n",
    "        # Ensure the number of actions does not exceed the number of basis states (determined by n_qubits)\n",
    "        if log_n_actions > self.n_qubits:\n",
    "            raise ValueError('Number of actions exceeds the number of basis states!')\n",
    "\n",
    "        # Split the probabilities in a contiguous manner\n",
    "        probs_split = torch.chunk(probs, self.n_actions)\n",
    "        policy = [torch.sum(prob) for prob in probs_split]\n",
    "        return(torch.stack(policy))\n",
    "\n",
    "\n",
    "    def raw_parity(self,probs):\n",
    "\n",
    "        log_n_actions = int(np.log2(self.n_actions))\n",
    "\n",
    "        # Check if the number of actions is a power of 2\n",
    "        if log_n_actions < 1.0 or not (np.floor(log_n_actions) == np.ceil(log_n_actions)):\n",
    "            raise NotImplementedError('Number of actions needs to be a power of two!')\n",
    "\n",
    "        # Ensure the number of actions does not exceed the number of qubits\n",
    "        if log_n_actions > self.n_qubits:\n",
    "            raise ValueError('Number of actions exceeds number of basis states!')\n",
    "\n",
    "        # Flatten the probability distribution to handle it as a single-dimensional array\n",
    "        if log_n_actions == 1:\n",
    "            summed_tensors = []\n",
    "            even_tensor = probs[::2]  # Elements at even indices\n",
    "            odd_tensor = probs[1::2]  # Elements at odd indices\n",
    "            summed_tensors.append(torch.sum(even_tensor))\n",
    "            summed_tensors.append(torch.sum(odd_tensor))\n",
    "        else:\n",
    "            probs_split = list(torch.chunk(probs, self.n_actions//2))\n",
    "            summed_tensors = []\n",
    "\n",
    "            for tensor in probs_split:\n",
    "                even_tensor = tensor[::2]  # Even indexed elements\n",
    "                odd_tensor = tensor[1::2]  # Odd indexed elements\n",
    "                summed_tensors.append(torch.sum(even_tensor))\n",
    "                summed_tensors.append(torch.sum(odd_tensor))\n",
    "\n",
    "        return torch.stack(summed_tensors)\n",
    "    \n",
    "\n",
    "    def softmax(self, probs):\n",
    "        \n",
    "        if self.output_scaling == True:\n",
    "            probs *= self.output_params\n",
    "\n",
    "        scaled_output = probs * self.beta\n",
    "        softmax_output = F.softmax(scaled_output, dim=0)\n",
    "        return softmax_output\n",
    "    \n",
    "    \n",
    "    def beta_schedule(self):\n",
    "        if self.beta_scheduling == True and self.policy_type == 'softmax':\n",
    "            self.beta += self.increase_rate\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def info(cls):\n",
    "        '''\n",
    "        Provides a summary of the PolicyType class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        PolicyType Class:\n",
    "\n",
    "        Responsible for processing the outputs of the variational quantum circuit.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_actions (int): \n",
    "            Number of actions available for the agent to choose from.\n",
    "        \n",
    "        policy_type (str): \n",
    "            Type of policy applied to the probability distribution:\n",
    "            - 'raw_contiguous': Sums up contiguous chunks of probabilities.\n",
    "            - 'raw_parity': Sums up probabilities based on parity.\n",
    "            - 'softmax': Applies a softmax function to the expectation values.\n",
    "\n",
    "        beta_scheduling (bool): \n",
    "            If True, updates the inverse temperature parameter (beta) after each episode. Used only for the softmax policy.\n",
    "        \n",
    "        beta (float): \n",
    "            Inverse temperature parameter used for scaling probabilities in the softmax policy.\n",
    "        \n",
    "        increase_rate (float): \n",
    "            Amount added to beta at the end of each episode, if beta_scheduling is True.\n",
    "        \n",
    "        output_scaling (bool): \n",
    "            If True, scales the output probabilities by learnable parameters.\n",
    "        \n",
    "        output_init (function): \n",
    "            Initialization function for output parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, etc.\n",
    "        \n",
    "        Methods:\n",
    "        -------\n",
    "        forward(self, probs):\n",
    "            Selects an action based on the chosen post_processing method.\n",
    "        \n",
    "        raw_contiguous(self, probs):\n",
    "            Sums up contiguous chunks of probabilities and returns the probability of each action.\n",
    "        \n",
    "        raw_parity(self, probs):\n",
    "            Sums up probabilities based on parity and returns the probability of each action.\n",
    "        \n",
    "        softmax(self, probs):\n",
    "            Applies a softmax function to the scaled probabilities and returns the probability of each action.\n",
    "        \n",
    "        beta_schedule(self):\n",
    "            Updates the beta parameter if beta_scheduling is True. Only applicable for the softmax method.\n",
    "        \"\"\"\n",
    "        print(info_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumPolicy(nn.Module):\n",
    "    \n",
    "    def __init__(self, circuit, post_processing):\n",
    "        super(QuantumPolicy, self).__init__()\n",
    "        self.circuit = circuit\n",
    "        self.post_processing = post_processing\n",
    "\n",
    "    def sample(self, inputs):\n",
    "        '''\n",
    "        Samples an action from the action probability distribution\n",
    "        '''\n",
    "        policy = self.forward(inputs)\n",
    "        dist = torch.distributions.Categorical(policy)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), policy\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Input state is fed to the circuit - its output is then fed to the post processing \n",
    "        '''\n",
    "        probs = self.circuit.forward(inputs)\n",
    "        probs_processed = self.post_processing.forward(probs)\n",
    "        return probs_processed\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        '''\n",
    "        Returns the values of each set of parameters\n",
    "        '''\n",
    "        parameters = []\n",
    "\n",
    "        circuit_parameters = [param.clone().detach().numpy().flatten() for param in self.circuit.parameters()]\n",
    "        parameters.extend(circuit_parameters)\n",
    "\n",
    "        policy_parameters = [param.clone().detach().numpy().flatten() for param in self.post_processing.parameters()]\n",
    "        parameters.extend(policy_parameters)\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    def get_gradients(self):\n",
    "        '''\n",
    "        Returns the gradient values of each set of parameters\n",
    "        '''\n",
    "        gradients = []\n",
    "\n",
    "        # Get gradients from circuit parameters\n",
    "        circuit_gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for param in self.circuit.parameters()]\n",
    "        gradients.extend(circuit_gradients)\n",
    "\n",
    "        # Get gradients from policy parameters\n",
    "        policy_gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for param in self.post_processing.parameters()]\n",
    "        gradients.extend(policy_gradients)\n",
    "\n",
    "        return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceAgent:\n",
    "\n",
    "    def __init__(self, \n",
    "                policy, \n",
    "                policy_optimizer, \n",
    "                env_name, \n",
    "                n_episodes, \n",
    "                max_t, \n",
    "                gamma = 0.98, \n",
    "                baseline = True, \n",
    "                batch_size = 10, \n",
    "                normalize = False,\n",
    "                print_every = 100, \n",
    "                verbose = 1):\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy = policy.to(self.device)\n",
    "        self.policy_optimizer = policy_optimizer\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env_name = env_name\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.baseline = baseline\n",
    "        self.batch_size = batch_size\n",
    "        self.normalize = normalize\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.solved = False\n",
    "        self.scores = deque(maxlen=100)\n",
    "        self.batch_log_probs = []\n",
    "        self.batch_rewards = []\n",
    "        self.loss = torch.tensor(0.0)\n",
    "\n",
    "    def train(self, run_count=None, rundate = None, path=None, tensorboard=False):\n",
    "\n",
    "        # Creates data saving files if specified\n",
    "        if run_count is not None and path is not None:\n",
    "            data_path = create_directory(os.path.join(path, 'data'))\n",
    "            env_folder = create_directory(os.path.join(data_path, self.env_name))\n",
    "            experiment_folder_name = f\"{self.policy.circuit.__class__.__name__}_{self.policy.circuit.n_qubits}qubits_{self.policy.circuit.n_layers}layer_{rundate}\"\n",
    "            experiment_path = create_directory(os.path.join(env_folder, experiment_folder_name))\n",
    "            run_path = create_directory(os.path.join(experiment_path, f'run_{str(run_count)}'))\n",
    "            self.save_agent_data(experiment_path)\n",
    "        \n",
    "\n",
    "        # Create TensorBoard session if specified\n",
    "        if tensorboard:\n",
    "            writer = SummaryWriter(log_dir=run_path)\n",
    "\n",
    "\n",
    "        for i in range(1, self.n_episodes + 1):\n",
    "            start_time = time.time()\n",
    "            # Get episode\n",
    "            self.get_trajectory()\n",
    "\n",
    "            # Check if environment is solved\n",
    "            self.env_solved_verification()\n",
    "\n",
    "            # Update parameters if the batch is full and environment is not solved\n",
    "            if i % self.batch_size == 0 and self.solved is False:\n",
    "                self.update_policy()\n",
    "                self.policy.post_processing.beta_schedule()\n",
    " \n",
    "            end_time = time.time()\n",
    "\n",
    "            # Calculate the runtime\n",
    "            self.runtime = end_time - start_time\n",
    "\n",
    "            # Write in the TensorBoard session\n",
    "            if tensorboard:\n",
    "                self.writer_function(writer, i)\n",
    "\n",
    "            # Save the episode data\n",
    "            if run_count is not None and path is not None:\n",
    "                self.save_data(run_path, i)\n",
    "\n",
    "            # Print out episode data\n",
    "            if self.verbose == 1:\n",
    "                print('Episode {} reward: {:.2f}\\t Solved: {}'.format(i, self.scores[-1]),self.solved)\n",
    "            if self.verbose == 2:\n",
    "                print('Episode {} reward: {:.2f}\\t Solved: {}\\t Runtime: {:.2f}\\t Loss: {:.2f}'.format(i, self.scores[-1], self.runtime, self.loss))\n",
    "            if i % self.print_every == 0:\n",
    "                print('Last {} Episodes average reward: {:.2f}\\t'.format(len(self.scores),np.mean(self.scores)))\n",
    "\n",
    "        # Close TensorBoard session\n",
    "        if tensorboard:\n",
    "            writer.close()\n",
    "\n",
    "    def get_trajectory(self):\n",
    "        '''\n",
    "        Gets a trajectory based on the running policy until it runs out of bounds or achieves maximum reward of an episode\n",
    "        '''\n",
    "        # Get an episode trajectory\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()[0]\n",
    "        for t in range(self.max_t):\n",
    "            state_tensor = torch.tensor(self.normalize_state(state)).float().to(self.device)\n",
    "            action, log_prob, _ = self.policy.sample(state_tensor)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            \n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        self.scores.append(sum(self.rewards))\n",
    "        self.batch_log_probs.append(self.saved_log_probs)\n",
    "        self.batch_rewards.append(self.rewards)\n",
    "\n",
    "\n",
    "        # Clear data\n",
    "        del self.saved_log_probs, self.rewards\n",
    "        if self.solved is True:\n",
    "            self.batch_log_probs = []\n",
    "            self.batch_rewards = []\n",
    "      \n",
    "\n",
    "    def update_policy(self):\n",
    "        '''\n",
    "        Computes the loss and gradients and updates the policy via gradient methods\n",
    "        '''\n",
    "        # Discounting of the rewards\n",
    "        all_returns = []\n",
    "        for batch in self.batch_rewards:\n",
    "            R = 0\n",
    "            ep_return = []\n",
    "            for r in reversed(batch):\n",
    "                R = r + self.gamma * R\n",
    "                ep_return.insert(0, R)\n",
    "            \n",
    "            ep_return = torch.tensor(ep_return).to(self.device)\n",
    "            ep_return = (ep_return - ep_return.mean()) / (ep_return.std() + 1e-8)\n",
    "            all_returns.append(ep_return)\n",
    "\n",
    "\n",
    "        # Calculate the policy loss\n",
    "        policy_loss = []     \n",
    "        if self.baseline:\n",
    "            baseline = np.mean([sum(lst) for lst in all_returns])\n",
    "            for log_probs, ep_returns in zip(self.batch_log_probs, all_returns):\n",
    "                for log_prob, ret in zip(log_probs, ep_returns):\n",
    "                    advantage = ret - baseline \n",
    "                    policy_loss.append(-log_prob * advantage)\n",
    "        else:\n",
    "            for log_probs, ep_returns in zip(self.batch_log_probs, all_returns):\n",
    "                for log_prob, ret in zip(log_probs, ep_returns):\n",
    "                    policy_loss.append(-log_prob * ret)\n",
    "        if policy_loss:\n",
    "            policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "            self.loss = torch.cat(policy_unsqueezed).mean()\n",
    "\n",
    "\n",
    "        # Compute the gradients \n",
    "        self.policy_optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "\n",
    "        # Clear old data\n",
    "        del all_returns\n",
    "        del policy_loss\n",
    "        del policy_unsqueezed \n",
    "        self.batch_log_probs = []\n",
    "        self.batch_rewards = []\n",
    "\n",
    "\n",
    "    def normalize_state(self, state):\n",
    "        '''\n",
    "        Processes the input state by reducing its dimensionality and normalizing it\n",
    "        '''\n",
    "        # State-space reduction for the Acrobot\n",
    "        if self.env_name in ('Acrobot-v0', 'Acrobot-v1'):\n",
    "            theta1 = np.arccos(state[0])\n",
    "            theta2 = np.arccos(state[2])\n",
    "            state = [theta1,theta2,state[4],state[5]]\n",
    "\n",
    "\n",
    "        # Normalize each feature by the maximum absolute value at each step\n",
    "        if self.normalize == True:\n",
    "            max_abs_value = max(abs(value) for value in state)\n",
    "            state = np.array([value / max_abs_value for value in state])\n",
    "        \n",
    "        \n",
    "        return state\n",
    "    \n",
    "\n",
    "    def env_solved_verification(self):\n",
    "        '''\n",
    "        Checks if the environment is solved\n",
    "        '''\n",
    "        # Acrobot-v1\n",
    "        if self.env_name in ('Acrobot-v1'):\n",
    "            if np.mean(self.scores) > -125:\n",
    "                self.solved = True\n",
    "        \n",
    "        # CartPole-v0 and CartPole-v1\n",
    "        elif self.env_name in ('CartPole-v0','CartPole-v1'):\n",
    "            if np.mean(self.scores) > self.env.spec.reward_threshold:\n",
    "                self.solved = True\n",
    "        \n",
    "        else:              \n",
    "            warnings.warn(f\"No reward threshold defined for environment {self.env_name}. \"\n",
    "                          \"Consider specifying a solved condition explicitly.\",\n",
    "                          UserWarning\n",
    "            )\n",
    "\n",
    "\n",
    "    def save_agent_data(self,main_path):\n",
    "        '''\n",
    "        Stores the most relevant model parameters into a .json file\n",
    "        '''\n",
    "        agent_variables = {\n",
    "            \"Model\": self.policy.circuit.__class__.__name__,\n",
    "            \"Number of Qubits\": self.policy.circuit.n_qubits,\n",
    "            \"Number of Layers\": self.policy.circuit.n_layers,\n",
    "            \"Shots\": self.policy.circuit.shots,\n",
    "            \"Differentiation Method\": self.policy.circuit.diff_method,\n",
    "            \"Entanglement\": self.policy.circuit.entanglement,\n",
    "            \"Entanglement Pattern\": self.policy.circuit.entanglement_pattern,\n",
    "            \"Entanglement Gate\": get_function_representation(self.policy.circuit.entanglement_gate),\n",
    "            \"Input Scaling\": self.policy.circuit.input_scaling,\n",
    "            \"Input Initialization\": get_function_representation(self.policy.circuit.input_init),\n",
    "            \"Weight Initialization\": get_function_representation(self.policy.circuit.weight_init),\n",
    "            \"Measure\": get_function_representation(self.policy.circuit.measure),\n",
    "            \"Policy Post Processing\": self.policy.post_processing.policy_type,\n",
    "            \"Optimizers\": str(self.policy_optimizer),\n",
    "            \"Environment\": str(self.env_name),\n",
    "            \"Gamma (discounting factor)\": self.gamma,\n",
    "            \"Batch Size\": str(self.batch_size),\n",
    "            \"Baseline\": str(self.baseline)\n",
    "        }\n",
    "        if self.policy.post_processing.policy_type in ['softmax', 'softmax_probs']:\n",
    "            agent_variables.update({\n",
    "                \"Softmax scheduling (in case policy is softmax)\": f\"{self.policy.post_processing.beta_scheduling}. Starting beta: {self.policy.post_processing.beta}. Increase rate: {self.policy.post_processing.increase_rate}\",\n",
    "                \"Softmax output scaling (in case policy is softmax)\": f\"{self.policy.post_processing.output_scaling}. Output Initialization: {get_function_representation(self.policy.post_processing.output_init)}\",\n",
    "            })\n",
    "        with open(os.path.join(main_path, \"agent_characteristics.json\"), \"w\") as f:\n",
    "            json.dump(agent_variables, f, indent=4)\n",
    "    \n",
    "\n",
    "    def save_data(self, run_path, iteration):\n",
    "        '''\n",
    "        Saves the data into a .npz file for each episode\n",
    "        '''\n",
    "        data_file = os.path.join(run_path, \"run_data.npz\")\n",
    "        \n",
    "        # Load existing data if the file exists\n",
    "        if os.path.exists(data_file):\n",
    "            data = np.load(data_file, allow_pickle=True)\n",
    "            old_episode_reward = data['episode_reward'].tolist()\n",
    "            old_loss = data['loss'].tolist()\n",
    "            old_runtime = data['runtime'].tolist()\n",
    "            old_params_gradients = data['params_gradients'].tolist()\n",
    "        else:\n",
    "            old_episode_reward = []\n",
    "            old_loss = []\n",
    "            old_runtime = []\n",
    "            old_params_gradients = []\n",
    "\n",
    "        # Add episode reward and runtime\n",
    "        old_episode_reward.append(self.scores[-1])\n",
    "        old_runtime.append(self.runtime)\n",
    "\n",
    "\n",
    "        # Stores the loss and parameter gradients when batch is full\n",
    "        current_episode_gradients = []\n",
    "        if iteration % self.batch_size == 0 and iteration != 0 and self.solved is False:\n",
    "            old_loss.append(self.loss.item())\n",
    "            for name, param in self.policy.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    grad_array = param.grad.cpu().numpy().flatten()\n",
    "                    current_episode_gradients.append(grad_array)\n",
    "            old_params_gradients.append(current_episode_gradients)\n",
    "            \n",
    "        # Save data to .npz file\n",
    "        np.savez_compressed(data_file,\n",
    "                            episode_reward=np.array(old_episode_reward),\n",
    "                            loss=np.array(old_loss),\n",
    "                            runtime=np.array(old_runtime),\n",
    "                            params_gradients=np.array(old_params_gradients, dtype=object))  # Use dtype=object to handle lists\n",
    "\n",
    "        # Clear old data lists to free up memory\n",
    "        del old_episode_reward[:]\n",
    "        del old_loss[:]\n",
    "        del old_runtime[:]\n",
    "        del old_params_gradients[:]\n",
    "\n",
    "\n",
    "    def writer_function(self, writer, iteration):\n",
    "        '''\n",
    "        Stores data into a tensorboard session\n",
    "        '''\n",
    "        writer.add_scalar(\"Episode Reward\", self.scores[-1], global_step=iteration)\n",
    "        writer.add_scalar(\"Runtime\", self.runtime, global_step=iteration)\n",
    "        writer.add_scalar(\"Loss\", self.loss.item(), global_step=iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jerbi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 6\n",
    "n_layers = 4\n",
    "shots = None\n",
    "diff_method = 'best' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = torch.nn.init.ones_\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).normal_(-np.pi, np.pi)\n",
    "policy_circuit_measure = three_measure_expval\n",
    "policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "policy_circuit.visualize_circuit()\n",
    "#policy_circuit.circuit_spectrum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 5\n",
    "device = 'default.qubit'\n",
    "shots = None\n",
    "diff_method = 'backprop' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "policy_circuit_measure = measure_probs\n",
    "policy_circuit = JerbiModel(n_qubits, n_layers, device, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "\n",
    "n_actions = 2\n",
    "post_processing = 'raw_parity'\n",
    "beta_scheduling = False\n",
    "beta = 1\n",
    "increase_rate = 0.0005\n",
    "output_scaling = True\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_post_process = PolicyPostProcessing(n_qubits, n_actions, \n",
    "                         post_processing, \n",
    "                         beta_scheduling, \n",
    "                         beta, increase_rate, \n",
    "                         output_scaling, \n",
    "                         output_init)\n",
    "\n",
    "policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "policy_lr_list= [0.1, 0.01, 0.1]  # [weights, input_weights, output_weights]\n",
    "policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "n_episodes = 1000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 100\n",
    "verbose = 1\n",
    "baseline = True\n",
    "batch_size = 10\n",
    "reinforce_update = ReinforceAgent(policy, \n",
    "                                  policy_optimizer, \n",
    "                                  env_name, \n",
    "                                  n_episodes, \n",
    "                                  max_t, \n",
    "                                  gamma, \n",
    "                                  baseline, \n",
    "                                  batch_size, \n",
    "                                  print_every, \n",
    "                                  verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 5\n",
    "n_layers = 5\n",
    "state_dim = 4\n",
    "shots = None\n",
    "diff_method = 'best' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "entanglement_training = False\n",
    "entanglement_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "input_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "bias_init = torch.nn.init.zeros_\n",
    "policy_circuit_measure = two_measure_expval\n",
    "policy_circuit = UQC_FullEnc(n_qubits,\n",
    "                            n_layers, \n",
    "                            state_dim, \n",
    "                            shots, \n",
    "                            diff_method, \n",
    "                            entanglement, \n",
    "                            entanglement_pattern, \n",
    "                            entanglement_gate, \n",
    "                            entanglement_training,\n",
    "                            entanglement_init,\n",
    "                            weight_init,\n",
    "                            weight_init,\n",
    "                            bias_init, \n",
    "                            policy_circuit_measure)\n",
    "\n",
    "policy_circuit.visualize_circuit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 5\n",
    "state_dim = 4\n",
    "shots = None\n",
    "diff_method = 'adjoint' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "bias_init = torch.nn.init.zeros_\n",
    "policy_circuit_measure = three_measure_expval\n",
    "policy_circuit = UQC_FullEnc(n_qubits,\n",
    "                            n_layers, \n",
    "                            state_dim, \n",
    "                            shots, \n",
    "                            diff_method, \n",
    "                            entanglement, \n",
    "                            entanglement_pattern, \n",
    "                            entanglement_gate, \n",
    "                            input_init,\n",
    "                            weight_init,\n",
    "                            bias_init, \n",
    "                            policy_circuit_measure)\n",
    "\n",
    "n_actions = 3\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = False\n",
    "beta = 1\n",
    "increase_rate = 0.0005\n",
    "output_scaling = True\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_post_process = PolicyPostProcessing( n_qubits,\n",
    "                                            n_actions, \n",
    "                                            post_processing, \n",
    "                                            beta_scheduling, \n",
    "                                            beta, increase_rate, \n",
    "                                            output_scaling, \n",
    "                                            output_init)\n",
    "\n",
    "policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [input_weights, weights, bias, enttanglement_weights, output_weights]\n",
    "policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "env_name = 'Acrobot-v1'\n",
    "n_episodes = 1000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 100\n",
    "verbose = 1\n",
    "baseline = True\n",
    "batch_size = 10\n",
    "normalize = True\n",
    "reinforce_update = ReinforceAgent(policy, \n",
    "                                  policy_optimizer, \n",
    "                                  env_name, \n",
    "                                  n_episodes, \n",
    "                                  max_t, \n",
    "                                  gamma, \n",
    "                                  baseline, \n",
    "                                  batch_size, \n",
    "                                  normalize,\n",
    "                                  print_every, \n",
    "                                  verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 5\n",
    "shots = None\n",
    "diff_method = 'adjoint' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"ring\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = torch.nn.init.ones_\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval\n",
    "policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "\n",
    "n_actions = 2\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = False\n",
    "beta = 1\n",
    "increase_rate = 0.0005\n",
    "output_scaling = True\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                         post_processing, \n",
    "                         beta_scheduling, \n",
    "                         beta, increase_rate, \n",
    "                         output_scaling, \n",
    "                         output_init)\n",
    "\n",
    "policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "n_episodes = 1000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 100\n",
    "verbose = 1\n",
    "baseline = True\n",
    "batch_size = 10\n",
    "reinforce_update = ReinforceAgent(policy, \n",
    "                                  policy_optimizer, \n",
    "                                  env_name, \n",
    "                                  n_episodes, \n",
    "                                  max_t, \n",
    "                                  gamma, \n",
    "                                  baseline, \n",
    "                                  batch_size, \n",
    "                                  print_every, \n",
    "                                  verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    device = 'default.qubit'\n",
    "    shots = None\n",
    "    diff_method = 'backprop' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    bias_init = torch.nn.init.zeros_\n",
    "    policy_circuit_measure = measure_probs\n",
    "    policy_circuit = UQC_FullEnc(n_qubits,\n",
    "                                n_layers, \n",
    "                                state_dim, \n",
    "                                device,\n",
    "                                shots, \n",
    "                                diff_method, \n",
    "                                entanglement, \n",
    "                                entanglement_pattern, \n",
    "                                entanglement_gate, \n",
    "                                input_init,\n",
    "                                weight_init,\n",
    "                                bias_init,\n",
    "                                policy_circuit_measure)\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_contiguous'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing( n_qubits,\n",
    "                                                n_actions, \n",
    "                                                post_processing, \n",
    "                                                beta_scheduling, \n",
    "                                                beta, increase_rate, \n",
    "                                                output_scaling, \n",
    "                                                output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer = create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = False\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 3   \n",
    "\n",
    "    for run_index in range(2):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    device = 'default.qubit'\n",
    "    shots = None\n",
    "    diff_method = 'backprop' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    bias_init = torch.nn.init.zeros_\n",
    "    policy_circuit_measure = measure_probs\n",
    "    policy_circuit = UQC_FullEnc(n_qubits,\n",
    "                                n_layers, \n",
    "                                state_dim, \n",
    "                                device,\n",
    "                                shots, \n",
    "                                diff_method, \n",
    "                                entanglement, \n",
    "                                entanglement_pattern, \n",
    "                                entanglement_gate, \n",
    "                                input_init,\n",
    "                                weight_init,\n",
    "                                bias_init,\n",
    "                                policy_circuit_measure)\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_parity'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing( n_qubits,\n",
    "                                                n_actions, \n",
    "                                                post_processing, \n",
    "                                                beta_scheduling, \n",
    "                                                beta, increase_rate, \n",
    "                                                output_scaling, \n",
    "                                                output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer = create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = False\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 3  \n",
    "\n",
    "    for run_index in range(2):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jerbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    device = 'default.qubit'\n",
    "    shots = None\n",
    "    diff_method = 'backprop' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = measure_probs\n",
    "    policy_circuit = JerbiModel(n_qubits, n_layers,device, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_contiguous'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_qubits, n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.1, 0.01, 0.1]  # [input_weights, weight, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 4\n",
    "\n",
    "    for run_index in range(2):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    device = 'default.qubit'\n",
    "    shots = None\n",
    "    diff_method = 'backprop' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = measure_probs\n",
    "    policy_circuit = JerbiModel(n_qubits, n_layers,device, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_parity'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_qubits, n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.1, 0.01, 0.1]  # [input_weights, weight, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 2\n",
    "\n",
    "    for run_index in range(2):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    device = 'default.qubit'\n",
    "    shots = None\n",
    "    diff_method = 'backprop' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = measure_probs\n",
    "    policy_circuit = TfqTutorial(n_qubits, n_layers,device, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_contiguous'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_qubits, n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.1, 0.01, 0.1]  # [input_weights, weight, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 2\n",
    "\n",
    "    for run_index in range(2):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    device = 'default.qubit'\n",
    "    shots = None\n",
    "    diff_method = 'backprop' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = measure_probs\n",
    "    policy_circuit = TfqTutorial(n_qubits, n_layers,device, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_parity'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_qubits, n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.1, 0.01, 0.1]  # [input_weights, weight, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 2\n",
    "\n",
    "    for run_index in range(2):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acrobot tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jerbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    device = 'default.qubit'\n",
    "    shots = None\n",
    "    diff_method = 'backprop' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = measure_probs\n",
    "    policy_circuit = JerbiModel(n_qubits, n_layers, device, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 3\n",
    "    post_processing = 'raw_contiguous'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_qubits,n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.1, 0.01, 0.1]  # [input_weights, weight, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'Acrobot-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 1\n",
    "\n",
    "    for run_index in range(2):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    device = 'lightning.qubit'\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    bias_init = torch.nn.init.zeros_\n",
    "    policy_circuit_measure = three_measure_expval\n",
    "    policy_circuit = UQC_FullEnc(n_qubits,\n",
    "                                n_layers, \n",
    "                                state_dim, \n",
    "                                device,\n",
    "                                shots, \n",
    "                                diff_method, \n",
    "                                entanglement, \n",
    "                                entanglement_pattern, \n",
    "                                entanglement_gate, \n",
    "                                input_init,\n",
    "                                weight_init,\n",
    "                                bias_init,\n",
    "                                policy_circuit_measure)\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 3\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing( n_qubits,\n",
    "                                                n_actions, \n",
    "                                                post_processing, \n",
    "                                                beta_scheduling, \n",
    "                                                beta, increase_rate, \n",
    "                                                output_scaling, \n",
    "                                                output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer = create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'Acrobot-v1'\n",
    "    n_episodes = 20\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 4  \n",
    "\n",
    "    for run_index in range(2):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 10\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    device = 'lightning.qubit'\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    bias_init = torch.nn.init.zeros_\n",
    "    policy_circuit_measure = three_measure_expval\n",
    "    policy_circuit = UQC_FullEnc(n_qubits,\n",
    "                                n_layers, \n",
    "                                state_dim, \n",
    "                                device,\n",
    "                                shots, \n",
    "                                diff_method, \n",
    "                                entanglement, \n",
    "                                entanglement_pattern, \n",
    "                                entanglement_gate, \n",
    "                                input_init,\n",
    "                                weight_init,\n",
    "                                bias_init,\n",
    "                                policy_circuit_measure)\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 3\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing( n_qubits,\n",
    "                                                n_actions, \n",
    "                                                post_processing, \n",
    "                                                beta_scheduling, \n",
    "                                                beta, increase_rate, \n",
    "                                                output_scaling, \n",
    "                                                output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer = create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'Acrobot-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 4   \n",
    "\n",
    "    for run_index in range(2):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    device = 'default.qubit'\n",
    "    shots = None\n",
    "    diff_method = 'backprop' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = measure_probs\n",
    "    policy_circuit = TfqTutorial(n_qubits, n_layers, device, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 3\n",
    "    post_processing = 'raw_contiguous'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_qubits,n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.1, 0.01, 0.1]  # [input_weights, weight, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'Acrobot-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 2\n",
    "\n",
    "    for run_index in range(1):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn Off PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(100)\n",
    "\n",
    "import os\n",
    "os.system(\"shutdown /s /t 1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
