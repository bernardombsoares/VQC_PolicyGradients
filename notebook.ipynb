{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pennylane as qml \n",
    "import torch\n",
    "\n",
    "def create_circuit(n_qubits,n_layers=None,circ = \"simplified_two_design\",fim=False, shots=None):\n",
    "\n",
    "    dev = qml.device(\"default.qubit.torch\", wires=n_qubits, shots=shots)\n",
    "\n",
    "    def RZRY(params):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        #qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        #qml.AngleEmbedding(params,wires=range(n_qubits))\n",
    "        for q in range(n_qubits):\n",
    "            qml.Hadamard(wires=q)\n",
    "\n",
    "        for w in range(n_layers): \n",
    "            for q in range(n_qubits):\n",
    "                index = w * (2*n_qubits) + q * 2\n",
    "                qml.RZ(params[index],wires=q)\n",
    "                qml.RY(params[index + 1],wires=q)\n",
    "        \n",
    "        qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        return qml.probs(wires=range(n_qubits))\n",
    "\n",
    "    def S2D(init_params,params,measurement_qubits=0,prod_approx=False):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        \n",
    "        #qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        if not prod_approx:\n",
    "            return qml.probs(wires=list(range(measurement_qubits)))\n",
    "        else:\n",
    "            return [qml.probs(i) for i in range(measurement_qubits)]\n",
    "\n",
    "    def SU(params):\n",
    "        qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        \n",
    "        ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "        for i in range(2,n_qubits):\n",
    "            ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "        return qml.expval(ZZ)\n",
    "    \n",
    "    def simmpleRZRY(params,cnots=True):\n",
    "        qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RZ, wires=range(n_qubits), pattern=\"single\", parameters=params[0])\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params[1])\n",
    "        if cnots:\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "            return qml.expval(qml.PauliZ(n_qubits-1))\n",
    "        else:\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "            return qml.expval(ZZ)\n",
    "        \n",
    "    def RY(params,y=True,probs=False,prod=False, entanglement=None):\n",
    "        #qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params)\n",
    "        #qml.broadcast(qml.CZ, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "\n",
    "        if entanglement==\"all_to_all\":\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        if y==True:\n",
    "            #YY = qml.operation.Tensor(qml.PauliY(0), qml.PauliY(1))\n",
    "            YY = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                #YY = qml.operation.Tensor(YY, qml.PauliY(i))\n",
    "                YY.append(qml.PauliZ(i))\n",
    "            \n",
    "            #return [qml.expval(i) for i in YY]\n",
    "            return qml.expval(YY)\n",
    "\n",
    "        elif probs==False:\n",
    "\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            #ZZ = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))        \n",
    "                #ZZ.append(qml.PauliZ(i))        \n",
    "\n",
    "            #return [qml.expval(i) for i in ZZ]\n",
    "            return qml.expval(ZZ)\n",
    "\n",
    "        else:\n",
    "            if prod:\n",
    "                return [qml.probs(i) for i in range(n_qubits)]\n",
    "            else:\n",
    "                return qml.probs(wires=range(n_qubits))\n",
    "            \n",
    "        \n",
    "        \n",
    "    def GHZ(params,measurement_qubits=0):\n",
    "        qml.RY(params,wires=0)\n",
    "        qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "\n",
    "    def random_product_state(params,gate_sequence=None):\n",
    "                \n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(np.pi / 4, wires=i)\n",
    "\n",
    "        for ll in range(len(params)):\n",
    "\n",
    "            for i in range(n_qubits):\n",
    "                gate_sequence[\"{}{}\".format(ll,i)](params[ll][i], wires=i)\n",
    "\n",
    "            #for i in range(n_qubits - 1):\n",
    "                #qml.CZ(wires=[i, i + 1])\n",
    "    def SEL(params, measurement_qubits=0):\n",
    "        qml.StronglyEntanglingLayers(params, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    def RL(params, measurement_qubits=0):\n",
    "        qml.RandomLayers(params, ratio_imprim=0.8 ,imprimitive=qml.CZ, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    if circ == \"rzry\":\n",
    "        qcircuit = RZRY\n",
    "    elif circ == \"simplified_two_design\":\n",
    "        qcircuit = S2D\n",
    "    elif circ == \"special_unitary\":\n",
    "        qcircuit = SU\n",
    "    elif circ == \"simpleRZRY\":\n",
    "        qcircuit = simmpleRZRY\n",
    "    elif circ == \"RY\":\n",
    "        qcircuit = RY\n",
    "    elif circ == \"ghz\":\n",
    "        qcircuit = GHZ\n",
    "    elif circ == \"random_product_state\":\n",
    "        qcircuit = random_product_state\n",
    "    elif circ == \"SEL\":\n",
    "        qcircuit = SEL\n",
    "    elif circ == \"RL\":\n",
    "        qcircuit = RL\n",
    "    if not fim:\n",
    "        circuit = qml.QNode(qcircuit, dev,interface=\"torch\", diff_method=\"backprop\")\n",
    "    else:\n",
    "        circuit = qml.QNode(qcircuit, dev)\n",
    "\n",
    "    return circuit\n",
    "\n",
    "def compute_gradient(log_prob, w):\n",
    "    \"\"\"Compute gradient of the log probability with respect to weights.\n",
    "    \n",
    "    Args:\n",
    "    - log_prob (torch.Tensor): The log probability tensor.\n",
    "    - w (torch.Tensor): The weights tensor, with requires_grad=True.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The gradient of log_prob with respect to w, flattened.\n",
    "    \"\"\"\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()\n",
    "    log_prob.backward(retain_graph=True)\n",
    "    \n",
    "    if w.grad is None:\n",
    "        raise ValueError(\"The gradient for the given log_prob with respect to w is None.\")\n",
    "    \n",
    "    return w.grad.view(-1).detach().numpy()\n",
    "\n",
    "def policy(probs, policy_type=\"contiguous-like\", n_actions=2, n_qubits=1):\n",
    "\n",
    "    if policy_type == \"contiguous-like\":\n",
    "        return probs\n",
    "    elif policy_type == \"parity-like\":\n",
    "        policy = torch.zeros(n_actions)\n",
    "        for i in range(len(probs)):\n",
    "            a=[]\n",
    "            for m in range(int(np.log2(n_actions))):\n",
    "                if m==0:    \n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)\n",
    "                else:\n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)[:-m]\n",
    "                \n",
    "                a.append(bitstring.count(\"1\") % 2)\n",
    "            policy[int(\"\".join(str(x) for x in a),2)] += probs[i]\n",
    "\n",
    "        return policy    \n",
    "    \n",
    "def compute_policy_and_gradient(args):\n",
    "    n_qubits, shapes, type , n_actions, policy_type, clamp = args\n",
    "\n",
    "    if policy_type == \"parity-like\":\n",
    "        measure_qubits = n_qubits\n",
    "    else:\n",
    "        measure_qubits = int(np.log2(n_actions))\n",
    "\n",
    "    qc = create_circuit(n_qubits, circ=type, fim=False, shots=None)\n",
    "\n",
    "    if type == \"simplified_two_design\":\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_init = torch.tensor(weights[0], requires_grad=False)\n",
    "        weights_tensor_params = torch.tensor(weights[1], requires_grad=True)\n",
    "        \n",
    "        probs = qc(weights_tensor_init,weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    else:\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_params = torch.tensor(weights, requires_grad=True)\n",
    "\n",
    "        probs = qc(weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    pi = policy(probs, policy_type=policy_type, n_actions=n_actions, n_qubits=n_qubits)\n",
    "    if clamp is not None:\n",
    "        pi = torch.clamp(pi, clamp, 1)\n",
    "\n",
    "    dist = torch.distributions.Categorical(probs=pi)\n",
    "    \n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    gradient_no_clamp = np.linalg.norm(compute_gradient(log_prob, weights_tensor_params), 2)\n",
    "    return gradient_no_clamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, env, n_episodes=1000, max_t=1000, gamma=1.0, print_every=5):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    average_scores = []\n",
    "    runtime_sum = 0\n",
    "    for e in range(1, n_episodes):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Sample the action from current policy\n",
    "            if t==0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = policy.sample(state_tensor)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # Total expected reward\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "    #standardized returns\n",
    "        R=0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0,R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + np.finfo(np.float32).eps)\n",
    "\n",
    "        for log_prob, R in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        policy_sum = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "    # Backpropagation\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        policy_sum.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        runtime = end_time-start_time\n",
    "        \n",
    "        runtime_sum += runtime\n",
    "        if e % print_every == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tLast {}\\tEpisodes average reward: {:.2f}\\tRuntime: {:.2f}'.format(e, scores_deque[-1], print_every, np.mean(scores_deque), runtime_sum))\n",
    "            runtime_sum = 0\n",
    "        if np.mean(scores_deque) == 500:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores, policy.gradient_list, average_scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def save_training_data(self):\n",
    "        ''' \n",
    "        Saves training data into json files\n",
    "        '''\n",
    "        current_directory = os.path.dirname(__file__)\n",
    "        folder_name = f\"{str(self.env_name)}_{self.policy.policy.post_processing}_{self.policy.circuit.n_layers}\"\n",
    "        folder_path = os.path.join(current_directory, folder_name)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        episode_data = [self.scores_deque, \n",
    "                        self.runtime, \n",
    "                        self.loss.item(), \n",
    "                        tensor_to_list(self.policy.get_gradients()[0]), \n",
    "                        tensor_to_list(self.policy.get_gradients()[1])]\n",
    "\n",
    "        if folder_path is not None:\n",
    "            file_path = os.path.join(self.folder_path, f\"{self.file_name}.json\")\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, 'r') as f:\n",
    "                    existing_data = json.load(f)\n",
    "                existing_data.append(episode_data)\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump(existing_data, f, indent=4)\n",
    "            else:\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump([episode_data], f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "def create_directory(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def tensor_to_list(tensor):\n",
    "    \"\"\"\n",
    "    Convert a tensor or numpy array to a nested list.\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, list):\n",
    "        return [tensor_to_list(t) for t in tensor]\n",
    "    elif isinstance(tensor, dict):\n",
    "        return {key: tensor_to_list(value) for key, value in tensor.items()}\n",
    "    elif isinstance(tensor, np.ndarray):\n",
    "        return tensor.tolist()\n",
    "    elif isinstance(tensor, torch.Tensor):\n",
    "        return tensor.tolist()\n",
    "    else:\n",
    "        return tensor\n",
    "    \n",
    "def create_optimizer_with_lr(params, lr_list, use_amsgrad=False):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': p, 'lr': lr} for p, lr in zip(params, lr_list)\n",
    "    ], amsgrad=use_amsgrad)\n",
    "    return optimizer\n",
    "\n",
    "def get_function_representation(func):\n",
    "    if callable(func):\n",
    "        # Check if the function is a lambda\n",
    "        if func.__name__ == \"<lambda>\":\n",
    "            # Optionally, check if the function has a custom description attribute\n",
    "            return f\"{func.__module__}.<lambda>\" + (getattr(func, 'description', ''))\n",
    "        else:\n",
    "            return f\"{func.__module__}.{func.__name__}\"\n",
    "    return \"Unknown Function Type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measures\n",
    "\n",
    "def measure_probs(qubits):\n",
    "    return qml.probs(wires=range(qubits)) \n",
    "\n",
    "def one_measure_expval_global(qubits):\n",
    "\n",
    "    pauli_string = qml.PauliZ(0)\n",
    "    for i in range(1, qubits):\n",
    "        pauli_string = pauli_string @ qml.PauliZ(i)\n",
    "    \n",
    "    expvals = []\n",
    "    expvals.append(qml.expval(pauli_string))\n",
    "\n",
    "    return expvals\n",
    "\n",
    "def two_measure_expval_global(qubits):\n",
    "\n",
    "    pauli_string = qml.PauliZ(0)\n",
    "    for i in range(1, qubits):\n",
    "        pauli_string = pauli_string @ qml.PauliZ(i)\n",
    "    \n",
    "    expvals = []\n",
    "    expvals.append(qml.expval(pauli_string))\n",
    "    expvals.append(qml.expval(-pauli_string))\n",
    "\n",
    "    return expvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vqc operations\n",
    "\n",
    "def strong_entangling(n_qubits, layer):\n",
    "    for qubit in range(n_qubits):\n",
    "        target = (qubit + layer + 1) % n_qubits\n",
    "        if target != qubit:\n",
    "            qml.CNOT(wires=[qubit, target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JerbiModel(nn.Module):\n",
    "    '''\n",
    "    Creates a parametrized quantum circuit based on the work in https://doi.org/10.48550/arXiv.2103.05577.\n",
    "\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers, shots = None, diff_method = 'best', entanglement = True,\n",
    "                entanglement_pattern = \"all_to_all\", entanglement_gate = qml.CNOT, input_scaling = True, \n",
    "                input_init = torch.nn.init.ones_, weight_init = torch.nn.init.normal_, measure = two_measure_expval_global):\n",
    "        super(JerbiModel, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_scaling = input_scaling\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        if self.n_layers < 1:\n",
    "            raise ValueError(\"Number of layers can't take values below 1\")\n",
    "        \n",
    "        self.weight_shapes = {\n",
    "            \"params\": (self.n_layers + 1, self.n_qubits, 2),\n",
    "            \"input_params\": (self.n_layers, self.n_qubits, 2),\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"params\": self.weight_init,\n",
    "            \"input_params\": self.input_init,\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, params, input_params):\n",
    "            # In case the number of qubits is greater than the input size AND it is multiple of the input length, \n",
    "            # then tile input tensor 'multiplier' times\n",
    "            if self.n_qubits > len(inputs) and self.n_qubits % len(inputs) == 0:\n",
    "                multiplier = self.n_qubits // len(inputs)\n",
    "                inputs = torch.cat([inputs] * multiplier)\n",
    "\n",
    "            # If the number of qubits is not equal to the input length and not a multiple of the input length, raise an error\n",
    "            elif self.n_qubits != len(inputs) and self.n_qubits % len(inputs) != 0:\n",
    "                raise ValueError('Number of qubits cannot be divided by input lenght')\n",
    "\n",
    "            # Apply Hadamard gates to every qubits\n",
    "            qml.broadcast(qml.Hadamard, wires=range(self.n_qubits), pattern=\"single\")\n",
    "            \n",
    "            # Iterate through layers\n",
    "            for layer in range(self.n_layers):\n",
    "                # Iterate through qubits\n",
    "                for wire in range(self.n_qubits):\n",
    "                    # Parameterized block\n",
    "                    qml.RZ(params[layer][wire][0], wires=wire)\n",
    "                    qml.RY(params[layer][wire][1], wires=wire)\n",
    "\n",
    "                # Entanglement between qubits\n",
    "                if self.entanglement:\n",
    "                    if self.entanglement_pattern == 'strong_entangling':\n",
    "                        strong_entangling(self.n_qubits, layer)\n",
    "                    else:\n",
    "                        qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "                # If input scaling set to True, multiply inputs by weights\n",
    "                if self.input_scaling:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RY(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                        qml.RZ(input_params[layer][wire][1] * inputs[wire], wires=wire)\n",
    "                # If input scaling set to False, use only the inputs\n",
    "                else:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RY(inputs[wire], wires=wire)\n",
    "                        qml.RZ(inputs[wire], wires=wire)\n",
    "                \n",
    "            # Last parameterized layer\n",
    "            for wire in range(self.n_qubits):\n",
    "                qml.RZ(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)  \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        \n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        circuit = qml.draw_mpl(self.qnode)(inputs, \n",
    "                                        initialized_params[\"params\"], \n",
    "                                        initialized_params[\"input_params\"])\n",
    "        \n",
    "\n",
    "    def circuit_spectrum(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        res = qml.fourier.circuit_spectrum(self.qnode)(inputs, initialized_params[\"params\"], initialized_params[\"input_params\"])\n",
    "        for inp, freqs in res.items():\n",
    "            print(f\"{inp}: {freqs}\")       \n",
    "\n",
    "        \n",
    "    @classmethod\n",
    "    def info(cls):        \n",
    "        '''\n",
    "        Provides a summary of the JerbiModel class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        Creates the Jerbi policy (Parameterized Quantum Circuit) based on the design in https://doi.org/10.48550/arXiv.2103.05577.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_qubits (int): \n",
    "            Number of qubits used in the quantum circuit.\n",
    "        \n",
    "        n_layers (int): \n",
    "            Number of layers in the quantum circuit. Each layer typically consists of parameterized rotations followed by entanglement gates.\n",
    "        \n",
    "        shots (int, optional): \n",
    "            Number of times the circuit gets executed (repeated measurements). If None, the circuit is executed with analytic calculations (no shot noise).\n",
    "        \n",
    "        diff_method (str): \n",
    "            Differentiation method used for training the model. Common options are 'best', 'parameter-shift', 'backprop', etc.\n",
    "        \n",
    "        entanglement_pattern (str): \n",
    "            Entanglement pattern used in the circuit, such as 'chain', 'ring', 'all_to_all', etc., as defined by qml.broadcast patterns.\n",
    "        \n",
    "        entanglement_gate (function): \n",
    "            Quantum gate used for entanglement, such as qml.CZ or qml.CNOT. This gate will be applied between qubits according to the specified entanglement pattern.\n",
    "        \n",
    "        input_scaling (bool): \n",
    "            If True, input parameters are scaled by additional learnable parameters (input_params). The input is multiplied by these parameters before being applied to the qubits.\n",
    "                \n",
    "        input_init (function): \n",
    "            Function to initialize the input scaling parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, and function defined by the user.\n",
    "        \n",
    "        weight_init (function): \n",
    "            Function to initialize the weights of the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.normal_, and function defined by the user..\n",
    "                \n",
    "        measure (function): \n",
    "            Measurement function that takes the number of qubits as an argument and returns the measurement result. Common choices are measure_probs, measure_expval_pairs, or any user-defined measurement function.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        JerbiModel: \n",
    "            An instance of the JerbiModel class representing the quantum neural network.\n",
    "        \"\"\"\n",
    "        print(info_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UQC(nn.Module):\n",
    "    '''\n",
    "    Creates a parameterized quantum circuit (Universal Quantum Classifier) based in https://doi.org/10.22331/q-2020-02-06-226.\n",
    "\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers, state_dim, shots = None, diff_method = 'best', entanglement = True,\n",
    "                entanglement_pattern = \"all_to_all\", entanglement_gate = qml.CNOT, \n",
    "                weight_init = torch.nn.init.normal_, measure = two_measure_expval_global):\n",
    "        super(UQC, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.state_dim = state_dim\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        self.weight_shapes = {\n",
    "            \"weights\": (self.n_layers, self.n_qubits, self.state_dim),\n",
    "            \"params\": (self.n_layers, self.n_qubits, 1),\n",
    "            \"bias\": (self.n_layers, self.n_qubits)\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"weights\": self.weight_init,\n",
    "            \"params\": self.weight_init,\n",
    "            \"bias\": torch.nn.init.ones_\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, weights, params, bias):\n",
    "\n",
    "            # Iterate through layers\n",
    "            for layer in range(self.n_layers):\n",
    "                # Iterate through qubits\n",
    "                for wire in range(self.n_qubits):\n",
    "                    # Compute the Hadamard product, sum with bias, and use it as the angle for R_X\n",
    "                    hadamard_product = torch.dot(inputs.clone().detach(), weights[layer][wire])\n",
    "                    angle = hadamard_product + bias[layer][wire]\n",
    "\n",
    "                    # Apply the Rx gate with computed angle\n",
    "                    qml.RX(angle, wires=wire)\n",
    "\n",
    "                    qml.RZ(params[layer][wire][0], wires=wire)\n",
    "                # Entanglement between qubits\n",
    "                if self.entanglement:\n",
    "                    if self.entanglement_pattern == 'strong_entangling':\n",
    "                        strong_entangling(self.n_qubits, layer)\n",
    "                    else:\n",
    "                        qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(self.qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        \n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.state_dim)], dtype=torch.float32)\n",
    "        \n",
    "        # Initialize all parameters using the provided initialization methods\n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        circuit = qml.draw_mpl(self.qnode)(inputs, \n",
    "                                        initialized_params[\"weights\"], \n",
    "                                        initialized_params[\"params\"], \n",
    "                                        initialized_params[\"bias\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfqTutorial(nn.Module):\n",
    "    '''\n",
    "    Creates a parameterized quantum circuit based on the TensorFlow Quantum tutorial in https://www.tensorflow.org/quantum/tutorials/quantum_reinforcement_learning\n",
    "\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers, shots = None, diff_method = 'best', entanglement = True,\n",
    "                entanglement_pattern = \"ring\", entanglement_gate = qml.CNOT, input_scaling = True, \n",
    "                input_init = torch.nn.init.ones_, weight_init = torch.nn.init.normal_, measure = two_measure_expval_global):\n",
    "        super(TfqTutorial, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_scaling = input_scaling\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        if self.n_layers < 1:\n",
    "            raise ValueError(\"Number of layers can't take values below 1\")\n",
    "        \n",
    "        self.weight_shapes = {\n",
    "            \"params\": (self.n_layers + 1, self.n_qubits, 3),\n",
    "            \"input_params\": (self.n_layers, self.n_qubits, 1),\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"params\": self.weight_init,\n",
    "            \"input_params\": self.input_init,\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, params, input_params):\n",
    "            # In case the number of qubits is greater than the input size AND it is multiple of the input length, \n",
    "            # then tile input tensor 'multiplier' times\n",
    "            if self.n_qubits > len(inputs) and self.n_qubits % len(inputs) == 0:\n",
    "                multiplier = self.n_qubits // len(inputs)\n",
    "                inputs = torch.cat([inputs] * multiplier)\n",
    "\n",
    "            # If the number of qubits is not equal to the input length and not a multiple of the input length, raise an error\n",
    "            elif self.n_qubits != len(inputs) and self.n_qubits % len(inputs) != 0:\n",
    "                raise ValueError('Number of qubits cannot be divided by input lenght')\n",
    "            \n",
    "            # Iterate through layers\n",
    "            for layer in range(self.n_layers):\n",
    "                # Iterate through qubits\n",
    "                for wire in range(self.n_qubits):\n",
    "                    # Parameterized block\n",
    "                    qml.RX(params[layer][wire][0], wires=wire)\n",
    "                    qml.RY(params[layer][wire][1], wires=wire)\n",
    "                    qml.RZ(params[layer][wire][2], wires=wire)\n",
    "\n",
    "                # Entanglement between qubits\n",
    "                if self.entanglement:\n",
    "                    if self.entanglement_pattern == 'strong_entangling':\n",
    "                        strong_entangling(self.n_qubits, layer)\n",
    "                    else:\n",
    "                        qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "                # If input scaling set to True, multiply inputs by weights\n",
    "                if self.input_scaling:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RX(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                # If input scaling set to False, use only the inputs\n",
    "                else:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RX(inputs[wire], wires=wire)\n",
    "                \n",
    "            # Last parameterized layer\n",
    "            for wire in range(self.n_qubits):\n",
    "                qml.RX(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "                qml.RZ(params[-1][wire][2], wires=wire)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)  \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        \n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        circuit = qml.draw_mpl(self.qnode)(inputs, \n",
    "                                        initialized_params[\"params\"], \n",
    "                                        initialized_params[\"input_params\"])\n",
    "        \n",
    "\n",
    "    def circuit_spectrum(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        res = qml.fourier.circuit_spectrum(self.qnode)(inputs, initialized_params[\"params\"], initialized_params[\"input_params\"])\n",
    "        for inp, freqs in res.items():\n",
    "            print(f\"{inp}: {freqs}\")       \n",
    "\n",
    "        \n",
    "    @classmethod\n",
    "    def info(cls):        \n",
    "        '''\n",
    "        Provides a summary of the JerbiModel class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        Creates the Jerbi policy (Parameterized Quantum Circuit) based on the design in https://doi.org/10.48550/arXiv.2103.05577.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_qubits (int): \n",
    "            Number of qubits used in the quantum circuit.\n",
    "        \n",
    "        n_layers (int): \n",
    "            Number of layers in the quantum circuit. Each layer typically consists of parameterized rotations followed by entanglement gates.\n",
    "        \n",
    "        shots (int, optional): \n",
    "            Number of times the circuit gets executed (repeated measurements). If None, the circuit is executed with analytic calculations (no shot noise).\n",
    "        \n",
    "        diff_method (str): \n",
    "            Differentiation method used for training the model. Common options are 'best', 'parameter-shift', 'backprop', etc.\n",
    "        \n",
    "        entanglement_pattern (str): \n",
    "            Entanglement pattern used in the circuit, such as 'chain', 'ring', 'all_to_all', etc., as defined by qml.broadcast patterns.\n",
    "        \n",
    "        entanglement_gate (function): \n",
    "            Quantum gate used for entanglement, such as qml.CZ or qml.CNOT. This gate will be applied between qubits according to the specified entanglement pattern.\n",
    "        \n",
    "        input_scaling (bool): \n",
    "            If True, input parameters are scaled by additional learnable parameters (input_params). The input is multiplied by these parameters before being applied to the qubits.\n",
    "                \n",
    "        input_init (function): \n",
    "            Function to initialize the input scaling parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, and function defined by the user.\n",
    "        \n",
    "        weight_init (function): \n",
    "            Function to initialize the weights of the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.normal_, and function defined by the user..\n",
    "                \n",
    "        measure (function): \n",
    "            Measurement function that takes the number of qubits as an argument and returns the measurement result. Common choices are measure_probs, measure_expval_pairs, or any user-defined measurement function.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        JerbiModel: \n",
    "            An instance of the JerbiModel class representing the quantum neural network.\n",
    "        \"\"\"\n",
    "        print(info_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JerbiModelModified(nn.Module):\n",
    "    '''\n",
    "    Creates a parametrized quantum circuit based on the work in https://doi.org/10.48550/arXiv.2103.05577.\n",
    "\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers, shots = None, diff_method = 'best', entanglement = True,\n",
    "                entanglement_pattern = \"all_to_all\", entanglement_gate = qml.CNOT, input_scaling = True, \n",
    "                input_init = torch.nn.init.ones_, weight_init = torch.nn.init.normal_, measure = two_measure_expval_global):\n",
    "        super(JerbiModelModified, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_scaling = input_scaling\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        if self.n_layers < 1:\n",
    "            raise ValueError(\"Number of layers can't take values below 1\")\n",
    "        \n",
    "        self.weight_shapes = {\n",
    "            \"params\": (self.n_layers + 1, self.n_qubits, 3),\n",
    "            \"input_params\": (self.n_layers, self.n_qubits, 3),\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"params\": self.weight_init,\n",
    "            \"input_params\": self.input_init,\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, params, input_params):\n",
    "            # In case the number of qubits is greater than the input size AND it is multiple of the input length, \n",
    "            # then tile input tensor 'multiplier' times\n",
    "            if self.n_qubits > len(inputs) and self.n_qubits % len(inputs) == 0:\n",
    "                multiplier = self.n_qubits // len(inputs)\n",
    "                inputs = torch.cat([inputs] * multiplier)\n",
    "\n",
    "            # If the number of qubits is not equal to the input length and not a multiple of the input length, raise an error\n",
    "            elif self.n_qubits != len(inputs) and self.n_qubits % len(inputs) != 0:\n",
    "                raise ValueError('Number of qubits cannot be divided by input lenght')\n",
    "\n",
    "            # Apply Hadamard gates to every qubits\n",
    "            qml.broadcast(qml.Hadamard, wires=range(self.n_qubits), pattern=\"single\")\n",
    "            \n",
    "            # Iterate through layers\n",
    "            for layer in range(self.n_layers):\n",
    "                # Iterate through qubits\n",
    "                for wire in range(self.n_qubits):\n",
    "                    # Parameterized block\n",
    "                    qml.RZ(params[layer][wire][0], wires=wire)\n",
    "                    qml.RY(params[layer][wire][1], wires=wire)\n",
    "                    qml.RZ(params[layer][wire][2], wires=wire)\n",
    "\n",
    "                # Entanglement between qubits\n",
    "                if self.entanglement:\n",
    "                    if self.entanglement_pattern == 'strong_entangling':\n",
    "                        strong_entangling(self.n_qubits, layer)\n",
    "                    else:\n",
    "                        qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "                # If input scaling set to True, multiply inputs by weights\n",
    "                if self.input_scaling:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RZ(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                        qml.RY(input_params[layer][wire][1] * inputs[wire], wires=wire)\n",
    "                        qml.RZ(input_params[layer][wire][2] * inputs[wire], wires=wire)\n",
    "\n",
    "                # If input scaling set to False, use only the inputs\n",
    "                else:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RZ(inputs[wire], wires=wire)\n",
    "                        qml.RY(inputs[wire], wires=wire)\n",
    "                        qml.RZ(inputs[wire], wires=wire)\n",
    "\n",
    "                \n",
    "            # Last parameterized layer\n",
    "            for wire in range(self.n_qubits):\n",
    "                qml.RZ(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "                qml.RZ(params[-1][wire][2], wires=wire)\n",
    "                \n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)  \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        \n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        circuit = qml.draw_mpl(self.qnode)(inputs, \n",
    "                                        initialized_params[\"params\"], \n",
    "                                        initialized_params[\"input_params\"])\n",
    "        \n",
    "\n",
    "    def circuit_spectrum(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        res = qml.fourier.circuit_spectrum(self.qnode)(inputs, initialized_params[\"params\"], initialized_params[\"input_params\"])\n",
    "        for inp, freqs in res.items():\n",
    "            print(f\"{inp}: {freqs}\")       \n",
    "\n",
    "        \n",
    "    @classmethod\n",
    "    def info(cls):        \n",
    "        '''\n",
    "        Provides a summary of the JerbiModel class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        Creates the Jerbi policy (Parameterized Quantum Circuit) based on the design in https://doi.org/10.48550/arXiv.2103.05577.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_qubits (int): \n",
    "            Number of qubits used in the quantum circuit.\n",
    "        \n",
    "        n_layers (int): \n",
    "            Number of layers in the quantum circuit. Each layer typically consists of parameterized rotations followed by entanglement gates.\n",
    "        \n",
    "        shots (int, optional): \n",
    "            Number of times the circuit gets executed (repeated measurements). If None, the circuit is executed with analytic calculations (no shot noise).\n",
    "        \n",
    "        diff_method (str): \n",
    "            Differentiation method used for training the model. Common options are 'best', 'parameter-shift', 'backprop', etc.\n",
    "        \n",
    "        entanglement_pattern (str): \n",
    "            Entanglement pattern used in the circuit, such as 'chain', 'ring', 'all_to_all', etc., as defined by qml.broadcast patterns.\n",
    "        \n",
    "        entanglement_gate (function): \n",
    "            Quantum gate used for entanglement, such as qml.CZ or qml.CNOT. This gate will be applied between qubits according to the specified entanglement pattern.\n",
    "        \n",
    "        input_scaling (bool): \n",
    "            If True, input parameters are scaled by additional learnable parameters (input_params). The input is multiplied by these parameters before being applied to the qubits.\n",
    "                \n",
    "        input_init (function): \n",
    "            Function to initialize the input scaling parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, and function defined by the user.\n",
    "        \n",
    "        weight_init (function): \n",
    "            Function to initialize the weights of the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.normal_, and function defined by the user..\n",
    "                \n",
    "        measure (function): \n",
    "            Measurement function that takes the number of qubits as an argument and returns the measurement result. Common choices are measure_probs, measure_expval_pairs, or any user-defined measurement function.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        JerbiModel: \n",
    "            An instance of the JerbiModel class representing the quantum neural network.\n",
    "        \"\"\"\n",
    "        print(info_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyPostProcessing(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_actions, policy_type = 'raw_contiguous', \n",
    "                 beta_scheduling = False, beta = 1, increase_rate = 0.0005, \n",
    "                 output_scaling = False, output_init = torch.nn.init.ones_):\n",
    "        super(PolicyPostProcessing, self).__init__()\n",
    "\n",
    "        '''\n",
    "        Determines the type of policy used based on the arguments:\n",
    "\n",
    "            n_actions(int) = Number of actions\n",
    "            post_processing(str) = Type of policy ('raw_contiguous', 'raw_parity', 'softmax')\n",
    "            beta_scheduling(bool) = Inverse temperature parameter used in softmax (used if set to True)\n",
    "            beta(float) = Beta parameter or inverse temperature (used only for softmax)\n",
    "            increase_rate(float) = Amount added to beta at the end of each episode (used only for softmax)\n",
    "            output_scaling(bool) = Output parameters are used if True\n",
    "            output_init(function) = How the output parameters are initialized\n",
    "            \n",
    "        '''\n",
    "        self.n_actions = n_actions\n",
    "        self.policy_type = policy_type\n",
    "        self.beta_scheduling = beta_scheduling\n",
    "        self.beta = beta\n",
    "        self.increase_rate = increase_rate\n",
    "        self.output_scaling = output_scaling\n",
    "        self.output_init = output_init\n",
    "\n",
    "        if self.output_scaling == True:\n",
    "            #self.output_params = nn.parameter.Parameter(torch.Tensor(self.n_actions), requires_grad=True)\n",
    "            self.output_params = nn.parameter.Parameter(torch.Tensor(1), requires_grad=True)\n",
    "            self.output_init(self.output_params)\n",
    "        else:\n",
    "            self.register_parameter('w_input', None)\n",
    "\n",
    "    def forward(self,probs):\n",
    "        if self.policy_type == 'raw_contiguous':\n",
    "            policy = self.raw_contiguous(probs)\n",
    "        elif self.policy_type == 'raw_parity':\n",
    "            policy = self.raw_parity(probs)\n",
    "        elif self.policy_type == 'softmax':\n",
    "            policy = self.softmax(probs)\n",
    "        elif self.policy_type == 'softmax_probs':\n",
    "            policy = self.softmax_probs(probs)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid post-processing method specified.\")\n",
    "        return policy\n",
    "\n",
    "    def raw_contiguous(self,probs):\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        chunk_size = len(probs_flatten) // self.n_actions\n",
    "        remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "        policy = []\n",
    "\n",
    "        for i in range(self.n_actions):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size\n",
    "\n",
    "            if i < remainder:\n",
    "                end += 1\n",
    "\n",
    "            # Update the original policy list instead of creating a new one\n",
    "            policy.append(sum(probs_flatten[start:end]))\n",
    "\n",
    "        policy_tensor = torch.stack(policy)\n",
    "        return policy_tensor\n",
    "        \n",
    "    def raw_parity(self,probs):\n",
    "\n",
    "        if self.n_actions % 2 != 0:\n",
    "            raise ValueError('For parity-like policy, n_actions must be an even number')\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        policy = torch.zeros(self.n_actions)\n",
    "        counter = 0\n",
    "        for prob in probs_flatten:\n",
    "            policy[counter] += prob\n",
    "            counter += 1\n",
    "            if counter == self.n_actions:\n",
    "                counter = 0\n",
    "        \n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    def softmax(self, probs):\n",
    "        \n",
    "        if self.output_scaling == True:\n",
    "            probs *= self.output_params\n",
    "\n",
    "        scaled_output = probs * self.beta\n",
    "        softmax_output = F.softmax(scaled_output, dim=0)\n",
    "        return softmax_output\n",
    "    \n",
    "    def softmax_probs(self, probs):\n",
    "\n",
    "        if self.output_scaling == True:\n",
    "            probs *= self.output_params\n",
    "            \n",
    "        probs_flatten = probs.flatten()\n",
    "        chunk_size = len(probs_flatten) // self.n_actions\n",
    "        remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "        policy = []\n",
    "\n",
    "        for i in range(self.n_actions):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size\n",
    "\n",
    "            if i < remainder:\n",
    "                end += 1\n",
    "\n",
    "            policy.append(sum(probs_flatten[start:end]))\n",
    "        policy_tensor = torch.stack(policy)\n",
    "        softmax_output = F.softmax(policy_tensor * self.beta, dim=0)\n",
    "        return softmax_output\n",
    "    \n",
    "    def beta_schedule(self):\n",
    "        if self.beta_scheduling == True:\n",
    "            if self.policy_type == 'softmax' or self.policy_type == 'softmax_probs':\n",
    "                self.beta += self.increase_rate\n",
    "\n",
    "    @classmethod\n",
    "    def info(cls):\n",
    "        '''\n",
    "        Provides a summary of the PolicyType class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        PolicyType Class:\n",
    "\n",
    "        Determines the type of policy used based on the arguments:\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_actions (int): \n",
    "            Number of actions available for the agent to choose from.\n",
    "        \n",
    "        policy_type (str): \n",
    "            Type of policy applied to the probability distribution:\n",
    "            - 'raw_contiguous': Sums up contiguous chunks of probabilities.\n",
    "            - 'raw_parity': Sums up probabilities in a circular manner based on parity.\n",
    "            - 'softmax': Applies a softmax function to the scaled probabilities.\n",
    "            - 'softmax_probs': Sums up contiguous chunks of probabilities and then applies softmax.\n",
    "        \n",
    "        beta_scheduling (bool): \n",
    "            If True, updates the inverse temperature parameter (beta) after each episode. Used only for 'softmax' and 'softmax_probs'.\n",
    "        \n",
    "        beta (float): \n",
    "            Inverse temperature parameter used for scaling probabilities in the softmax policy.\n",
    "        \n",
    "        increase_rate (float): \n",
    "            Amount added to beta at the end of each episode, if beta_scheduling is True.\n",
    "        \n",
    "        output_scaling (bool): \n",
    "            If True, scales the output probabilities by learnable parameters.\n",
    "        \n",
    "        output_init (function): \n",
    "            Initialization function for output parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, etc.\n",
    "        \n",
    "        Methods:\n",
    "        -------\n",
    "        sample(probs):\n",
    "            Selects an action based on the chosen post_processing method.\n",
    "        \n",
    "        raw_contiguous(probs):\n",
    "            Sums up contiguous chunks of probabilities and returns the resulting tensor.\n",
    "        \n",
    "        raw_parity(probs):\n",
    "            Sums up probabilities in a circular manner based on parity and returns the action, log probability, and policy tensor.\n",
    "        \n",
    "        softmax(probs):\n",
    "            Applies a softmax function to the scaled probabilities and returns the action, log probability, and policy tensor.\n",
    "        \n",
    "        softmax_probs(probs):\n",
    "            Sums up contiguous chunks of probabilities, applies softmax, and returns the action, log probability, and policy tensor.\n",
    "        \n",
    "        beta_schedule():\n",
    "            Updates the beta parameter if beta_scheduling is True. Only applicable for 'softmax' and 'softmax_probs' methods.\n",
    "        \"\"\"\n",
    "        print(info_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumPolicy(nn.Module):\n",
    "    \n",
    "    def __init__(self, circuit, post_processing):\n",
    "        super(QuantumPolicy, self).__init__()\n",
    "        self.circuit = circuit\n",
    "        self.post_processing = post_processing\n",
    "\n",
    "    def sample(self, inputs):\n",
    "        '''\n",
    "        Samples an action from the action probability distribution\n",
    "        '''\n",
    "        policy = self.forward(inputs)\n",
    "        dist = torch.distributions.Categorical(policy)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), policy\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Input state is fed to the circuit - its output is then fed to the post processing \n",
    "        '''\n",
    "        probs = self.circuit.forward(inputs)\n",
    "        probs_processed = self.post_processing.forward(probs)\n",
    "        return probs_processed\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        '''\n",
    "        Returns the values of each set of parameters\n",
    "        '''\n",
    "        parameter_values = [param.clone().detach().numpy().flatten() for param in self.circuit.parameters()]\n",
    "        return parameter_values\n",
    "    \n",
    "    def get_gradients(self):\n",
    "        '''\n",
    "        Returns the gradient values of each set of parameters\n",
    "        '''\n",
    "        gradients = []\n",
    "\n",
    "        # Get gradients from circuit parameters\n",
    "        circuit_gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for param in self.circuit.parameters()]\n",
    "        gradients.extend(circuit_gradients)\n",
    "\n",
    "        # Get gradients from policy parameters\n",
    "        policy_gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for param in self.policy.parameters()]\n",
    "        gradients.extend(policy_gradients)\n",
    "\n",
    "        return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update aos parametros deve vir depois da if clause que verifica se foi resolvido - caso verdade, os weights n devem levar update\n",
    "\n",
    "Baselines parecem ter problemas, principalmente a baseline com PQC a aproximar a value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceAgent:\n",
    "\n",
    "    def __init__(self, policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy = policy.to(self.device)\n",
    "        self.policy_optimizer = policy_optimizer\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env_name = env_name\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "        self.solved = False\n",
    "        self.scores_deque = deque(maxlen=self.print_every)\n",
    "\n",
    "    def train(self, run_count='0', datetime = None, path=None, tensorboard=False):\n",
    "\n",
    "        if run_count is not None and path is not None:\n",
    "            base_path = create_directory(path)\n",
    "            env_folder = create_directory(os.path.join(base_path, self.env_name))\n",
    "            experiment_folder_name = f\"{self.policy.post_processing.policy_type}_{self.policy.circuit.n_layers}layer_{datetime}\"\n",
    "            experiment_path = create_directory(os.path.join(env_folder, experiment_folder_name))\n",
    "            run_path = create_directory(os.path.join(experiment_path, f'run_{str(run_count)}'))\n",
    "            self.save_agent_data(run_path)\n",
    "        \n",
    "        if tensorboard:\n",
    "            writer = SummaryWriter(log_dir=run_path)\n",
    "        \n",
    "        for i in range(1, self.n_episodes + 1):\n",
    "            start_time = time.time()\n",
    "            self.get_trajectory()\n",
    "            self.update_policy()\n",
    "            end_time = time.time()\n",
    "            self.runtime = end_time - start_time\n",
    "            if tensorboard:\n",
    "                self.writer_function(writer, i)\n",
    "            if run_count is not None and path is not None:\n",
    "                self.save_data(run_path)\n",
    "            if not self.solved:\n",
    "                self.policy.post_processing.beta_schedule()\n",
    "            \n",
    "            if np.mean(self.scores_deque) > self.env.spec.reward_threshold:\n",
    "                self.solved = True\n",
    "\n",
    "            if i % self.print_every == 0 and self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}s\\t Last {} Episodes average reward: {:.2f}\\t '.format(i, self.scores_deque[-1], self.runtime, self.print_every, np.mean(self.scores_deque)))\n",
    "            elif self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}s\\t '.format(i, self.scores_deque[-1], self.runtime))\n",
    "        \n",
    "    def get_trajectory(self):\n",
    "        '''\n",
    "        Gets a trajectory based on the running policy until it runs out of bounds or achieves maximum reward of an episode\n",
    "        '''\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(self.max_t):\n",
    "            if t == 0:\n",
    "                state_tensor = torch.tensor(state[0]).float().to(self.device)\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float().to(self.device)\n",
    "            action, log_prob, _ = self.policy.sample(state_tensor)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            \n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                self.scores_deque.append(sum(self.rewards))\n",
    "                break\n",
    "\n",
    "    def update_policy(self):\n",
    "        '''\n",
    "        Computes the loss and gradients and updates the policy via gradient methods\n",
    "        '''\n",
    "\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns).to(self.device)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        for log_prob, ret in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * ret)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        self.loss = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        if not self.solved:\n",
    "            self.policy_optimizer.step()\n",
    "        \n",
    "    def save_agent_data(self,main_path):\n",
    "        '''\n",
    "        Stores the model parameters into a json file\n",
    "\n",
    "        '''\n",
    "        agent_variables = {\n",
    "            \"Model\": self.policy.circuit.__class__.__name__,\n",
    "            \"Number of Qubits\": self.policy.circuit.n_qubits,\n",
    "            \"Number of Layers\": self.policy.circuit.n_layers,\n",
    "            \"Shots\": self.policy.circuit.shots,\n",
    "            \"Differentiation Method\": self.policy.circuit.diff_method,\n",
    "            \"Entanglement\": self.policy.circuit.entanglement,\n",
    "            \"Entanglement Pattern\": self.policy.circuit.entanglement_pattern,\n",
    "            \"Entanglement Gate\": self.policy.circuit.entanglement_gate,\n",
    "            \"Input Scaling\": self.policy.circuit.input_scaling,\n",
    "            \"Input Initialization\": get_function_representation(self.policy.circuit.input_init),\n",
    "            \"Weight Initialization\": get_function_representation(self.policy.circuit.weight_init),\n",
    "            \"Measure\": get_function_representation(self.policy.circuit.measure),\n",
    "            \"Policy Post Processing\": self.policy.post_processing.policy_type,\n",
    "            \"Optimizers\": str(self.policy_optimizer),\n",
    "            \"Envinronment\": str(self.env_name),\n",
    "            \"Gamma (discounting factor)\": self.gamma,\n",
    "        }\n",
    "\n",
    "        if self.policy.post_processing.policy_type in ['softmax', 'softmax_probs']:\n",
    "            agent_variables.update({\n",
    "                \"Softmax scheduling (in case policy is softmax)\": f\"{self.policy.post_processing.beta_scheduling}. Starting beta: {self.policy.post_processing.beta}. Increase rate: {self.policy.post_processing.increase_rate}\",\n",
    "                \"Softmax output scaling (in case policy is softmax)\": f\"{self.policy.post_processing.output_scaling}. Output Initialization: {get_function_representation(self.policy.post_processing.output_init)}\",\n",
    "            })\n",
    "        with open(os.path.join(main_path, \"agent_characteristics.json\"), \"w\") as f:\n",
    "            json.dump(agent_variables, f, indent=4)\n",
    "\n",
    "    def save_data(self, run_path):\n",
    "        '''\n",
    "        Saves the data into a .npz file of each episode\n",
    "\n",
    "        '''\n",
    "\n",
    "        if not os.path.exists(run_path):\n",
    "            os.makedirs(run_path)\n",
    "\n",
    "        if os.path.exists(run_path):\n",
    "            data = np.load(run_path, allow_pickle=True)\n",
    "            old_episode_reward = data['episode_reward'].tolist()\n",
    "            old_loss = data['loss'].tolist()\n",
    "            old_runtime = data['runtime'].tolist()\n",
    "            old_params_gradients = data['params_gradients'].tolist()\n",
    "            old_input_params_gradients = data['input_params_gradients'].tolist()\n",
    "        else:\n",
    "            old_episode_reward = []\n",
    "            old_loss = []\n",
    "            old_runtime = []\n",
    "            old_params_gradients = []\n",
    "            old_input_params_gradients = []\n",
    "\n",
    "\n",
    "        old_episode_reward.append(self.scores_deque[-1])\n",
    "        old_loss.append(self.loss.item())\n",
    "        old_runtime.append(self.runtime)\n",
    "        old_params_gradients.append(tensor_to_list(self.policy.get_gradients()[0]))\n",
    "        old_input_params_gradients.append(tensor_to_list(self.policy.get_gradients()[1]))\n",
    "\n",
    "        np.savez_compressed(run_path, episode_reward = np.array(old_episode_reward),\n",
    "                                 loss = np.array(old_loss),\n",
    "                                 runtime = np.array(old_runtime),\n",
    "                                 params_gradients = np.array(old_params_gradients), \n",
    "                                 input_params_gradients = np.array(old_input_params_gradients))\n",
    "        \n",
    "        del old_episode_reward[:]\n",
    "        del old_loss[:]\n",
    "        del old_runtime[:]\n",
    "        del old_params_gradients[:]\n",
    "        del old_input_params_gradients[:]\n",
    "\n",
    "    def writer_function(self, writer, iteration):\n",
    "        '''\n",
    "        Stores data into a tensorboard session\n",
    "\n",
    "        '''\n",
    "        writer.add_scalar(\"Episode Reward\", self.scores_deque[-1], global_step=iteration)\n",
    "        writer.add_scalar(\"Runtime\", self.runtime, global_step=iteration)\n",
    "        writer.add_scalar(\"Loss\", self.loss.item(), global_step=iteration)\n",
    "        if self.policy.post_processing.policy_type in ['softmax', 'softmax_probs']:\n",
    "            writer.add_scalar(\"Beta\", self.policy.post_processing.beta, global_step=iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceBatchAgent:\n",
    "\n",
    "    def __init__(self, policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose, batch_size=10):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy = policy.to(self.device)\n",
    "        self.policy_optimizer = policy_optimizer\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env_name = env_name\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.solved = False\n",
    "        self.scores_deque = deque(maxlen=self.print_every)\n",
    "\n",
    "    def train(self, run_count=None, path=None, tensorboard=False):\n",
    "\n",
    "        if run_count is not None and path is not None:\n",
    "            base_path = create_directory(path)\n",
    "            env_folder = create_directory(os.path.join(base_path, self.env_name))\n",
    "            experiment_folder_name = f\"{self.policy.post_processing.policy_type}_{self.policy.circuit.n_layers}layer_{datetime.now().strftime('%Y-%m-%d_%H.%M')}\"\n",
    "            experiment_path = create_directory(os.path.join(env_folder, experiment_folder_name))\n",
    "            run_path = create_directory(os.path.join(experiment_path, f'run_{str(run_count)}'))\n",
    "            self.save_agent_data(run_path)\n",
    "        \n",
    "        if tensorboard:\n",
    "            writer = SummaryWriter(log_dir=run_path)\n",
    "\n",
    "        batch_log_probs = []\n",
    "        batch_returns = []\n",
    "\n",
    "        for i in range(1, self.n_episodes + 1):\n",
    "            start_time = time.time()\n",
    "            self.get_trajectory()\n",
    "            batch_log_probs.extend(self.saved_log_probs)\n",
    "            batch_returns.append(self.rewards)\n",
    "            if i % self.batch_size == 0:\n",
    "                self.update_policy(batch_log_probs, batch_returns)\n",
    "                batch_log_probs = []\n",
    "                batch_returns = []\n",
    "            end_time = time.time()\n",
    "            self.runtime = end_time - start_time\n",
    "\n",
    "            if tensorboard:\n",
    "                self.writer_function(writer, i)\n",
    "\n",
    "            if run_count is not None and path is not None:\n",
    "                self.save_data(run_path)\n",
    "\n",
    "            if not self.solved:\n",
    "                self.policy.post_processing.beta_schedule()\n",
    "\n",
    "            if np.mean(self.scores_deque) > self.env.spec.reward_threshold:\n",
    "                self.solved = True\n",
    "\n",
    "            if i % self.print_every == 0 and self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}s\\t Last {} Episodes average reward: {:.2f}\\t '.format(i, self.scores_deque[-1], self.runtime, self.print_every, np.mean(self.scores_deque)))\n",
    "            elif self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}s\\t '.format(i, self.scores_deque[-1], self.runtime))\n",
    "\n",
    "        if batch_log_probs:\n",
    "            self.update_policy(batch_log_probs, batch_returns)\n",
    "\n",
    "    def get_trajectory(self):\n",
    "        '''\n",
    "        Gets a trajectory based on the running policy until it runs out of bounds or achieves maximum reward of an episode\n",
    "        '''\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(self.max_t):\n",
    "            if t == 0:\n",
    "                state_tensor = torch.tensor(state[0]).float().to(self.device)\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float().to(self.device)\n",
    "            action, log_prob, _ = self.policy.sample(state_tensor)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            \n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                self.scores_deque.append(sum(self.rewards))\n",
    "                break\n",
    "\n",
    "    def update_policy(self, batch_log_probs, batch_returns):\n",
    "        '''\n",
    "        Computes the loss and gradients and updates the policy via gradient methods\n",
    "        '''\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for batch in batch_returns:\n",
    "            ep_return = []\n",
    "            for r in batch[::-1]:\n",
    "                R = r + self.gamma * R\n",
    "                ep_return.insert(0, R)\n",
    "            ep_return = torch.tensor(ep_return).to(self.device)\n",
    "            ep_return = (ep_return - ep_return.mean()) / (ep_return.std() + 1e-8)\n",
    "            returns.extend(ep_return)\n",
    "            \n",
    "        for log_prob, ret in zip(batch_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * ret)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        self.loss = torch.cat(policy_unsqueezed).sum() / self.batch_size\n",
    "\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        if not self.solved:\n",
    "            self.policy_optimizer.step()\n",
    "        \n",
    "    def save_agent_data(self,main_path):\n",
    "        '''\n",
    "        Stores the model parameters into a json file\n",
    "\n",
    "        '''\n",
    "        agent_variables = {\n",
    "            \"Model\": self.policy.circuit.__class__.__name__,\n",
    "            \"Number of Qubits\": self.policy.circuit.n_qubits,\n",
    "            \"Number of Layers\": self.policy.circuit.n_layers,\n",
    "            \"Shots\": self.policy.circuit.shots,\n",
    "            \"Differentiation Method\": self.policy.circuit.diff_method,\n",
    "            \"Entanglement\": self.policy.circuit.entanglement,\n",
    "            \"Entanglement Pattern\": self.policy.circuit.entanglement_pattern,\n",
    "            \"Entanglement Gate\": self.policy.circuit.entanglement_gate,\n",
    "            \"Input Scaling\": self.policy.circuit.input_scaling,\n",
    "            \"Input Initialization\": get_function_representation(self.policy.circuit.input_init),\n",
    "            \"Weight Initialization\": get_function_representation(self.policy.circuit.weight_init),\n",
    "            \"Measure\": get_function_representation(self.policy.circuit.measure),\n",
    "            \"Policy Post Processing\": self.policy.post_processing.policy_type,\n",
    "            \"Optimizers\": str(self.policy_optimizer),\n",
    "            \"Envinronment\": str(self.env_name),\n",
    "            \"Gamma (discounting factor)\": self.gamma,\n",
    "        }\n",
    "\n",
    "        if self.policy.post_processing.policy_type in ['softmax', 'softmax_probs']:\n",
    "            agent_variables.update({\n",
    "                \"Softmax scheduling (in case policy is softmax)\": f\"{self.policy.post_processing.beta_scheduling}. Starting beta: {self.policy.post_processing.beta}. Increase rate: {self.policy.post_processing.increase_rate}\",\n",
    "                \"Softmax output scaling (in case policy is softmax)\": f\"{self.policy.post_processing.output_scaling}. Output Initialization: {get_function_representation(self.policy.post_processing.output_init)}\",\n",
    "            })\n",
    "        with open(os.path.join(main_path, \"agent_characteristics.json\"), \"w\") as f:\n",
    "            json.dump(agent_variables, f, indent=4)\n",
    "\n",
    "    def save_data(self, run_path):\n",
    "        '''\n",
    "        Saves the data into a .npz file of each episode\n",
    "\n",
    "        '''\n",
    "\n",
    "        if not os.path.exists(run_path):\n",
    "            os.makedirs(run_path)\n",
    "\n",
    "        if os.path.exists(run_path):\n",
    "            data = np.load(run_path, allow_pickle=True)\n",
    "            old_episode_reward = data['episode_reward'].tolist()\n",
    "            old_loss = data['loss'].tolist()\n",
    "            old_runtime = data['runtime'].tolist()\n",
    "            old_params_gradients = data['params_gradients'].tolist()\n",
    "            old_input_params_gradients = data['input_params_gradients'].tolist()\n",
    "        else:\n",
    "            old_episode_reward = []\n",
    "            old_loss = []\n",
    "            old_runtime = []\n",
    "            old_params_gradients = []\n",
    "            old_input_params_gradients = []\n",
    "\n",
    "\n",
    "        old_episode_reward.append(self.scores_deque[-1])\n",
    "        old_loss.append(self.loss.item())\n",
    "        old_runtime.append(self.runtime)\n",
    "        old_params_gradients.append(tensor_to_list(self.policy.get_gradients()[0]))\n",
    "        old_input_params_gradients.append(tensor_to_list(self.policy.get_gradients()[1]))\n",
    "\n",
    "        np.savez_compressed(run_path, episode_reward = np.array(old_episode_reward),\n",
    "                                 loss = np.array(old_loss),\n",
    "                                 runtime = np.array(old_runtime),\n",
    "                                 params_gradients = np.array(old_params_gradients), \n",
    "                                 input_params_gradients = np.array(old_input_params_gradients))\n",
    "        \n",
    "        del old_episode_reward[:]\n",
    "        del old_loss[:]\n",
    "        del old_runtime[:]\n",
    "        del old_params_gradients[:]\n",
    "        del old_input_params_gradients[:]\n",
    "\n",
    "    def writer_function(self, writer, iteration):\n",
    "        '''\n",
    "        Stores data into a tensorboard session\n",
    "\n",
    "        '''\n",
    "        writer.add_scalar(\"Episode Reward\", self.scores_deque[-1], global_step=iteration)\n",
    "        writer.add_scalar(\"Runtime\", self.runtime, global_step=iteration)\n",
    "        writer.add_scalar(\"Loss\", self.loss.item(), global_step=iteration)\n",
    "        if self.policy.post_processing.policy_type in ['softmax', 'softmax_probs']:\n",
    "            writer.add_scalar(\"Beta\", self.policy.post_processing.beta, global_step=iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceSimpleBaselineAgent:\n",
    "\n",
    "    def __init__(self, policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy = policy.to(self.device)\n",
    "        self.policy_optimizer = policy_optimizer\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env_name = env_name\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "        self.solved = False\n",
    "        self.scores_deque = deque(maxlen=self.print_every)\n",
    "\n",
    "    def train(self, run_count=None, path=None, tensorboard=False):\n",
    "\n",
    "        if run_count is not None and path is not None:\n",
    "            base_path = create_directory(path)\n",
    "            env_folder = create_directory(os.path.join(base_path, self.env_name))\n",
    "            experiment_folder_name = f\"{self.policy.post_processing.policy_type}_{self.policy.circuit.n_layers}layer_{datetime.now().strftime('%Y-%m-%d_%H.%M')}\"\n",
    "            experiment_path = create_directory(os.path.join(env_folder, experiment_folder_name))\n",
    "            run_path = create_directory(os.path.join(experiment_path, f'run_{str(run_count)}'))\n",
    "            self.save_agent_data(run_path)\n",
    "        \n",
    "        if tensorboard:\n",
    "            writer = SummaryWriter(log_dir=run_path)\n",
    "        \n",
    "        for i in range(1, self.n_episodes + 1):\n",
    "            start_time = time.time()\n",
    "            self.get_trajectory()\n",
    "            self.update_policy()\n",
    "            end_time = time.time()\n",
    "            self.runtime = end_time - start_time\n",
    "            if tensorboard:\n",
    "                self.writer_function(writer, i)\n",
    "            if run_count is not None and path is not None:\n",
    "                self.save_data(run_path)\n",
    "            if not self.solved:\n",
    "                self.policy.post_processing.beta_schedule()\n",
    "            \n",
    "            if np.mean(self.scores_deque) > self.env.spec.reward_threshold:\n",
    "                self.solved = True\n",
    "\n",
    "            if i % self.print_every == 0 and self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}s\\t Last {} Episodes average reward: {:.2f}\\t '.format(i, self.scores_deque[-1], self.runtime, self.print_every, np.mean(self.scores_deque)))\n",
    "            elif self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}s\\t '.format(i, self.scores_deque[-1], self.runtime))\n",
    "        \n",
    "    def get_trajectory(self):\n",
    "        '''\n",
    "        Gets a trajectory based on the running policy until it runs out of bounds or achieves maximum reward of an episode\n",
    "        '''\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(self.max_t):\n",
    "            if t == 0:\n",
    "                state_tensor = torch.tensor(state[0]).float().to(self.device)\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float().to(self.device)\n",
    "            action, log_prob, _ = self.policy.sample(state_tensor)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            \n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                self.scores_deque.append(sum(self.rewards))\n",
    "                break\n",
    "\n",
    "    def update_policy(self):\n",
    "        '''\n",
    "        Computes the loss and gradients and updates the policy via gradient methods\n",
    "        '''\n",
    "\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns).to(self.device)\n",
    "        #returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        baseline = returns.mean()\n",
    "        for log_prob, ret in zip(self.saved_log_probs, returns):\n",
    "            advantage = ret - baseline\n",
    "            policy_loss.append(-log_prob * advantage)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        self.loss = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        if self.solved is False:\n",
    "            self.policy_optimizer.step()\n",
    "        \n",
    "    def save_agent_data(self,main_path):\n",
    "        '''\n",
    "        Stores the model parameters into a json file\n",
    "\n",
    "        '''\n",
    "        agent_variables = {\n",
    "            \"Model\": self.policy.circuit.__class__.__name__,\n",
    "            \"Number of Qubits\": self.policy.circuit.n_qubits,\n",
    "            \"Number of Layers\": self.policy.circuit.n_layers,\n",
    "            \"Shots\": self.policy.circuit.shots,\n",
    "            \"Differentiation Method\": self.policy.circuit.diff_method,\n",
    "            \"Entanglement\": self.policy.circuit.entanglement,\n",
    "            \"Entanglement Pattern\": self.policy.circuit.entanglement_pattern,\n",
    "            \"Entanglement Gate\": self.policy.circuit.entanglement_gate,\n",
    "            \"Input Scaling\": self.policy.circuit.input_scaling,\n",
    "            \"Input Initialization\": get_function_representation(self.policy.circuit.input_init),\n",
    "            \"Weight Initialization\": get_function_representation(self.policy.circuit.weight_init),\n",
    "            \"Measure\": get_function_representation(self.policy.circuit.measure),\n",
    "            \"Policy Post Processing\": self.policy.post_processing.policy_type,\n",
    "            \"Optimizers\": str(self.policy_optimizer),\n",
    "            \"Envinronment\": str(self.env_name),\n",
    "            \"Gamma (discounting factor)\": self.gamma,\n",
    "        }\n",
    "\n",
    "        if self.policy.post_processing.policy_type in ['softmax', 'softmax_probs']:\n",
    "            agent_variables.update({\n",
    "                \"Softmax scheduling (in case policy is softmax)\": f\"{self.policy.post_processing.beta_scheduling}. Starting beta: {self.policy.post_processing.beta}. Increase rate: {self.policy.post_processing.increase_rate}\",\n",
    "                \"Softmax output scaling (in case policy is softmax)\": f\"{self.policy.post_processing.output_scaling}. Output Initialization: {get_function_representation(self.policy.post_processing.output_init)}\",\n",
    "            })\n",
    "        with open(os.path.join(main_path, \"agent_characteristics.json\"), \"w\") as f:\n",
    "            json.dump(agent_variables, f, indent=4)\n",
    "\n",
    "    def save_data(self, run_path):\n",
    "        '''\n",
    "        Saves the data into a .npz file of each episode\n",
    "\n",
    "        '''\n",
    "\n",
    "        if not os.path.exists(run_path):\n",
    "            os.makedirs(run_path)\n",
    "\n",
    "        if os.path.exists(run_path):\n",
    "            data = np.load(run_path, allow_pickle=True)\n",
    "            old_episode_reward = data['episode_reward'].tolist()\n",
    "            old_loss = data['loss'].tolist()\n",
    "            old_runtime = data['runtime'].tolist()\n",
    "            old_params_gradients = data['params_gradients'].tolist()\n",
    "            old_input_params_gradients = data['input_params_gradients'].tolist()\n",
    "        else:\n",
    "            old_episode_reward = []\n",
    "            old_loss = []\n",
    "            old_runtime = []\n",
    "            old_params_gradients = []\n",
    "            old_input_params_gradients = []\n",
    "\n",
    "\n",
    "        old_episode_reward.append(self.scores_deque[-1])\n",
    "        old_loss.append(self.loss.item())\n",
    "        old_runtime.append(self.runtime)\n",
    "        old_params_gradients.append(tensor_to_list(self.policy.get_gradients()[0]))\n",
    "        old_input_params_gradients.append(tensor_to_list(self.policy.get_gradients()[1]))\n",
    "\n",
    "        np.savez_compressed(run_path, episode_reward = np.array(old_episode_reward),\n",
    "                                 loss = np.array(old_loss),\n",
    "                                 runtime = np.array(old_runtime),\n",
    "                                 params_gradients = np.array(old_params_gradients), \n",
    "                                 input_params_gradients = np.array(old_input_params_gradients))\n",
    "        \n",
    "        del old_episode_reward[:]\n",
    "        del old_loss[:]\n",
    "        del old_runtime[:]\n",
    "        del old_params_gradients[:]\n",
    "        del old_input_params_gradients[:]\n",
    "\n",
    "    def writer_function(self, writer, iteration):\n",
    "        '''\n",
    "        Stores data into a tensorboard session\n",
    "\n",
    "        '''\n",
    "        writer.add_scalar(\"Episode Reward\", self.scores_deque[-1], global_step=iteration)\n",
    "        writer.add_scalar(\"Runtime\", self.runtime, global_step=iteration)\n",
    "        writer.add_scalar(\"Loss\", self.loss.item(), global_step=iteration)\n",
    "        if self.policy.post_processing.policy_type in ['softmax', 'softmax_probs']:\n",
    "            writer.add_scalar(\"Beta\", self.policy.post_processing.beta, global_step=iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceDynamicBaselineAgent:\n",
    "\n",
    "    def __init__(self, policy, policy_optimizer, value_net, value_net_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose):\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy = policy.to(self.device)\n",
    "        self.policy_optimizer = policy_optimizer\n",
    "        self.value_net = value_net.to(self.device)\n",
    "        self.value_net_optimizer = value_net_optimizer\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env_name = env_name\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "        self.solved = False\n",
    "        self.scores_deque = deque(maxlen=self.print_every)\n",
    "\n",
    "\n",
    "    def train(self, run_count=None, path=None, tensorboard=False):\n",
    "        if run_count is not None and path is not None:\n",
    "            base_path = create_directory(path)\n",
    "            env_folder = create_directory(os.path.join(base_path, self.env_name))\n",
    "            experiment_folder_name = f\"{self.policy.post_processing.policy_type}_{self.policy.circuit.n_layers}layer_{datetime.now().strftime('%Y-%m-%d_%H.%M')}\"\n",
    "            experiment_path = create_directory(os.path.join(env_folder, experiment_folder_name))\n",
    "            run_path = create_directory(os.path.join(experiment_path, f'run_{str(run_count)}'))\n",
    "            self.save_agent_data(run_path)\n",
    "\n",
    "        if tensorboard:\n",
    "            writer = SummaryWriter(log_dir=run_path)\n",
    "\n",
    "        for i in range(1, self.n_episodes):\n",
    "            start_time = time.time()\n",
    "            self.get_trajectory()\n",
    "            self.update_policy()\n",
    "            end_time = time.time()\n",
    "            self.runtime = end_time - start_time\n",
    "            if tensorboard:\n",
    "                self.writer_function(writer, i)\n",
    "            if run_count is not None and path is not None:\n",
    "                self.save_data(run_path)\n",
    "            if not self.solved:\n",
    "                self.policy.post_processing.beta_schedule()\n",
    "\n",
    "            if np.mean(self.scores_deque) > self.env.spec.reward_threshold:\n",
    "                self.solved = True\n",
    "\n",
    "            if i % self.print_every == 0 and self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}s\\t Last {} Episodes average reward: {:.2f}\\t '.format(i, self.scores_deque[-1], self.runtime, self.print_every, np.mean(self.scores_deque)))\n",
    "                print(self.scores_deque, np.mean(self.scores_deque))            \n",
    "            elif self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}s\\t '.format(i, self.scores_deque[-1], self.runtime))\n",
    "\n",
    "    def get_trajectory(self):\n",
    "        '''\n",
    "        Gets a trajectory based on the running policy until it runs out of bounds or achieves maximum reward of an episode\n",
    "        '''\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.states = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(self.max_t):\n",
    "            if t == 0:\n",
    "                state_tensor = torch.tensor(state[0]).float().to(self.device)\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float().to(self.device)\n",
    "            self.states.append(state_tensor)\n",
    "            action, log_prob, _ = self.policy.sample(state_tensor)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                self.scores_deque.append(sum(self.rewards))\n",
    "                break\n",
    "\n",
    "    def update_policy(self):\n",
    "        '''\n",
    "        Computes the loss and gradients and updates the policy via gradient methods\n",
    "        '''\n",
    "\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        value_net_loss = []\n",
    "        returns = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns).to(self.device)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        for log_prob, ret, state in zip(self.saved_log_probs, returns, self.states):\n",
    "            value_estimate = self.value_net.forward(state)\n",
    "            advantage = ret - value_estimate.item()\n",
    "            policy_loss.append(-log_prob * advantage)\n",
    "            value_net_loss.append(F.mse_loss(value_estimate, torch.tensor([ret]).to(self.device)))\n",
    "\n",
    "        self.policy_loss = torch.cat([torch.unsqueeze(loss, 0) for loss in policy_loss]).sum()\n",
    "        self.value_loss = torch.cat([torch.unsqueeze(loss, 0) for loss in value_net_loss]).sum()\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        self.policy_loss.backward()\n",
    "        if not self.solved:\n",
    "            self.policy_optimizer.step()\n",
    "\n",
    "        self.value_net_optimizer.zero_grad()\n",
    "        self.value_loss.backward()\n",
    "        if not self.solved:\n",
    "            self.value_net_optimizer.step()\n",
    "        \n",
    "    def save_agent_data(self,main_path):\n",
    "        '''\n",
    "        Stores the model parameters into a json file\n",
    "\n",
    "        '''\n",
    "        agent_variables = {\n",
    "            \"Model\": self.policy.circuit.__class__.__name__,\n",
    "            \"Number of Qubits\": self.policy.circuit.n_qubits,\n",
    "            \"Number of Layers\": self.policy.circuit.n_layers,\n",
    "            \"Shots\": self.policy.circuit.shots,\n",
    "            \"Differentiation Method\": self.policy.circuit.diff_method,\n",
    "            \"Entanglement\": self.policy.circuit.entanglement,\n",
    "            \"Entanglement Pattern\": self.policy.circuit.entanglement_pattern,\n",
    "            \"Entanglement Gate\": self.policy.circuit.entanglement_gate,\n",
    "            \"Input Scaling\": self.policy.circuit.input_scaling,\n",
    "            \"Input Initialization\": get_function_representation(self.policy.circuit.input_init),\n",
    "            \"Weight Initialization\": get_function_representation(self.policy.circuit.weight_init),\n",
    "            \"Measure\": get_function_representation(self.policy.circuit.measure),\n",
    "            \"Policy Post Processing\": self.policy.post_processing.policy_type,\n",
    "            \"Optimizers\": str(self.optimizer),\n",
    "            \"Envinronment\": str(self.env_name),\n",
    "            \"Gamma (discounting factor)\": self.gamma,\n",
    "        }\n",
    "\n",
    "        if self.policy.post_processing.policy_type in ['softmax', 'softmax_probs']:\n",
    "            agent_variables.update({\n",
    "                \"Softmax scheduling (in case policy is softmax)\": f\"{self.policy.post_processing.beta_scheduling}. Starting beta: {self.policy.post_processing.beta}. Increase rate: {self.policy.post_processing.increase_rate}\",\n",
    "                \"Softmax output scaling (in case policy is softmax)\": f\"{self.policy.post_processing.output_scaling}. Output Initialization: {get_function_representation(self.policy.post_processing.output_init)}\",\n",
    "            })\n",
    "        with open(os.path.join(main_path, \"agent_characteristics.json\"), \"w\") as f:\n",
    "            json.dump(agent_variables, f, indent=4)\n",
    "\n",
    "    def save_data(self, run_path):\n",
    "        '''\n",
    "        Saves the data into a .npz file of each episode\n",
    "\n",
    "        '''\n",
    "\n",
    "        if not os.path.exists(run_path):\n",
    "            os.makedirs(run_path)\n",
    "\n",
    "        if os.path.exists(run_path):\n",
    "            data = np.load(run_path, allow_pickle=True)\n",
    "            old_episode_reward = data['episode_reward'].tolist()\n",
    "            old_loss = data['loss'].tolist()\n",
    "            old_runtime = data['runtime'].tolist()\n",
    "            old_params_gradients = data['params_gradients'].tolist()\n",
    "            old_input_params_gradients = data['input_params_gradients'].tolist()\n",
    "        else:\n",
    "            old_episode_reward = []\n",
    "            old_loss = []\n",
    "            old_runtime = []\n",
    "            old_params_gradients = []\n",
    "            old_input_params_gradients = []\n",
    "\n",
    "\n",
    "        old_episode_reward.append(self.scores_deque[-1])\n",
    "        old_loss.append(self.loss.item())\n",
    "        old_runtime.append(self.runtime)\n",
    "        old_params_gradients.append(tensor_to_list(self.policy.get_gradients()[0]))\n",
    "        old_input_params_gradients.append(tensor_to_list(self.policy.get_gradients()[1]))\n",
    "\n",
    "        np.savez_compressed(run_path, episode_reward = np.array(old_episode_reward),\n",
    "                                 loss = np.array(old_loss),\n",
    "                                 runtime = np.array(old_runtime),\n",
    "                                 params_gradients = np.array(old_params_gradients), \n",
    "                                 input_params_gradients = np.array(old_input_params_gradients))\n",
    "        \n",
    "        del old_episode_reward[:]\n",
    "        del old_loss[:]\n",
    "        del old_runtime[:]\n",
    "        del old_params_gradients[:]\n",
    "        del old_input_params_gradients[:]\n",
    "\n",
    "    def writer_function(self, writer, iteration):\n",
    "        '''\n",
    "        Stores data into a tensorboard session\n",
    "\n",
    "        '''\n",
    "        writer.add_scalar(\"Episode Reward\", self.scores_deque[-1], global_step=iteration)\n",
    "        writer.add_scalar(\"Runtime\", self.runtime, global_step=iteration)\n",
    "        writer.add_scalar(\"Loss\", self.loss.item(), global_step=iteration)\n",
    "        if self.policy.post_processing.policy_type in ['softmax', 'softmax_probs']:\n",
    "            writer.add_scalar(\"Beta\", self.policy.post_processing.beta, global_step=iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jerbi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 4300x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 4\n",
    "shots = None\n",
    "diff_method = 'backprop' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = torch.nn.init.ones_\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval_global\n",
    "policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "policy_circuit.visualize_circuit()\n",
    "#policy_circuit.circuit_spectrum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 4\n",
    "shots = None\n",
    "diff_method = 'best' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = torch.nn.init.ones_\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).normal_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval_global\n",
    "policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "#value_circuit_measure = one_measure_expval_global\n",
    "#value_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "#                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "#                     input_scaling, input_init, weight_init, value_circuit_measure)\n",
    "\n",
    "n_actions = 2\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = True\n",
    "beta = 1\n",
    "increase_rate = 0.0005\n",
    "output_scaling = True\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                         post_processing, \n",
    "                         beta_scheduling, \n",
    "                         beta, increase_rate, \n",
    "                         output_scaling, \n",
    "                         output_init)\n",
    "\n",
    "policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "value_lr_list = [0.01, 0.1]\n",
    "#value_params = list(value_circuit.parameters())\n",
    "#value_optimizer = create_optimizer_with_lr(value_params, value_lr_list)\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "n_episodes = 5000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 10\n",
    "verbose = 1\n",
    "reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "#reinforce_update = ReinforceBatchAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "#reinforce_update = ReinforceSimpleBaselineAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "#reinforce_update = ReinforceDynamicBaselineAgent(policy, policy_optimizer, value_circuit, value_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1900x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_qubits = 2\n",
    "n_layers = 4\n",
    "state_dim = 4\n",
    "shots = None\n",
    "diff_method = 'best' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval_global\n",
    "policy_circuit = UQC(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     weight_init, policy_circuit_measure)\n",
    "\n",
    "policy_circuit.visualize_circuit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 1\n",
    "n_layers = 4\n",
    "state_dim = 4\n",
    "shots = None\n",
    "diff_method = 'best' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval_global\n",
    "policy_circuit = UQC(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     weight_init, policy_circuit_measure)\n",
    "\n",
    "#value_circuit_measure = one_measure_expval_global\n",
    "#value_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "#                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "#                     input_scaling, input_init, weight_init, value_circuit_measure)\n",
    "\n",
    "n_actions = 2\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = False\n",
    "beta = 1\n",
    "increase_rate = 0.003\n",
    "output_scaling = False\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                         post_processing, \n",
    "                         beta_scheduling, \n",
    "                         beta, increase_rate, \n",
    "                         output_scaling, \n",
    "                         output_init)\n",
    "\n",
    "policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "value_lr_list = [0.01, 0.1]\n",
    "#value_params = list(value_circuit.parameters())\n",
    "#value_optimizer = create_optimizer_with_lr(value_params, value_lr_list)\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "n_episodes = 5000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 10\n",
    "verbose = 1\n",
    "reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "#reinforce_update = ReinforceBatchAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "#reinforce_update = ReinforceSimpleBaselineAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "#reinforce_update = ReinforceDynamicBaselineAgent(policy, policy_optimizer, value_circuit, value_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFQ Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 1\n",
    "shots = None\n",
    "diff_method = 'best' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"ring\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = torch.nn.init.ones_\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval_global\n",
    "policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "policy_circuit.visualize_circuit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tLast reward: 25.00\tRuntime: 0.60s\t \n",
      "Episode 2\tLast reward: 9.00\tRuntime: 0.19s\t \n",
      "Episode 3\tLast reward: 25.00\tRuntime: 0.49s\t \n",
      "Episode 4\tLast reward: 16.00\tRuntime: 0.34s\t \n",
      "Episode 5\tLast reward: 12.00\tRuntime: 0.25s\t \n",
      "Episode 6\tLast reward: 22.00\tRuntime: 0.48s\t \n",
      "Episode 7\tLast reward: 20.00\tRuntime: 0.44s\t \n",
      "Episode 8\tLast reward: 47.00\tRuntime: 0.92s\t \n",
      "Episode 9\tLast reward: 19.00\tRuntime: 0.40s\t \n",
      "Episode 10\tLast reward: 19.00\tRuntime: 0.39s\t Last 10 Episodes average reward: 21.40\t \n",
      "Episode 11\tLast reward: 10.00\tRuntime: 0.21s\t \n",
      "Episode 12\tLast reward: 14.00\tRuntime: 0.28s\t \n",
      "Episode 13\tLast reward: 27.00\tRuntime: 0.53s\t \n",
      "Episode 14\tLast reward: 66.00\tRuntime: 1.31s\t \n",
      "Episode 15\tLast reward: 43.00\tRuntime: 0.91s\t \n",
      "Episode 16\tLast reward: 15.00\tRuntime: 0.33s\t \n",
      "Episode 17\tLast reward: 17.00\tRuntime: 0.34s\t \n",
      "Episode 18\tLast reward: 39.00\tRuntime: 0.75s\t \n",
      "Episode 19\tLast reward: 24.00\tRuntime: 0.49s\t \n",
      "Episode 20\tLast reward: 17.00\tRuntime: 0.35s\t Last 10 Episodes average reward: 27.20\t \n",
      "Episode 21\tLast reward: 16.00\tRuntime: 0.32s\t \n",
      "Episode 22\tLast reward: 25.00\tRuntime: 0.50s\t \n",
      "Episode 23\tLast reward: 17.00\tRuntime: 0.35s\t \n",
      "Episode 24\tLast reward: 17.00\tRuntime: 0.34s\t \n",
      "Episode 25\tLast reward: 14.00\tRuntime: 0.29s\t \n",
      "Episode 26\tLast reward: 18.00\tRuntime: 0.36s\t \n",
      "Episode 27\tLast reward: 11.00\tRuntime: 0.22s\t \n",
      "Episode 28\tLast reward: 18.00\tRuntime: 0.36s\t \n",
      "Episode 29\tLast reward: 10.00\tRuntime: 0.21s\t \n",
      "Episode 30\tLast reward: 23.00\tRuntime: 0.46s\t Last 10 Episodes average reward: 16.90\t \n",
      "Episode 31\tLast reward: 11.00\tRuntime: 0.22s\t \n",
      "Episode 32\tLast reward: 25.00\tRuntime: 0.49s\t \n",
      "Episode 33\tLast reward: 24.00\tRuntime: 0.48s\t \n",
      "Episode 34\tLast reward: 13.00\tRuntime: 0.26s\t \n",
      "Episode 35\tLast reward: 28.00\tRuntime: 0.55s\t \n",
      "Episode 36\tLast reward: 21.00\tRuntime: 0.43s\t \n",
      "Episode 37\tLast reward: 21.00\tRuntime: 0.42s\t \n",
      "Episode 38\tLast reward: 22.00\tRuntime: 0.44s\t \n",
      "Episode 39\tLast reward: 15.00\tRuntime: 0.30s\t \n",
      "Episode 40\tLast reward: 12.00\tRuntime: 0.24s\t Last 10 Episodes average reward: 19.20\t \n",
      "Episode 41\tLast reward: 37.00\tRuntime: 0.75s\t \n",
      "Episode 42\tLast reward: 11.00\tRuntime: 0.27s\t \n",
      "Episode 43\tLast reward: 51.00\tRuntime: 1.17s\t \n",
      "Episode 44\tLast reward: 25.00\tRuntime: 0.55s\t \n",
      "Episode 45\tLast reward: 29.00\tRuntime: 0.59s\t \n",
      "Episode 46\tLast reward: 34.00\tRuntime: 0.69s\t \n",
      "Episode 47\tLast reward: 21.00\tRuntime: 0.43s\t \n",
      "Episode 48\tLast reward: 18.00\tRuntime: 0.37s\t \n",
      "Episode 49\tLast reward: 10.00\tRuntime: 0.21s\t \n",
      "Episode 50\tLast reward: 19.00\tRuntime: 0.37s\t Last 10 Episodes average reward: 25.50\t \n",
      "Episode 51\tLast reward: 53.00\tRuntime: 1.02s\t \n",
      "Episode 52\tLast reward: 18.00\tRuntime: 0.45s\t \n",
      "Episode 53\tLast reward: 24.00\tRuntime: 0.49s\t \n",
      "Episode 54\tLast reward: 26.00\tRuntime: 0.52s\t \n",
      "Episode 55\tLast reward: 12.00\tRuntime: 0.25s\t \n",
      "Episode 56\tLast reward: 36.00\tRuntime: 0.71s\t \n",
      "Episode 57\tLast reward: 57.00\tRuntime: 1.11s\t \n",
      "Episode 58\tLast reward: 22.00\tRuntime: 0.48s\t \n",
      "Episode 59\tLast reward: 15.00\tRuntime: 0.33s\t \n",
      "Episode 60\tLast reward: 38.00\tRuntime: 0.77s\t Last 10 Episodes average reward: 30.10\t \n",
      "Episode 61\tLast reward: 26.00\tRuntime: 0.56s\t \n",
      "Episode 62\tLast reward: 18.00\tRuntime: 0.38s\t \n",
      "Episode 63\tLast reward: 22.00\tRuntime: 0.45s\t \n",
      "Episode 64\tLast reward: 29.00\tRuntime: 0.57s\t \n",
      "Episode 65\tLast reward: 21.00\tRuntime: 0.43s\t \n",
      "Episode 66\tLast reward: 48.00\tRuntime: 1.09s\t \n",
      "Episode 67\tLast reward: 63.00\tRuntime: 1.46s\t \n",
      "Episode 68\tLast reward: 15.00\tRuntime: 0.35s\t \n",
      "Episode 69\tLast reward: 18.00\tRuntime: 0.38s\t \n",
      "Episode 70\tLast reward: 14.00\tRuntime: 0.30s\t Last 10 Episodes average reward: 27.40\t \n",
      "Episode 71\tLast reward: 22.00\tRuntime: 0.45s\t \n",
      "Episode 72\tLast reward: 56.00\tRuntime: 1.18s\t \n",
      "Episode 73\tLast reward: 23.00\tRuntime: 0.52s\t \n",
      "Episode 74\tLast reward: 34.00\tRuntime: 0.71s\t \n",
      "Episode 75\tLast reward: 16.00\tRuntime: 0.33s\t \n",
      "Episode 76\tLast reward: 40.00\tRuntime: 0.78s\t \n",
      "Episode 77\tLast reward: 30.00\tRuntime: 0.61s\t \n",
      "Episode 78\tLast reward: 37.00\tRuntime: 0.76s\t \n",
      "Episode 79\tLast reward: 13.00\tRuntime: 0.29s\t \n",
      "Episode 80\tLast reward: 31.00\tRuntime: 0.61s\t Last 10 Episodes average reward: 30.20\t \n",
      "Episode 81\tLast reward: 28.00\tRuntime: 0.57s\t \n",
      "Episode 82\tLast reward: 14.00\tRuntime: 0.30s\t \n",
      "Episode 83\tLast reward: 35.00\tRuntime: 0.69s\t \n",
      "Episode 84\tLast reward: 18.00\tRuntime: 0.37s\t \n",
      "Episode 85\tLast reward: 45.00\tRuntime: 0.88s\t \n",
      "Episode 86\tLast reward: 15.00\tRuntime: 0.32s\t \n",
      "Episode 87\tLast reward: 36.00\tRuntime: 0.70s\t \n",
      "Episode 88\tLast reward: 21.00\tRuntime: 0.43s\t \n",
      "Episode 89\tLast reward: 20.00\tRuntime: 0.40s\t \n",
      "Episode 90\tLast reward: 23.00\tRuntime: 0.45s\t Last 10 Episodes average reward: 25.50\t \n",
      "Episode 91\tLast reward: 33.00\tRuntime: 0.64s\t \n",
      "Episode 92\tLast reward: 22.00\tRuntime: 0.48s\t \n",
      "Episode 93\tLast reward: 19.00\tRuntime: 0.39s\t \n",
      "Episode 94\tLast reward: 16.00\tRuntime: 0.32s\t \n",
      "Episode 95\tLast reward: 16.00\tRuntime: 0.32s\t \n",
      "Episode 96\tLast reward: 22.00\tRuntime: 0.44s\t \n",
      "Episode 97\tLast reward: 14.00\tRuntime: 0.29s\t \n",
      "Episode 98\tLast reward: 33.00\tRuntime: 0.68s\t \n",
      "Episode 99\tLast reward: 20.00\tRuntime: 0.41s\t \n",
      "Episode 100\tLast reward: 12.00\tRuntime: 0.25s\t Last 10 Episodes average reward: 20.70\t \n",
      "Episode 101\tLast reward: 16.00\tRuntime: 0.32s\t \n",
      "Episode 102\tLast reward: 80.00\tRuntime: 1.63s\t \n",
      "Episode 103\tLast reward: 24.00\tRuntime: 0.51s\t \n",
      "Episode 104\tLast reward: 42.00\tRuntime: 0.87s\t \n",
      "Episode 105\tLast reward: 23.00\tRuntime: 0.48s\t \n",
      "Episode 106\tLast reward: 31.00\tRuntime: 0.62s\t \n",
      "Episode 107\tLast reward: 20.00\tRuntime: 0.41s\t \n",
      "Episode 108\tLast reward: 38.00\tRuntime: 0.74s\t \n",
      "Episode 109\tLast reward: 25.00\tRuntime: 0.51s\t \n",
      "Episode 110\tLast reward: 20.00\tRuntime: 0.40s\t Last 10 Episodes average reward: 31.90\t \n",
      "Episode 111\tLast reward: 12.00\tRuntime: 0.25s\t \n",
      "Episode 112\tLast reward: 29.00\tRuntime: 0.56s\t \n",
      "Episode 113\tLast reward: 40.00\tRuntime: 0.80s\t \n",
      "Episode 114\tLast reward: 27.00\tRuntime: 0.55s\t \n",
      "Episode 115\tLast reward: 54.00\tRuntime: 1.04s\t \n",
      "Episode 116\tLast reward: 20.00\tRuntime: 0.43s\t \n",
      "Episode 117\tLast reward: 52.00\tRuntime: 1.02s\t \n",
      "Episode 118\tLast reward: 15.00\tRuntime: 0.37s\t \n",
      "Episode 119\tLast reward: 38.00\tRuntime: 0.74s\t \n",
      "Episode 120\tLast reward: 31.00\tRuntime: 0.61s\t Last 10 Episodes average reward: 31.80\t \n",
      "Episode 121\tLast reward: 53.00\tRuntime: 1.03s\t \n",
      "Episode 122\tLast reward: 25.00\tRuntime: 0.52s\t \n",
      "Episode 123\tLast reward: 22.00\tRuntime: 0.44s\t \n",
      "Episode 124\tLast reward: 35.00\tRuntime: 0.69s\t \n",
      "Episode 125\tLast reward: 74.00\tRuntime: 1.83s\t \n",
      "Episode 126\tLast reward: 29.00\tRuntime: 0.63s\t \n",
      "Episode 127\tLast reward: 48.00\tRuntime: 0.94s\t \n",
      "Episode 128\tLast reward: 33.00\tRuntime: 0.67s\t \n",
      "Episode 129\tLast reward: 28.00\tRuntime: 0.57s\t \n",
      "Episode 130\tLast reward: 23.00\tRuntime: 0.46s\t Last 10 Episodes average reward: 37.00\t \n",
      "Episode 131\tLast reward: 39.00\tRuntime: 0.78s\t \n",
      "Episode 132\tLast reward: 19.00\tRuntime: 0.40s\t \n",
      "Episode 133\tLast reward: 34.00\tRuntime: 0.67s\t \n",
      "Episode 134\tLast reward: 28.00\tRuntime: 0.56s\t \n",
      "Episode 135\tLast reward: 24.00\tRuntime: 0.48s\t \n",
      "Episode 136\tLast reward: 22.00\tRuntime: 0.44s\t \n",
      "Episode 137\tLast reward: 34.00\tRuntime: 0.66s\t \n",
      "Episode 138\tLast reward: 39.00\tRuntime: 0.77s\t \n",
      "Episode 139\tLast reward: 32.00\tRuntime: 0.65s\t \n",
      "Episode 140\tLast reward: 47.00\tRuntime: 0.94s\t Last 10 Episodes average reward: 31.80\t \n",
      "Episode 141\tLast reward: 33.00\tRuntime: 0.68s\t \n",
      "Episode 142\tLast reward: 12.00\tRuntime: 0.26s\t \n",
      "Episode 143\tLast reward: 18.00\tRuntime: 0.36s\t \n",
      "Episode 144\tLast reward: 27.00\tRuntime: 0.54s\t \n",
      "Episode 145\tLast reward: 76.00\tRuntime: 1.49s\t \n",
      "Episode 146\tLast reward: 24.00\tRuntime: 0.52s\t \n",
      "Episode 147\tLast reward: 10.00\tRuntime: 0.22s\t \n",
      "Episode 148\tLast reward: 20.00\tRuntime: 0.38s\t \n",
      "Episode 149\tLast reward: 28.00\tRuntime: 0.56s\t \n",
      "Episode 150\tLast reward: 28.00\tRuntime: 0.55s\t Last 10 Episodes average reward: 27.60\t \n",
      "Episode 151\tLast reward: 32.00\tRuntime: 0.63s\t \n",
      "Episode 152\tLast reward: 10.00\tRuntime: 0.21s\t \n",
      "Episode 153\tLast reward: 113.00\tRuntime: 2.18s\t \n",
      "Episode 154\tLast reward: 41.00\tRuntime: 0.88s\t \n",
      "Episode 155\tLast reward: 15.00\tRuntime: 0.33s\t \n",
      "Episode 156\tLast reward: 23.00\tRuntime: 0.46s\t \n",
      "Episode 157\tLast reward: 15.00\tRuntime: 0.31s\t \n",
      "Episode 158\tLast reward: 41.00\tRuntime: 0.80s\t \n",
      "Episode 159\tLast reward: 20.00\tRuntime: 0.42s\t \n",
      "Episode 160\tLast reward: 36.00\tRuntime: 0.72s\t Last 10 Episodes average reward: 34.60\t \n",
      "Episode 161\tLast reward: 20.00\tRuntime: 0.43s\t \n",
      "Episode 162\tLast reward: 17.00\tRuntime: 0.40s\t \n",
      "Episode 163\tLast reward: 16.00\tRuntime: 0.34s\t \n",
      "Episode 164\tLast reward: 60.00\tRuntime: 1.18s\t \n",
      "Episode 165\tLast reward: 21.00\tRuntime: 0.46s\t \n",
      "Episode 166\tLast reward: 57.00\tRuntime: 1.12s\t \n",
      "Episode 167\tLast reward: 53.00\tRuntime: 1.17s\t \n",
      "Episode 168\tLast reward: 42.00\tRuntime: 0.86s\t \n",
      "Episode 169\tLast reward: 14.00\tRuntime: 0.31s\t \n",
      "Episode 170\tLast reward: 18.00\tRuntime: 0.36s\t Last 10 Episodes average reward: 31.80\t \n",
      "Episode 171\tLast reward: 54.00\tRuntime: 1.03s\t \n",
      "Episode 172\tLast reward: 22.00\tRuntime: 0.49s\t \n",
      "Episode 173\tLast reward: 18.00\tRuntime: 0.36s\t \n",
      "Episode 174\tLast reward: 27.00\tRuntime: 0.53s\t \n",
      "Episode 175\tLast reward: 29.00\tRuntime: 0.57s\t \n",
      "Episode 176\tLast reward: 48.00\tRuntime: 0.93s\t \n",
      "Episode 177\tLast reward: 11.00\tRuntime: 0.25s\t \n",
      "Episode 178\tLast reward: 27.00\tRuntime: 0.53s\t \n",
      "Episode 179\tLast reward: 57.00\tRuntime: 1.10s\t \n",
      "Episode 180\tLast reward: 60.00\tRuntime: 1.20s\t Last 10 Episodes average reward: 35.30\t \n",
      "Episode 181\tLast reward: 22.00\tRuntime: 0.52s\t \n",
      "Episode 182\tLast reward: 20.00\tRuntime: 0.41s\t \n",
      "Episode 183\tLast reward: 107.00\tRuntime: 2.05s\t \n",
      "Episode 184\tLast reward: 38.00\tRuntime: 0.80s\t \n",
      "Episode 185\tLast reward: 15.00\tRuntime: 0.33s\t \n",
      "Episode 186\tLast reward: 36.00\tRuntime: 0.69s\t \n",
      "Episode 187\tLast reward: 64.00\tRuntime: 1.25s\t \n",
      "Episode 188\tLast reward: 48.00\tRuntime: 1.00s\t \n",
      "Episode 189\tLast reward: 131.00\tRuntime: 2.60s\t \n",
      "Episode 190\tLast reward: 44.00\tRuntime: 0.95s\t Last 10 Episodes average reward: 52.50\t \n",
      "Episode 191\tLast reward: 44.00\tRuntime: 0.91s\t \n",
      "Episode 192\tLast reward: 38.00\tRuntime: 0.80s\t \n",
      "Episode 193\tLast reward: 37.00\tRuntime: 0.77s\t \n",
      "Episode 194\tLast reward: 36.00\tRuntime: 0.74s\t \n",
      "Episode 195\tLast reward: 14.00\tRuntime: 0.30s\t \n",
      "Episode 196\tLast reward: 11.00\tRuntime: 0.23s\t \n",
      "Episode 197\tLast reward: 32.00\tRuntime: 0.63s\t \n",
      "Episode 198\tLast reward: 41.00\tRuntime: 0.81s\t \n",
      "Episode 199\tLast reward: 30.00\tRuntime: 0.60s\t \n",
      "Episode 200\tLast reward: 79.00\tRuntime: 1.52s\t Last 10 Episodes average reward: 36.20\t \n",
      "Episode 201\tLast reward: 65.00\tRuntime: 1.33s\t \n",
      "Episode 202\tLast reward: 37.00\tRuntime: 0.75s\t \n",
      "Episode 203\tLast reward: 44.00\tRuntime: 0.86s\t \n",
      "Episode 204\tLast reward: 60.00\tRuntime: 1.16s\t \n",
      "Episode 205\tLast reward: 41.00\tRuntime: 0.85s\t \n",
      "Episode 206\tLast reward: 31.00\tRuntime: 0.64s\t \n",
      "Episode 207\tLast reward: 73.00\tRuntime: 1.51s\t \n",
      "Episode 208\tLast reward: 33.00\tRuntime: 0.70s\t \n",
      "Episode 209\tLast reward: 33.00\tRuntime: 0.68s\t \n",
      "Episode 210\tLast reward: 48.00\tRuntime: 0.94s\t Last 10 Episodes average reward: 46.50\t \n",
      "Episode 211\tLast reward: 37.00\tRuntime: 0.74s\t \n",
      "Episode 212\tLast reward: 35.00\tRuntime: 0.71s\t \n",
      "Episode 213\tLast reward: 14.00\tRuntime: 0.30s\t \n",
      "Episode 214\tLast reward: 27.00\tRuntime: 0.53s\t \n",
      "Episode 215\tLast reward: 41.00\tRuntime: 0.79s\t \n",
      "Episode 216\tLast reward: 31.00\tRuntime: 0.65s\t \n",
      "Episode 217\tLast reward: 56.00\tRuntime: 1.11s\t \n",
      "Episode 218\tLast reward: 39.00\tRuntime: 0.80s\t \n",
      "Episode 219\tLast reward: 54.00\tRuntime: 1.08s\t \n",
      "Episode 220\tLast reward: 69.00\tRuntime: 1.43s\t Last 10 Episodes average reward: 40.30\t \n",
      "Episode 221\tLast reward: 23.00\tRuntime: 0.51s\t \n",
      "Episode 222\tLast reward: 43.00\tRuntime: 0.86s\t \n",
      "Episode 223\tLast reward: 27.00\tRuntime: 0.55s\t \n",
      "Episode 224\tLast reward: 54.00\tRuntime: 1.07s\t \n",
      "Episode 225\tLast reward: 50.00\tRuntime: 1.30s\t \n",
      "Episode 226\tLast reward: 72.00\tRuntime: 1.44s\t \n",
      "Episode 227\tLast reward: 75.00\tRuntime: 1.52s\t \n",
      "Episode 228\tLast reward: 30.00\tRuntime: 0.68s\t \n",
      "Episode 229\tLast reward: 38.00\tRuntime: 0.75s\t \n",
      "Episode 230\tLast reward: 88.00\tRuntime: 1.71s\t Last 10 Episodes average reward: 50.00\t \n",
      "Episode 231\tLast reward: 51.00\tRuntime: 1.05s\t \n",
      "Episode 232\tLast reward: 59.00\tRuntime: 1.25s\t \n",
      "Episode 233\tLast reward: 50.00\tRuntime: 1.04s\t \n",
      "Episode 234\tLast reward: 45.00\tRuntime: 0.96s\t \n",
      "Episode 235\tLast reward: 48.00\tRuntime: 0.98s\t \n",
      "Episode 236\tLast reward: 85.00\tRuntime: 1.68s\t \n",
      "Episode 237\tLast reward: 51.00\tRuntime: 1.05s\t \n",
      "Episode 238\tLast reward: 45.00\tRuntime: 0.96s\t \n",
      "Episode 239\tLast reward: 25.00\tRuntime: 0.52s\t \n",
      "Episode 240\tLast reward: 85.00\tRuntime: 1.65s\t Last 10 Episodes average reward: 54.40\t \n",
      "Episode 241\tLast reward: 41.00\tRuntime: 0.86s\t \n",
      "Episode 242\tLast reward: 53.00\tRuntime: 1.07s\t \n",
      "Episode 243\tLast reward: 60.00\tRuntime: 1.25s\t \n",
      "Episode 244\tLast reward: 21.00\tRuntime: 0.46s\t \n",
      "Episode 245\tLast reward: 37.00\tRuntime: 0.74s\t \n",
      "Episode 246\tLast reward: 110.00\tRuntime: 2.21s\t \n",
      "Episode 247\tLast reward: 42.00\tRuntime: 0.90s\t \n",
      "Episode 248\tLast reward: 50.00\tRuntime: 1.01s\t \n",
      "Episode 249\tLast reward: 41.00\tRuntime: 0.84s\t \n",
      "Episode 250\tLast reward: 117.00\tRuntime: 2.31s\t Last 10 Episodes average reward: 57.20\t \n",
      "Episode 251\tLast reward: 31.00\tRuntime: 0.69s\t \n",
      "Episode 252\tLast reward: 27.00\tRuntime: 0.56s\t \n",
      "Episode 253\tLast reward: 16.00\tRuntime: 0.35s\t \n",
      "Episode 254\tLast reward: 36.00\tRuntime: 0.70s\t \n",
      "Episode 255\tLast reward: 46.00\tRuntime: 0.90s\t \n",
      "Episode 256\tLast reward: 91.00\tRuntime: 1.78s\t \n",
      "Episode 257\tLast reward: 26.00\tRuntime: 0.57s\t \n",
      "Episode 258\tLast reward: 87.00\tRuntime: 1.73s\t \n",
      "Episode 259\tLast reward: 111.00\tRuntime: 2.27s\t \n",
      "Episode 260\tLast reward: 11.00\tRuntime: 0.30s\t Last 10 Episodes average reward: 48.20\t \n",
      "Episode 261\tLast reward: 49.00\tRuntime: 0.94s\t \n",
      "Episode 262\tLast reward: 30.00\tRuntime: 0.61s\t \n",
      "Episode 263\tLast reward: 92.00\tRuntime: 1.82s\t \n",
      "Episode 264\tLast reward: 37.00\tRuntime: 0.79s\t \n",
      "Episode 265\tLast reward: 26.00\tRuntime: 0.56s\t \n",
      "Episode 266\tLast reward: 36.00\tRuntime: 0.73s\t \n",
      "Episode 267\tLast reward: 35.00\tRuntime: 0.72s\t \n",
      "Episode 268\tLast reward: 33.00\tRuntime: 0.68s\t \n",
      "Episode 269\tLast reward: 72.00\tRuntime: 1.45s\t \n",
      "Episode 270\tLast reward: 113.00\tRuntime: 2.29s\t Last 10 Episodes average reward: 52.30\t \n",
      "Episode 271\tLast reward: 125.00\tRuntime: 2.53s\t \n",
      "Episode 272\tLast reward: 101.00\tRuntime: 2.12s\t \n",
      "Episode 273\tLast reward: 56.00\tRuntime: 1.20s\t \n",
      "Episode 274\tLast reward: 50.00\tRuntime: 1.04s\t \n",
      "Episode 275\tLast reward: 71.00\tRuntime: 1.76s\t \n",
      "Episode 276\tLast reward: 70.00\tRuntime: 1.95s\t \n",
      "Episode 277\tLast reward: 41.00\tRuntime: 0.91s\t \n",
      "Episode 278\tLast reward: 20.00\tRuntime: 0.45s\t \n",
      "Episode 279\tLast reward: 43.00\tRuntime: 0.85s\t \n",
      "Episode 280\tLast reward: 161.00\tRuntime: 3.14s\t Last 10 Episodes average reward: 73.80\t \n",
      "Episode 281\tLast reward: 17.00\tRuntime: 0.44s\t \n",
      "Episode 282\tLast reward: 40.00\tRuntime: 0.78s\t \n",
      "Episode 283\tLast reward: 94.00\tRuntime: 1.87s\t \n",
      "Episode 284\tLast reward: 41.00\tRuntime: 0.86s\t \n",
      "Episode 285\tLast reward: 73.00\tRuntime: 1.45s\t \n",
      "Episode 286\tLast reward: 102.00\tRuntime: 2.10s\t \n",
      "Episode 287\tLast reward: 45.00\tRuntime: 1.06s\t \n",
      "Episode 288\tLast reward: 88.00\tRuntime: 1.81s\t \n",
      "Episode 289\tLast reward: 45.00\tRuntime: 0.95s\t \n",
      "Episode 290\tLast reward: 151.00\tRuntime: 2.96s\t Last 10 Episodes average reward: 69.60\t \n",
      "Episode 291\tLast reward: 30.00\tRuntime: 0.68s\t \n",
      "Episode 292\tLast reward: 43.00\tRuntime: 0.85s\t \n",
      "Episode 293\tLast reward: 55.00\tRuntime: 1.08s\t \n",
      "Episode 294\tLast reward: 116.00\tRuntime: 2.34s\t \n",
      "Episode 295\tLast reward: 15.00\tRuntime: 0.38s\t \n",
      "Episode 296\tLast reward: 210.00\tRuntime: 4.06s\t \n",
      "Episode 297\tLast reward: 49.00\tRuntime: 1.10s\t \n",
      "Episode 298\tLast reward: 165.00\tRuntime: 3.27s\t \n",
      "Episode 299\tLast reward: 62.00\tRuntime: 1.34s\t \n",
      "Episode 300\tLast reward: 9.00\tRuntime: 0.25s\t Last 10 Episodes average reward: 75.40\t \n",
      "Episode 301\tLast reward: 38.00\tRuntime: 0.74s\t \n",
      "Episode 302\tLast reward: 25.00\tRuntime: 0.51s\t \n",
      "Episode 303\tLast reward: 90.00\tRuntime: 1.78s\t \n",
      "Episode 304\tLast reward: 12.00\tRuntime: 0.29s\t \n",
      "Episode 305\tLast reward: 43.00\tRuntime: 0.87s\t \n",
      "Episode 306\tLast reward: 19.00\tRuntime: 0.40s\t \n",
      "Episode 307\tLast reward: 112.00\tRuntime: 2.17s\t \n",
      "Episode 308\tLast reward: 147.00\tRuntime: 3.01s\t \n",
      "Episode 309\tLast reward: 106.00\tRuntime: 2.17s\t \n",
      "Episode 310\tLast reward: 43.00\tRuntime: 0.96s\t Last 10 Episodes average reward: 63.50\t \n",
      "Episode 311\tLast reward: 100.00\tRuntime: 2.03s\t \n",
      "Episode 312\tLast reward: 141.00\tRuntime: 2.85s\t \n",
      "Episode 313\tLast reward: 45.00\tRuntime: 1.22s\t \n",
      "Episode 314\tLast reward: 30.00\tRuntime: 0.69s\t \n",
      "Episode 315\tLast reward: 49.00\tRuntime: 0.96s\t \n",
      "Episode 316\tLast reward: 40.00\tRuntime: 0.80s\t \n",
      "Episode 317\tLast reward: 98.00\tRuntime: 1.92s\t \n",
      "Episode 318\tLast reward: 13.00\tRuntime: 0.33s\t \n",
      "Episode 319\tLast reward: 102.00\tRuntime: 2.03s\t \n",
      "Episode 320\tLast reward: 136.00\tRuntime: 2.72s\t Last 10 Episodes average reward: 75.40\t \n",
      "Episode 321\tLast reward: 169.00\tRuntime: 3.44s\t \n",
      "Episode 322\tLast reward: 134.00\tRuntime: 2.92s\t \n",
      "Episode 323\tLast reward: 32.00\tRuntime: 0.78s\t \n",
      "Episode 324\tLast reward: 132.00\tRuntime: 2.62s\t \n",
      "Episode 325\tLast reward: 14.00\tRuntime: 0.36s\t \n",
      "Episode 326\tLast reward: 101.00\tRuntime: 1.96s\t \n",
      "Episode 327\tLast reward: 51.00\tRuntime: 1.06s\t \n",
      "Episode 328\tLast reward: 132.00\tRuntime: 2.66s\t \n",
      "Episode 329\tLast reward: 144.00\tRuntime: 2.93s\t \n",
      "Episode 330\tLast reward: 87.00\tRuntime: 1.81s\t Last 10 Episodes average reward: 99.60\t \n",
      "Episode 331\tLast reward: 97.00\tRuntime: 2.04s\t \n",
      "Episode 332\tLast reward: 41.00\tRuntime: 0.88s\t \n",
      "Episode 333\tLast reward: 220.00\tRuntime: 4.34s\t \n",
      "Episode 334\tLast reward: 30.00\tRuntime: 0.73s\t \n",
      "Episode 335\tLast reward: 16.00\tRuntime: 0.37s\t \n",
      "Episode 336\tLast reward: 115.00\tRuntime: 2.23s\t \n",
      "Episode 337\tLast reward: 81.00\tRuntime: 1.64s\t \n",
      "Episode 338\tLast reward: 42.00\tRuntime: 0.90s\t \n",
      "Episode 339\tLast reward: 49.00\tRuntime: 0.97s\t \n",
      "Episode 340\tLast reward: 119.00\tRuntime: 2.35s\t Last 10 Episodes average reward: 81.00\t \n",
      "Episode 341\tLast reward: 107.00\tRuntime: 2.14s\t \n",
      "Episode 342\tLast reward: 122.00\tRuntime: 2.73s\t \n",
      "Episode 343\tLast reward: 46.00\tRuntime: 1.00s\t \n",
      "Episode 344\tLast reward: 45.00\tRuntime: 0.91s\t \n",
      "Episode 345\tLast reward: 80.00\tRuntime: 1.56s\t \n",
      "Episode 346\tLast reward: 94.00\tRuntime: 1.93s\t \n",
      "Episode 347\tLast reward: 109.00\tRuntime: 2.18s\t \n",
      "Episode 348\tLast reward: 26.00\tRuntime: 0.59s\t \n",
      "Episode 349\tLast reward: 163.00\tRuntime: 3.19s\t \n",
      "Episode 350\tLast reward: 145.00\tRuntime: 2.97s\t Last 10 Episodes average reward: 93.70\t \n",
      "Episode 351\tLast reward: 27.00\tRuntime: 0.65s\t \n",
      "Episode 352\tLast reward: 52.00\tRuntime: 1.02s\t \n",
      "Episode 353\tLast reward: 82.00\tRuntime: 1.63s\t \n",
      "Episode 354\tLast reward: 189.00\tRuntime: 3.76s\t \n",
      "Episode 355\tLast reward: 47.00\tRuntime: 1.05s\t \n",
      "Episode 356\tLast reward: 18.00\tRuntime: 0.43s\t \n",
      "Episode 357\tLast reward: 44.00\tRuntime: 0.85s\t \n",
      "Episode 358\tLast reward: 222.00\tRuntime: 4.29s\t \n",
      "Episode 359\tLast reward: 64.00\tRuntime: 1.45s\t \n",
      "Episode 360\tLast reward: 121.00\tRuntime: 2.43s\t Last 10 Episodes average reward: 86.60\t \n",
      "Episode 361\tLast reward: 23.00\tRuntime: 0.53s\t \n",
      "Episode 362\tLast reward: 120.00\tRuntime: 2.36s\t \n",
      "Episode 363\tLast reward: 159.00\tRuntime: 3.16s\t \n",
      "Episode 364\tLast reward: 34.00\tRuntime: 0.79s\t \n",
      "Episode 365\tLast reward: 11.00\tRuntime: 0.27s\t \n",
      "Episode 366\tLast reward: 38.00\tRuntime: 0.73s\t \n",
      "Episode 367\tLast reward: 94.00\tRuntime: 1.81s\t \n",
      "Episode 368\tLast reward: 69.00\tRuntime: 1.44s\t \n",
      "Episode 369\tLast reward: 106.00\tRuntime: 2.17s\t \n",
      "Episode 370\tLast reward: 133.00\tRuntime: 2.68s\t Last 10 Episodes average reward: 78.70\t \n",
      "Episode 371\tLast reward: 62.00\tRuntime: 1.58s\t \n",
      "Episode 372\tLast reward: 97.00\tRuntime: 1.93s\t \n",
      "Episode 373\tLast reward: 122.00\tRuntime: 2.45s\t \n",
      "Episode 374\tLast reward: 49.00\tRuntime: 1.08s\t \n",
      "Episode 375\tLast reward: 11.00\tRuntime: 0.29s\t \n",
      "Episode 376\tLast reward: 44.00\tRuntime: 0.86s\t \n",
      "Episode 377\tLast reward: 158.00\tRuntime: 3.12s\t \n",
      "Episode 378\tLast reward: 178.00\tRuntime: 3.60s\t \n",
      "Episode 379\tLast reward: 61.00\tRuntime: 1.41s\t \n",
      "Episode 380\tLast reward: 29.00\tRuntime: 0.65s\t Last 10 Episodes average reward: 81.10\t \n",
      "Episode 381\tLast reward: 85.00\tRuntime: 1.65s\t \n",
      "Episode 382\tLast reward: 19.00\tRuntime: 0.42s\t \n",
      "Episode 383\tLast reward: 84.00\tRuntime: 1.65s\t \n",
      "Episode 384\tLast reward: 217.00\tRuntime: 4.25s\t \n",
      "Episode 385\tLast reward: 46.00\tRuntime: 1.06s\t \n",
      "Episode 386\tLast reward: 94.00\tRuntime: 1.92s\t \n",
      "Episode 387\tLast reward: 25.00\tRuntime: 0.55s\t \n",
      "Episode 388\tLast reward: 57.00\tRuntime: 1.11s\t \n",
      "Episode 389\tLast reward: 61.00\tRuntime: 1.26s\t \n",
      "Episode 390\tLast reward: 39.00\tRuntime: 0.80s\t Last 10 Episodes average reward: 72.70\t \n",
      "Episode 391\tLast reward: 73.00\tRuntime: 1.49s\t \n",
      "Episode 392\tLast reward: 36.00\tRuntime: 0.76s\t \n",
      "Episode 393\tLast reward: 142.00\tRuntime: 2.80s\t \n",
      "Episode 394\tLast reward: 102.00\tRuntime: 2.15s\t \n",
      "Episode 395\tLast reward: 28.00\tRuntime: 0.65s\t \n",
      "Episode 396\tLast reward: 53.00\tRuntime: 1.06s\t \n",
      "Episode 397\tLast reward: 17.00\tRuntime: 0.37s\t \n",
      "Episode 398\tLast reward: 23.00\tRuntime: 0.46s\t \n",
      "Episode 399\tLast reward: 36.00\tRuntime: 0.70s\t \n",
      "Episode 400\tLast reward: 13.00\tRuntime: 0.28s\t Last 10 Episodes average reward: 52.30\t \n",
      "Episode 401\tLast reward: 42.00\tRuntime: 0.81s\t \n",
      "Episode 402\tLast reward: 53.00\tRuntime: 1.03s\t \n",
      "Episode 403\tLast reward: 86.00\tRuntime: 1.81s\t \n",
      "Episode 404\tLast reward: 143.00\tRuntime: 2.98s\t \n",
      "Episode 405\tLast reward: 62.00\tRuntime: 1.62s\t \n",
      "Episode 406\tLast reward: 15.00\tRuntime: 0.39s\t \n",
      "Episode 407\tLast reward: 118.00\tRuntime: 2.39s\t \n",
      "Episode 408\tLast reward: 144.00\tRuntime: 2.90s\t \n",
      "Episode 409\tLast reward: 32.00\tRuntime: 0.76s\t \n",
      "Episode 410\tLast reward: 240.00\tRuntime: 4.82s\t Last 10 Episodes average reward: 93.50\t \n",
      "Episode 411\tLast reward: 36.00\tRuntime: 0.86s\t \n",
      "Episode 412\tLast reward: 20.00\tRuntime: 0.45s\t \n",
      "Episode 413\tLast reward: 35.00\tRuntime: 0.69s\t \n",
      "Episode 414\tLast reward: 130.00\tRuntime: 2.57s\t \n",
      "Episode 415\tLast reward: 85.00\tRuntime: 1.80s\t \n",
      "Episode 416\tLast reward: 85.00\tRuntime: 1.73s\t \n",
      "Episode 417\tLast reward: 120.00\tRuntime: 2.43s\t \n",
      "Episode 418\tLast reward: 97.00\tRuntime: 2.05s\t \n",
      "Episode 419\tLast reward: 26.00\tRuntime: 0.63s\t \n",
      "Episode 420\tLast reward: 122.00\tRuntime: 2.57s\t Last 10 Episodes average reward: 75.60\t \n",
      "Episode 421\tLast reward: 147.00\tRuntime: 3.05s\t \n",
      "Episode 422\tLast reward: 56.00\tRuntime: 1.24s\t \n",
      "Episode 423\tLast reward: 55.00\tRuntime: 1.23s\t \n",
      "Episode 424\tLast reward: 125.00\tRuntime: 2.59s\t \n",
      "Episode 425\tLast reward: 111.00\tRuntime: 2.28s\t \n",
      "Episode 426\tLast reward: 106.00\tRuntime: 2.24s\t \n",
      "Episode 427\tLast reward: 74.00\tRuntime: 1.56s\t \n",
      "Episode 428\tLast reward: 116.00\tRuntime: 2.36s\t \n",
      "Episode 429\tLast reward: 27.00\tRuntime: 0.63s\t \n",
      "Episode 430\tLast reward: 100.00\tRuntime: 1.95s\t Last 10 Episodes average reward: 91.70\t \n",
      "Episode 431\tLast reward: 125.00\tRuntime: 2.54s\t \n",
      "Episode 432\tLast reward: 72.00\tRuntime: 1.59s\t \n",
      "Episode 433\tLast reward: 133.00\tRuntime: 2.90s\t \n",
      "Episode 434\tLast reward: 90.00\tRuntime: 1.86s\t \n",
      "Episode 435\tLast reward: 41.00\tRuntime: 0.92s\t \n",
      "Episode 436\tLast reward: 87.00\tRuntime: 1.77s\t \n",
      "Episode 437\tLast reward: 45.00\tRuntime: 0.94s\t \n",
      "Episode 438\tLast reward: 74.00\tRuntime: 1.49s\t \n",
      "Episode 439\tLast reward: 86.00\tRuntime: 1.77s\t \n",
      "Episode 440\tLast reward: 125.00\tRuntime: 2.51s\t Last 10 Episodes average reward: 87.80\t \n",
      "Episode 441\tLast reward: 85.00\tRuntime: 1.83s\t \n",
      "Episode 442\tLast reward: 92.00\tRuntime: 1.87s\t \n",
      "Episode 443\tLast reward: 110.00\tRuntime: 2.27s\t \n",
      "Episode 444\tLast reward: 135.00\tRuntime: 2.77s\t \n",
      "Episode 445\tLast reward: 203.00\tRuntime: 4.20s\t \n",
      "Episode 446\tLast reward: 20.00\tRuntime: 0.59s\t \n",
      "Episode 447\tLast reward: 170.00\tRuntime: 3.25s\t \n",
      "Episode 448\tLast reward: 64.00\tRuntime: 1.37s\t \n",
      "Episode 449\tLast reward: 94.00\tRuntime: 1.91s\t \n",
      "Episode 450\tLast reward: 55.00\tRuntime: 1.13s\t Last 10 Episodes average reward: 102.80\t \n",
      "Episode 451\tLast reward: 82.00\tRuntime: 1.71s\t \n",
      "Episode 452\tLast reward: 44.00\tRuntime: 0.92s\t \n",
      "Episode 453\tLast reward: 19.00\tRuntime: 0.41s\t \n",
      "Episode 454\tLast reward: 88.00\tRuntime: 1.71s\t \n",
      "Episode 455\tLast reward: 81.00\tRuntime: 1.68s\t \n",
      "Episode 456\tLast reward: 135.00\tRuntime: 2.72s\t \n",
      "Episode 457\tLast reward: 74.00\tRuntime: 1.54s\t \n",
      "Episode 458\tLast reward: 122.00\tRuntime: 2.50s\t \n",
      "Episode 459\tLast reward: 29.00\tRuntime: 0.67s\t \n",
      "Episode 460\tLast reward: 131.00\tRuntime: 2.79s\t Last 10 Episodes average reward: 80.50\t \n",
      "Episode 461\tLast reward: 62.00\tRuntime: 1.32s\t \n",
      "Episode 462\tLast reward: 15.00\tRuntime: 0.37s\t \n",
      "Episode 463\tLast reward: 17.00\tRuntime: 0.35s\t \n",
      "Episode 464\tLast reward: 81.00\tRuntime: 1.56s\t \n",
      "Episode 465\tLast reward: 13.00\tRuntime: 0.30s\t \n",
      "Episode 466\tLast reward: 42.00\tRuntime: 0.84s\t \n",
      "Episode 467\tLast reward: 25.00\tRuntime: 0.54s\t \n",
      "Episode 468\tLast reward: 78.00\tRuntime: 1.52s\t \n",
      "Episode 469\tLast reward: 129.00\tRuntime: 2.62s\t \n",
      "Episode 470\tLast reward: 74.00\tRuntime: 1.56s\t Last 10 Episodes average reward: 53.60\t \n",
      "Episode 471\tLast reward: 30.00\tRuntime: 0.68s\t \n",
      "Episode 472\tLast reward: 97.00\tRuntime: 1.95s\t \n",
      "Episode 473\tLast reward: 63.00\tRuntime: 1.31s\t \n",
      "Episode 474\tLast reward: 52.00\tRuntime: 1.05s\t \n",
      "Episode 475\tLast reward: 30.00\tRuntime: 0.68s\t \n",
      "Episode 476\tLast reward: 162.00\tRuntime: 3.14s\t \n",
      "Episode 477\tLast reward: 92.00\tRuntime: 1.96s\t \n",
      "Episode 478\tLast reward: 188.00\tRuntime: 3.73s\t \n",
      "Episode 479\tLast reward: 32.00\tRuntime: 0.77s\t \n",
      "Episode 480\tLast reward: 78.00\tRuntime: 1.58s\t Last 10 Episodes average reward: 82.40\t \n",
      "Episode 481\tLast reward: 58.00\tRuntime: 1.17s\t \n",
      "Episode 482\tLast reward: 26.00\tRuntime: 0.58s\t \n",
      "Episode 483\tLast reward: 35.00\tRuntime: 0.70s\t \n",
      "Episode 484\tLast reward: 132.00\tRuntime: 2.59s\t \n",
      "Episode 485\tLast reward: 42.00\tRuntime: 0.90s\t \n",
      "Episode 486\tLast reward: 97.00\tRuntime: 1.92s\t \n",
      "Episode 487\tLast reward: 76.00\tRuntime: 1.60s\t \n",
      "Episode 488\tLast reward: 35.00\tRuntime: 0.74s\t \n",
      "Episode 489\tLast reward: 35.00\tRuntime: 0.71s\t \n",
      "Episode 490\tLast reward: 196.00\tRuntime: 3.75s\t Last 10 Episodes average reward: 73.20\t \n",
      "Episode 491\tLast reward: 90.00\tRuntime: 1.92s\t \n",
      "Episode 492\tLast reward: 175.00\tRuntime: 3.52s\t \n",
      "Episode 493\tLast reward: 145.00\tRuntime: 3.41s\t \n",
      "Episode 494\tLast reward: 30.00\tRuntime: 0.76s\t \n",
      "Episode 495\tLast reward: 95.00\tRuntime: 1.91s\t \n",
      "Episode 496\tLast reward: 135.00\tRuntime: 2.85s\t \n",
      "Episode 497\tLast reward: 64.00\tRuntime: 1.45s\t \n",
      "Episode 498\tLast reward: 84.00\tRuntime: 1.86s\t \n",
      "Episode 499\tLast reward: 132.00\tRuntime: 2.69s\t \n",
      "Episode 500\tLast reward: 35.00\tRuntime: 0.80s\t Last 10 Episodes average reward: 98.50\t \n",
      "Episode 501\tLast reward: 55.00\tRuntime: 1.11s\t \n",
      "Episode 502\tLast reward: 72.00\tRuntime: 1.49s\t \n",
      "Episode 503\tLast reward: 98.00\tRuntime: 2.02s\t \n",
      "Episode 504\tLast reward: 39.00\tRuntime: 0.86s\t \n",
      "Episode 505\tLast reward: 76.00\tRuntime: 1.54s\t \n",
      "Episode 506\tLast reward: 106.00\tRuntime: 2.20s\t \n",
      "Episode 507\tLast reward: 82.00\tRuntime: 1.74s\t \n",
      "Episode 508\tLast reward: 86.00\tRuntime: 1.82s\t \n",
      "Episode 509\tLast reward: 57.00\tRuntime: 1.19s\t \n",
      "Episode 510\tLast reward: 53.00\tRuntime: 1.07s\t Last 10 Episodes average reward: 72.40\t \n",
      "Episode 511\tLast reward: 54.00\tRuntime: 1.15s\t \n",
      "Episode 512\tLast reward: 113.00\tRuntime: 2.28s\t \n",
      "Episode 513\tLast reward: 99.00\tRuntime: 2.04s\t \n",
      "Episode 514\tLast reward: 33.00\tRuntime: 0.75s\t \n",
      "Episode 515\tLast reward: 109.00\tRuntime: 2.16s\t \n",
      "Episode 516\tLast reward: 80.00\tRuntime: 1.63s\t \n",
      "Episode 517\tLast reward: 62.00\tRuntime: 1.28s\t \n",
      "Episode 518\tLast reward: 117.00\tRuntime: 2.34s\t \n",
      "Episode 519\tLast reward: 22.00\tRuntime: 0.52s\t \n",
      "Episode 520\tLast reward: 34.00\tRuntime: 0.67s\t Last 10 Episodes average reward: 72.30\t \n",
      "Episode 521\tLast reward: 123.00\tRuntime: 2.41s\t \n",
      "Episode 522\tLast reward: 97.00\tRuntime: 2.01s\t \n",
      "Episode 523\tLast reward: 51.00\tRuntime: 1.09s\t \n",
      "Episode 524\tLast reward: 63.00\tRuntime: 1.34s\t \n",
      "Episode 525\tLast reward: 13.00\tRuntime: 0.31s\t \n",
      "Episode 526\tLast reward: 81.00\tRuntime: 1.60s\t \n",
      "Episode 527\tLast reward: 78.00\tRuntime: 1.56s\t \n",
      "Episode 528\tLast reward: 76.00\tRuntime: 1.58s\t \n",
      "Episode 529\tLast reward: 46.00\tRuntime: 0.96s\t \n",
      "Episode 530\tLast reward: 31.00\tRuntime: 0.86s\t Last 10 Episodes average reward: 65.90\t \n",
      "Episode 531\tLast reward: 24.00\tRuntime: 0.49s\t \n",
      "Episode 532\tLast reward: 59.00\tRuntime: 1.15s\t \n",
      "Episode 533\tLast reward: 63.00\tRuntime: 1.29s\t \n",
      "Episode 534\tLast reward: 13.00\tRuntime: 0.30s\t \n",
      "Episode 535\tLast reward: 32.00\tRuntime: 0.63s\t \n",
      "Episode 536\tLast reward: 58.00\tRuntime: 1.12s\t \n",
      "Episode 537\tLast reward: 133.00\tRuntime: 2.65s\t \n",
      "Episode 538\tLast reward: 47.00\tRuntime: 1.00s\t \n",
      "Episode 539\tLast reward: 50.00\tRuntime: 1.01s\t \n",
      "Episode 540\tLast reward: 37.00\tRuntime: 0.79s\t Last 10 Episodes average reward: 51.60\t \n",
      "Episode 541\tLast reward: 53.00\tRuntime: 1.04s\t \n",
      "Episode 542\tLast reward: 34.00\tRuntime: 0.70s\t \n",
      "Episode 543\tLast reward: 126.00\tRuntime: 2.50s\t \n",
      "Episode 544\tLast reward: 27.00\tRuntime: 0.61s\t \n",
      "Episode 545\tLast reward: 46.00\tRuntime: 0.91s\t \n",
      "Episode 546\tLast reward: 74.00\tRuntime: 1.46s\t \n",
      "Episode 547\tLast reward: 12.00\tRuntime: 0.29s\t \n",
      "Episode 548\tLast reward: 167.00\tRuntime: 3.24s\t \n",
      "Episode 549\tLast reward: 66.00\tRuntime: 1.43s\t \n",
      "Episode 550\tLast reward: 49.00\tRuntime: 1.02s\t Last 10 Episodes average reward: 65.40\t \n",
      "Episode 551\tLast reward: 113.00\tRuntime: 2.28s\t \n",
      "Episode 552\tLast reward: 132.00\tRuntime: 2.69s\t \n",
      "Episode 553\tLast reward: 10.00\tRuntime: 0.31s\t \n",
      "Episode 554\tLast reward: 51.00\tRuntime: 0.99s\t \n",
      "Episode 555\tLast reward: 105.00\tRuntime: 2.03s\t \n",
      "Episode 556\tLast reward: 70.00\tRuntime: 1.49s\t \n",
      "Episode 557\tLast reward: 13.00\tRuntime: 0.32s\t \n",
      "Episode 558\tLast reward: 176.00\tRuntime: 3.38s\t \n",
      "Episode 559\tLast reward: 74.00\tRuntime: 1.58s\t \n",
      "Episode 560\tLast reward: 29.00\tRuntime: 0.64s\t Last 10 Episodes average reward: 77.30\t \n",
      "Episode 561\tLast reward: 64.00\tRuntime: 1.26s\t \n",
      "Episode 562\tLast reward: 105.00\tRuntime: 2.11s\t \n",
      "Episode 563\tLast reward: 83.00\tRuntime: 1.69s\t \n",
      "Episode 564\tLast reward: 37.00\tRuntime: 0.80s\t \n",
      "Episode 565\tLast reward: 186.00\tRuntime: 3.65s\t \n",
      "Episode 566\tLast reward: 51.00\tRuntime: 1.14s\t \n",
      "Episode 567\tLast reward: 81.00\tRuntime: 1.68s\t \n",
      "Episode 568\tLast reward: 149.00\tRuntime: 2.93s\t \n",
      "Episode 569\tLast reward: 224.00\tRuntime: 4.73s\t \n",
      "Episode 570\tLast reward: 119.00\tRuntime: 2.51s\t Last 10 Episodes average reward: 109.90\t \n",
      "Episode 571\tLast reward: 59.00\tRuntime: 1.39s\t \n",
      "Episode 572\tLast reward: 24.00\tRuntime: 0.54s\t \n",
      "Episode 573\tLast reward: 66.00\tRuntime: 1.30s\t \n",
      "Episode 574\tLast reward: 51.00\tRuntime: 1.05s\t \n",
      "Episode 575\tLast reward: 123.00\tRuntime: 2.45s\t \n",
      "Episode 576\tLast reward: 38.00\tRuntime: 0.84s\t \n",
      "Episode 577\tLast reward: 118.00\tRuntime: 2.35s\t \n",
      "Episode 578\tLast reward: 91.00\tRuntime: 1.88s\t \n",
      "Episode 579\tLast reward: 47.00\tRuntime: 1.10s\t \n",
      "Episode 580\tLast reward: 11.00\tRuntime: 0.31s\t Last 10 Episodes average reward: 62.80\t \n",
      "Episode 581\tLast reward: 61.00\tRuntime: 1.21s\t \n",
      "Episode 582\tLast reward: 121.00\tRuntime: 2.54s\t \n",
      "Episode 583\tLast reward: 142.00\tRuntime: 2.86s\t \n",
      "Episode 584\tLast reward: 172.00\tRuntime: 3.52s\t \n",
      "Episode 585\tLast reward: 91.00\tRuntime: 2.05s\t \n",
      "Episode 586\tLast reward: 107.00\tRuntime: 2.52s\t \n",
      "Episode 587\tLast reward: 37.00\tRuntime: 0.94s\t \n",
      "Episode 588\tLast reward: 87.00\tRuntime: 1.92s\t \n",
      "Episode 589\tLast reward: 160.00\tRuntime: 3.31s\t \n",
      "Episode 590\tLast reward: 61.00\tRuntime: 1.33s\t Last 10 Episodes average reward: 103.90\t \n",
      "Episode 591\tLast reward: 123.00\tRuntime: 2.53s\t \n",
      "Episode 592\tLast reward: 82.00\tRuntime: 1.75s\t \n",
      "Episode 593\tLast reward: 13.00\tRuntime: 0.35s\t \n",
      "Episode 594\tLast reward: 25.00\tRuntime: 0.51s\t \n",
      "Episode 595\tLast reward: 120.00\tRuntime: 2.41s\t \n",
      "Episode 596\tLast reward: 99.00\tRuntime: 2.04s\t \n",
      "Episode 597\tLast reward: 36.00\tRuntime: 1.01s\t \n",
      "Episode 598\tLast reward: 37.00\tRuntime: 0.75s\t \n",
      "Episode 599\tLast reward: 169.00\tRuntime: 3.32s\t \n",
      "Episode 600\tLast reward: 42.00\tRuntime: 0.94s\t Last 10 Episodes average reward: 74.60\t \n",
      "Episode 601\tLast reward: 86.00\tRuntime: 1.79s\t \n",
      "Episode 602\tLast reward: 76.00\tRuntime: 1.57s\t \n",
      "Episode 603\tLast reward: 110.00\tRuntime: 2.21s\t \n",
      "Episode 604\tLast reward: 87.00\tRuntime: 1.82s\t \n",
      "Episode 605\tLast reward: 134.00\tRuntime: 2.67s\t \n",
      "Episode 606\tLast reward: 34.00\tRuntime: 0.80s\t \n",
      "Episode 607\tLast reward: 131.00\tRuntime: 2.66s\t \n",
      "Episode 608\tLast reward: 28.00\tRuntime: 0.65s\t \n",
      "Episode 609\tLast reward: 74.00\tRuntime: 1.47s\t \n",
      "Episode 610\tLast reward: 110.00\tRuntime: 2.24s\t Last 10 Episodes average reward: 87.00\t \n",
      "Episode 611\tLast reward: 42.00\tRuntime: 0.92s\t \n",
      "Episode 612\tLast reward: 51.00\tRuntime: 1.03s\t \n",
      "Episode 613\tLast reward: 32.00\tRuntime: 0.67s\t \n",
      "Episode 614\tLast reward: 84.00\tRuntime: 1.70s\t \n",
      "Episode 615\tLast reward: 142.00\tRuntime: 2.81s\t \n",
      "Episode 616\tLast reward: 99.00\tRuntime: 2.10s\t \n",
      "Episode 617\tLast reward: 104.00\tRuntime: 2.14s\t \n",
      "Episode 618\tLast reward: 101.00\tRuntime: 2.09s\t \n",
      "Episode 619\tLast reward: 94.00\tRuntime: 1.95s\t \n",
      "Episode 620\tLast reward: 233.00\tRuntime: 4.67s\t Last 10 Episodes average reward: 98.20\t \n",
      "Episode 621\tLast reward: 18.00\tRuntime: 0.54s\t \n",
      "Episode 622\tLast reward: 148.00\tRuntime: 2.90s\t \n",
      "Episode 623\tLast reward: 183.00\tRuntime: 3.93s\t \n",
      "Episode 624\tLast reward: 24.00\tRuntime: 0.62s\t \n",
      "Episode 625\tLast reward: 111.00\tRuntime: 2.15s\t \n",
      "Episode 626\tLast reward: 57.00\tRuntime: 1.17s\t \n",
      "Episode 627\tLast reward: 77.00\tRuntime: 1.60s\t \n",
      "Episode 628\tLast reward: 91.00\tRuntime: 1.89s\t \n",
      "Episode 629\tLast reward: 35.00\tRuntime: 0.77s\t \n",
      "Episode 630\tLast reward: 47.00\tRuntime: 0.95s\t Last 10 Episodes average reward: 79.10\t \n",
      "Episode 631\tLast reward: 98.00\tRuntime: 1.92s\t \n",
      "Episode 632\tLast reward: 54.00\tRuntime: 1.11s\t \n",
      "Episode 633\tLast reward: 82.00\tRuntime: 1.66s\t \n",
      "Episode 634\tLast reward: 149.00\tRuntime: 2.99s\t \n",
      "Episode 635\tLast reward: 58.00\tRuntime: 1.28s\t \n",
      "Episode 636\tLast reward: 96.00\tRuntime: 1.98s\t \n",
      "Episode 637\tLast reward: 59.00\tRuntime: 1.22s\t \n",
      "Episode 638\tLast reward: 77.00\tRuntime: 1.60s\t \n",
      "Episode 639\tLast reward: 115.00\tRuntime: 2.31s\t \n",
      "Episode 640\tLast reward: 78.00\tRuntime: 1.67s\t Last 10 Episodes average reward: 86.60\t \n",
      "Episode 641\tLast reward: 87.00\tRuntime: 1.78s\t \n",
      "Episode 642\tLast reward: 140.00\tRuntime: 2.83s\t \n",
      "Episode 643\tLast reward: 57.00\tRuntime: 1.25s\t \n",
      "Episode 644\tLast reward: 53.00\tRuntime: 1.11s\t \n",
      "Episode 645\tLast reward: 187.00\tRuntime: 3.73s\t \n",
      "Episode 646\tLast reward: 51.00\tRuntime: 1.13s\t \n",
      "Episode 647\tLast reward: 18.00\tRuntime: 0.48s\t \n",
      "Episode 648\tLast reward: 60.00\tRuntime: 1.18s\t \n",
      "Episode 649\tLast reward: 95.00\tRuntime: 1.95s\t \n",
      "Episode 650\tLast reward: 62.00\tRuntime: 1.29s\t Last 10 Episodes average reward: 81.00\t \n",
      "Episode 651\tLast reward: 76.00\tRuntime: 1.59s\t \n",
      "Episode 652\tLast reward: 102.00\tRuntime: 2.05s\t \n",
      "Episode 653\tLast reward: 72.00\tRuntime: 1.53s\t \n",
      "Episode 654\tLast reward: 100.00\tRuntime: 2.01s\t \n",
      "Episode 655\tLast reward: 92.00\tRuntime: 1.90s\t \n",
      "Episode 656\tLast reward: 65.00\tRuntime: 1.38s\t \n",
      "Episode 657\tLast reward: 48.00\tRuntime: 1.22s\t \n",
      "Episode 658\tLast reward: 70.00\tRuntime: 1.47s\t \n",
      "Episode 659\tLast reward: 50.00\tRuntime: 1.04s\t \n",
      "Episode 660\tLast reward: 84.00\tRuntime: 1.73s\t Last 10 Episodes average reward: 75.90\t \n",
      "Episode 661\tLast reward: 78.00\tRuntime: 1.65s\t \n",
      "Episode 662\tLast reward: 56.00\tRuntime: 1.19s\t \n",
      "Episode 663\tLast reward: 13.00\tRuntime: 0.32s\t \n",
      "Episode 664\tLast reward: 98.00\tRuntime: 1.92s\t \n",
      "Episode 665\tLast reward: 59.00\tRuntime: 1.21s\t \n",
      "Episode 666\tLast reward: 20.00\tRuntime: 0.48s\t \n",
      "Episode 667\tLast reward: 51.00\tRuntime: 1.06s\t \n",
      "Episode 668\tLast reward: 124.00\tRuntime: 2.58s\t \n",
      "Episode 669\tLast reward: 61.00\tRuntime: 1.33s\t \n",
      "Episode 670\tLast reward: 16.00\tRuntime: 0.42s\t Last 10 Episodes average reward: 57.60\t \n",
      "Episode 671\tLast reward: 89.00\tRuntime: 1.77s\t \n",
      "Episode 672\tLast reward: 13.00\tRuntime: 0.31s\t \n",
      "Episode 673\tLast reward: 91.00\tRuntime: 1.79s\t \n",
      "Episode 674\tLast reward: 16.00\tRuntime: 0.37s\t \n",
      "Episode 675\tLast reward: 71.00\tRuntime: 1.39s\t \n",
      "Episode 676\tLast reward: 17.00\tRuntime: 0.38s\t \n",
      "Episode 677\tLast reward: 61.00\tRuntime: 1.20s\t \n",
      "Episode 678\tLast reward: 83.00\tRuntime: 1.71s\t \n",
      "Episode 679\tLast reward: 88.00\tRuntime: 1.83s\t \n",
      "Episode 680\tLast reward: 67.00\tRuntime: 1.41s\t Last 10 Episodes average reward: 59.60\t \n",
      "Episode 681\tLast reward: 84.00\tRuntime: 1.77s\t \n",
      "Episode 682\tLast reward: 78.00\tRuntime: 1.61s\t \n",
      "Episode 683\tLast reward: 32.00\tRuntime: 0.68s\t \n",
      "Episode 684\tLast reward: 64.00\tRuntime: 1.29s\t \n",
      "Episode 685\tLast reward: 50.00\tRuntime: 1.02s\t \n",
      "Episode 686\tLast reward: 33.00\tRuntime: 0.69s\t \n",
      "Episode 687\tLast reward: 9.00\tRuntime: 0.21s\t \n",
      "Episode 688\tLast reward: 87.00\tRuntime: 1.71s\t \n",
      "Episode 689\tLast reward: 69.00\tRuntime: 1.43s\t \n",
      "Episode 690\tLast reward: 42.00\tRuntime: 0.88s\t Last 10 Episodes average reward: 54.80\t \n",
      "Episode 691\tLast reward: 96.00\tRuntime: 1.94s\t \n",
      "Episode 692\tLast reward: 17.00\tRuntime: 0.40s\t \n",
      "Episode 693\tLast reward: 78.00\tRuntime: 1.51s\t \n",
      "Episode 694\tLast reward: 126.00\tRuntime: 2.60s\t \n",
      "Episode 695\tLast reward: 85.00\tRuntime: 1.77s\t \n",
      "Episode 696\tLast reward: 27.00\tRuntime: 0.64s\t \n",
      "Episode 697\tLast reward: 104.00\tRuntime: 2.16s\t \n",
      "Episode 698\tLast reward: 94.00\tRuntime: 1.92s\t \n",
      "Episode 699\tLast reward: 55.00\tRuntime: 1.17s\t \n",
      "Episode 700\tLast reward: 41.00\tRuntime: 1.17s\t Last 10 Episodes average reward: 72.30\t \n",
      "Episode 701\tLast reward: 115.00\tRuntime: 2.29s\t \n",
      "Episode 702\tLast reward: 155.00\tRuntime: 3.07s\t \n",
      "Episode 703\tLast reward: 37.00\tRuntime: 0.85s\t \n",
      "Episode 704\tLast reward: 19.00\tRuntime: 0.43s\t \n",
      "Episode 705\tLast reward: 108.00\tRuntime: 2.28s\t \n",
      "Episode 706\tLast reward: 56.00\tRuntime: 1.17s\t \n",
      "Episode 707\tLast reward: 104.00\tRuntime: 2.11s\t \n",
      "Episode 708\tLast reward: 91.00\tRuntime: 1.84s\t \n",
      "Episode 709\tLast reward: 68.00\tRuntime: 1.47s\t \n",
      "Episode 710\tLast reward: 115.00\tRuntime: 2.30s\t Last 10 Episodes average reward: 86.80\t \n",
      "Episode 711\tLast reward: 57.00\tRuntime: 1.19s\t \n",
      "Episode 712\tLast reward: 91.00\tRuntime: 1.85s\t \n",
      "Episode 713\tLast reward: 84.00\tRuntime: 1.77s\t \n",
      "Episode 714\tLast reward: 71.00\tRuntime: 1.48s\t \n",
      "Episode 715\tLast reward: 46.00\tRuntime: 0.96s\t \n",
      "Episode 716\tLast reward: 61.00\tRuntime: 1.22s\t \n",
      "Episode 717\tLast reward: 126.00\tRuntime: 2.53s\t \n",
      "Episode 718\tLast reward: 75.00\tRuntime: 1.58s\t \n",
      "Episode 719\tLast reward: 78.00\tRuntime: 1.61s\t \n",
      "Episode 720\tLast reward: 71.00\tRuntime: 1.50s\t Last 10 Episodes average reward: 76.00\t \n",
      "Episode 721\tLast reward: 90.00\tRuntime: 1.82s\t \n",
      "Episode 722\tLast reward: 147.00\tRuntime: 2.96s\t \n",
      "Episode 723\tLast reward: 111.00\tRuntime: 2.29s\t \n",
      "Episode 724\tLast reward: 27.00\tRuntime: 0.65s\t \n",
      "Episode 725\tLast reward: 124.00\tRuntime: 2.43s\t \n",
      "Episode 726\tLast reward: 139.00\tRuntime: 2.79s\t \n",
      "Episode 727\tLast reward: 41.00\tRuntime: 0.93s\t \n",
      "Episode 728\tLast reward: 86.00\tRuntime: 1.79s\t \n",
      "Episode 729\tLast reward: 116.00\tRuntime: 2.36s\t \n",
      "Episode 730\tLast reward: 94.00\tRuntime: 2.22s\t Last 10 Episodes average reward: 97.50\t \n",
      "Episode 731\tLast reward: 58.00\tRuntime: 1.24s\t \n",
      "Episode 732\tLast reward: 89.00\tRuntime: 1.83s\t \n",
      "Episode 733\tLast reward: 34.00\tRuntime: 0.73s\t \n",
      "Episode 734\tLast reward: 55.00\tRuntime: 1.09s\t \n",
      "Episode 735\tLast reward: 191.00\tRuntime: 3.73s\t \n",
      "Episode 736\tLast reward: 193.00\tRuntime: 3.97s\t \n",
      "Episode 737\tLast reward: 37.00\tRuntime: 0.93s\t \n",
      "Episode 738\tLast reward: 48.00\tRuntime: 0.98s\t \n",
      "Episode 739\tLast reward: 115.00\tRuntime: 2.27s\t \n",
      "Episode 740\tLast reward: 39.00\tRuntime: 0.85s\t Last 10 Episodes average reward: 85.90\t \n",
      "Episode 741\tLast reward: 136.00\tRuntime: 2.71s\t \n",
      "Episode 742\tLast reward: 93.00\tRuntime: 1.97s\t \n",
      "Episode 743\tLast reward: 50.00\tRuntime: 1.06s\t \n",
      "Episode 744\tLast reward: 78.00\tRuntime: 1.61s\t \n",
      "Episode 745\tLast reward: 231.00\tRuntime: 4.56s\t \n",
      "Episode 746\tLast reward: 65.00\tRuntime: 1.50s\t \n",
      "Episode 747\tLast reward: 20.00\tRuntime: 0.51s\t \n",
      "Episode 748\tLast reward: 83.00\tRuntime: 1.61s\t \n",
      "Episode 749\tLast reward: 14.00\tRuntime: 0.32s\t \n",
      "Episode 750\tLast reward: 49.00\tRuntime: 0.95s\t Last 10 Episodes average reward: 81.90\t \n",
      "Episode 751\tLast reward: 75.00\tRuntime: 1.51s\t \n",
      "Episode 752\tLast reward: 40.00\tRuntime: 0.82s\t \n",
      "Episode 753\tLast reward: 35.00\tRuntime: 0.73s\t \n",
      "Episode 754\tLast reward: 61.00\tRuntime: 1.19s\t \n",
      "Episode 755\tLast reward: 118.00\tRuntime: 2.38s\t \n",
      "Episode 756\tLast reward: 75.00\tRuntime: 1.62s\t \n",
      "Episode 757\tLast reward: 157.00\tRuntime: 3.28s\t \n",
      "Episode 758\tLast reward: 140.00\tRuntime: 2.89s\t \n",
      "Episode 759\tLast reward: 73.00\tRuntime: 1.86s\t \n",
      "Episode 760\tLast reward: 13.00\tRuntime: 0.34s\t Last 10 Episodes average reward: 78.70\t \n",
      "Episode 761\tLast reward: 137.00\tRuntime: 2.70s\t \n",
      "Episode 762\tLast reward: 20.00\tRuntime: 0.48s\t \n",
      "Episode 763\tLast reward: 13.00\tRuntime: 0.27s\t \n",
      "Episode 764\tLast reward: 79.00\tRuntime: 1.59s\t \n",
      "Episode 765\tLast reward: 52.00\tRuntime: 1.05s\t \n",
      "Episode 766\tLast reward: 103.00\tRuntime: 2.09s\t \n",
      "Episode 767\tLast reward: 64.00\tRuntime: 1.33s\t \n",
      "Episode 768\tLast reward: 23.00\tRuntime: 0.50s\t \n",
      "Episode 769\tLast reward: 145.00\tRuntime: 2.84s\t \n",
      "Episode 770\tLast reward: 68.00\tRuntime: 1.43s\t Last 10 Episodes average reward: 70.40\t \n",
      "Episode 771\tLast reward: 117.00\tRuntime: 2.34s\t \n",
      "Episode 772\tLast reward: 179.00\tRuntime: 3.58s\t \n",
      "Episode 773\tLast reward: 158.00\tRuntime: 3.32s\t \n",
      "Episode 774\tLast reward: 34.00\tRuntime: 0.83s\t \n",
      "Episode 775\tLast reward: 31.00\tRuntime: 0.65s\t \n",
      "Episode 776\tLast reward: 15.00\tRuntime: 0.31s\t \n",
      "Episode 777\tLast reward: 26.00\tRuntime: 0.51s\t \n",
      "Episode 778\tLast reward: 102.00\tRuntime: 1.99s\t \n",
      "Episode 779\tLast reward: 140.00\tRuntime: 2.84s\t \n",
      "Episode 780\tLast reward: 57.00\tRuntime: 1.27s\t Last 10 Episodes average reward: 85.90\t \n",
      "Episode 781\tLast reward: 39.00\tRuntime: 0.84s\t \n",
      "Episode 782\tLast reward: 29.00\tRuntime: 0.60s\t \n",
      "Episode 783\tLast reward: 38.00\tRuntime: 0.76s\t \n",
      "Episode 784\tLast reward: 124.00\tRuntime: 2.45s\t \n",
      "Episode 785\tLast reward: 101.00\tRuntime: 2.04s\t \n",
      "Episode 786\tLast reward: 142.00\tRuntime: 2.89s\t \n",
      "Episode 787\tLast reward: 175.00\tRuntime: 3.57s\t \n",
      "Episode 788\tLast reward: 23.00\tRuntime: 0.61s\t \n",
      "Episode 789\tLast reward: 184.00\tRuntime: 3.63s\t \n",
      "Episode 790\tLast reward: 71.00\tRuntime: 1.75s\t Last 10 Episodes average reward: 92.60\t \n",
      "Episode 791\tLast reward: 76.00\tRuntime: 1.68s\t \n",
      "Episode 792\tLast reward: 25.00\tRuntime: 0.58s\t \n",
      "Episode 793\tLast reward: 42.00\tRuntime: 0.85s\t \n",
      "Episode 794\tLast reward: 27.00\tRuntime: 0.56s\t \n",
      "Episode 795\tLast reward: 15.00\tRuntime: 0.32s\t \n",
      "Episode 796\tLast reward: 86.00\tRuntime: 1.70s\t \n",
      "Episode 797\tLast reward: 136.00\tRuntime: 2.76s\t \n",
      "Episode 798\tLast reward: 17.00\tRuntime: 0.43s\t \n",
      "Episode 799\tLast reward: 76.00\tRuntime: 1.49s\t \n",
      "Episode 800\tLast reward: 28.00\tRuntime: 0.58s\t Last 10 Episodes average reward: 52.80\t \n",
      "Episode 801\tLast reward: 147.00\tRuntime: 2.89s\t \n",
      "Episode 802\tLast reward: 41.00\tRuntime: 0.88s\t \n",
      "Episode 803\tLast reward: 54.00\tRuntime: 1.11s\t \n",
      "Episode 804\tLast reward: 65.00\tRuntime: 1.35s\t \n",
      "Episode 805\tLast reward: 133.00\tRuntime: 2.65s\t \n",
      "Episode 806\tLast reward: 47.00\tRuntime: 1.01s\t \n",
      "Episode 807\tLast reward: 67.00\tRuntime: 1.39s\t \n",
      "Episode 808\tLast reward: 55.00\tRuntime: 1.10s\t \n",
      "Episode 809\tLast reward: 137.00\tRuntime: 2.79s\t \n",
      "Episode 810\tLast reward: 39.00\tRuntime: 0.87s\t Last 10 Episodes average reward: 78.50\t \n",
      "Episode 811\tLast reward: 129.00\tRuntime: 2.61s\t \n",
      "Episode 812\tLast reward: 135.00\tRuntime: 2.74s\t \n",
      "Episode 813\tLast reward: 181.00\tRuntime: 3.71s\t \n",
      "Episode 814\tLast reward: 32.00\tRuntime: 0.80s\t \n",
      "Episode 815\tLast reward: 46.00\tRuntime: 0.93s\t \n",
      "Episode 816\tLast reward: 90.00\tRuntime: 1.76s\t \n",
      "Episode 817\tLast reward: 38.00\tRuntime: 0.80s\t \n",
      "Episode 818\tLast reward: 48.00\tRuntime: 0.98s\t \n",
      "Episode 819\tLast reward: 128.00\tRuntime: 2.57s\t \n",
      "Episode 820\tLast reward: 105.00\tRuntime: 2.14s\t Last 10 Episodes average reward: 93.20\t \n",
      "Episode 821\tLast reward: 16.00\tRuntime: 0.41s\t \n",
      "Episode 822\tLast reward: 81.00\tRuntime: 1.57s\t \n",
      "Episode 823\tLast reward: 48.00\tRuntime: 0.97s\t \n",
      "Episode 824\tLast reward: 204.00\tRuntime: 4.26s\t \n",
      "Episode 825\tLast reward: 53.00\tRuntime: 1.20s\t \n",
      "Episode 826\tLast reward: 16.00\tRuntime: 0.43s\t \n",
      "Episode 827\tLast reward: 90.00\tRuntime: 1.73s\t \n",
      "Episode 828\tLast reward: 124.00\tRuntime: 2.54s\t \n",
      "Episode 829\tLast reward: 152.00\tRuntime: 3.09s\t \n",
      "Episode 830\tLast reward: 162.00\tRuntime: 3.35s\t Last 10 Episodes average reward: 94.60\t \n",
      "Episode 831\tLast reward: 92.00\tRuntime: 2.00s\t \n",
      "Episode 832\tLast reward: 139.00\tRuntime: 2.86s\t \n",
      "Episode 833\tLast reward: 130.00\tRuntime: 2.67s\t \n",
      "Episode 834\tLast reward: 152.00\tRuntime: 3.26s\t \n",
      "Episode 835\tLast reward: 134.00\tRuntime: 2.88s\t \n",
      "Episode 836\tLast reward: 106.00\tRuntime: 2.30s\t \n",
      "Episode 837\tLast reward: 33.00\tRuntime: 0.76s\t \n",
      "Episode 838\tLast reward: 220.00\tRuntime: 4.33s\t \n",
      "Episode 839\tLast reward: 36.00\tRuntime: 0.84s\t \n",
      "Episode 840\tLast reward: 141.00\tRuntime: 2.84s\t Last 10 Episodes average reward: 118.30\t \n",
      "Episode 841\tLast reward: 13.00\tRuntime: 0.35s\t \n",
      "Episode 842\tLast reward: 12.00\tRuntime: 0.26s\t \n",
      "Episode 843\tLast reward: 49.00\tRuntime: 0.95s\t \n",
      "Episode 844\tLast reward: 130.00\tRuntime: 2.56s\t \n",
      "Episode 845\tLast reward: 18.00\tRuntime: 0.44s\t \n",
      "Episode 846\tLast reward: 19.00\tRuntime: 0.39s\t \n",
      "Episode 847\tLast reward: 11.00\tRuntime: 0.23s\t \n",
      "Episode 848\tLast reward: 119.00\tRuntime: 2.25s\t \n",
      "Episode 849\tLast reward: 57.00\tRuntime: 1.47s\t \n",
      "Episode 850\tLast reward: 31.00\tRuntime: 0.65s\t Last 10 Episodes average reward: 45.90\t \n",
      "Episode 851\tLast reward: 88.00\tRuntime: 1.73s\t \n",
      "Episode 852\tLast reward: 12.00\tRuntime: 0.29s\t \n",
      "Episode 853\tLast reward: 95.00\tRuntime: 1.87s\t \n",
      "Episode 854\tLast reward: 128.00\tRuntime: 2.59s\t \n",
      "Episode 855\tLast reward: 155.00\tRuntime: 3.18s\t \n",
      "Episode 856\tLast reward: 113.00\tRuntime: 2.35s\t \n",
      "Episode 857\tLast reward: 14.00\tRuntime: 0.40s\t \n",
      "Episode 858\tLast reward: 39.00\tRuntime: 0.77s\t \n",
      "Episode 859\tLast reward: 14.00\tRuntime: 0.29s\t \n",
      "Episode 860\tLast reward: 59.00\tRuntime: 1.12s\t Last 10 Episodes average reward: 71.70\t \n",
      "Episode 861\tLast reward: 142.00\tRuntime: 2.83s\t \n",
      "Episode 862\tLast reward: 100.00\tRuntime: 2.19s\t \n",
      "Episode 863\tLast reward: 54.00\tRuntime: 1.15s\t \n",
      "Episode 864\tLast reward: 17.00\tRuntime: 0.39s\t \n",
      "Episode 865\tLast reward: 113.00\tRuntime: 2.17s\t \n",
      "Episode 866\tLast reward: 104.00\tRuntime: 2.15s\t \n",
      "Episode 867\tLast reward: 13.00\tRuntime: 0.34s\t \n",
      "Episode 868\tLast reward: 12.00\tRuntime: 0.25s\t \n",
      "Episode 869\tLast reward: 177.00\tRuntime: 3.42s\t \n",
      "Episode 870\tLast reward: 33.00\tRuntime: 0.76s\t Last 10 Episodes average reward: 76.50\t \n",
      "Episode 871\tLast reward: 179.00\tRuntime: 3.50s\t \n",
      "Episode 872\tLast reward: 96.00\tRuntime: 2.07s\t \n",
      "Episode 873\tLast reward: 64.00\tRuntime: 1.37s\t \n",
      "Episode 874\tLast reward: 41.00\tRuntime: 0.84s\t \n",
      "Episode 875\tLast reward: 69.00\tRuntime: 1.41s\t \n",
      "Episode 876\tLast reward: 126.00\tRuntime: 2.54s\t \n",
      "Episode 877\tLast reward: 16.00\tRuntime: 0.41s\t \n",
      "Episode 878\tLast reward: 262.00\tRuntime: 5.05s\t \n",
      "Episode 879\tLast reward: 41.00\tRuntime: 0.96s\t \n",
      "Episode 880\tLast reward: 112.00\tRuntime: 2.47s\t Last 10 Episodes average reward: 100.60\t \n",
      "Episode 881\tLast reward: 58.00\tRuntime: 1.22s\t \n",
      "Episode 882\tLast reward: 77.00\tRuntime: 1.59s\t \n",
      "Episode 883\tLast reward: 39.00\tRuntime: 0.87s\t \n",
      "Episode 884\tLast reward: 123.00\tRuntime: 2.43s\t \n",
      "Episode 885\tLast reward: 44.00\tRuntime: 0.95s\t \n",
      "Episode 886\tLast reward: 54.00\tRuntime: 1.11s\t \n",
      "Episode 887\tLast reward: 394.00\tRuntime: 7.76s\t \n",
      "Episode 888\tLast reward: 128.00\tRuntime: 2.86s\t \n",
      "Episode 889\tLast reward: 89.00\tRuntime: 1.97s\t \n",
      "Episode 890\tLast reward: 95.00\tRuntime: 1.95s\t Last 10 Episodes average reward: 110.10\t \n",
      "Episode 891\tLast reward: 93.00\tRuntime: 1.89s\t \n",
      "Episode 892\tLast reward: 109.00\tRuntime: 2.25s\t \n",
      "Episode 893\tLast reward: 198.00\tRuntime: 4.38s\t \n",
      "Episode 894\tLast reward: 187.00\tRuntime: 3.85s\t \n",
      "Episode 895\tLast reward: 58.00\tRuntime: 1.65s\t \n",
      "Episode 896\tLast reward: 228.00\tRuntime: 4.96s\t \n",
      "Episode 897\tLast reward: 198.00\tRuntime: 4.76s\t \n",
      "Episode 898\tLast reward: 73.00\tRuntime: 2.01s\t \n",
      "Episode 899\tLast reward: 97.00\tRuntime: 2.03s\t \n",
      "Episode 900\tLast reward: 140.00\tRuntime: 2.87s\t Last 10 Episodes average reward: 138.10\t \n",
      "Episode 901\tLast reward: 119.00\tRuntime: 2.47s\t \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 55\u001b[0m\n\u001b[1;32m     51\u001b[0m reinforce_update \u001b[38;5;241m=\u001b[39m ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#reinforce_update = ReinforceBatchAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#reinforce_update = ReinforceSimpleBaselineAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#reinforce_update = ReinforceDynamicBaselineAgent(policy, policy_optimizer, value_circuit, value_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[43mreinforce_update\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 32\u001b[0m, in \u001b[0;36mReinforceAgent.train\u001b[0;34m(self, run_count, path, tensorboard)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_episodes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     31\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_policy()\n\u001b[1;32m     34\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[11], line 64\u001b[0m, in \u001b[0;36mReinforceAgent.get_trajectory\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 64\u001b[0m action, log_prob, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaved_log_probs\u001b[38;5;241m.\u001b[39mappend(log_prob)\n",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m, in \u001b[0;36mQuantumPolicy.sample\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    Samples an action from the action probability distribution\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(policy)\n\u001b[1;32m     14\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m, in \u001b[0;36mQuantumPolicy.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    Input state is fed to the circuit - its output is then fed to the post processing \u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcircuit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     probs_processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_processing\u001b[38;5;241m.\u001b[39mforward(probs)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m probs_processed\n",
      "Cell \u001b[0;32mIn[5], line 98\u001b[0m, in \u001b[0;36mTfqTutorial.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m     95\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m''' \u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcircuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pennylane/qnn/torch.py:402\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    399\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(inputs, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# calculate the forward pass as usual\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_qnode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# reshape to the correct number of batch dims\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_batch_dim:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pennylane/qnn/torch.py:423\u001b[0m, in \u001b[0;36mTorchLayer._evaluate_qnode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluates the QNode for a single input datapoint.\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \n\u001b[1;32m    413\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    tensor: output datapoint\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    419\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_arg: x},\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{arg: weight\u001b[38;5;241m.\u001b[39mto(x) \u001b[38;5;28;01mfor\u001b[39;00m arg, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnode_weights\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[1;32m    422\u001b[0m }\n\u001b[0;32m--> 423\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqnode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mtype(x\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pennylane/workflow/qnode.py:1048\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         full_transform_program\u001b[38;5;241m.\u001b[39m_set_all_argnums(\n\u001b[1;32m   1044\u001b[0m             \u001b[38;5;28mself\u001b[39m, args, kwargs, argnums\n\u001b[1;32m   1045\u001b[0m         )  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# pylint: disable=unexpected-keyword-arg\u001b[39;00m\n\u001b[0;32m-> 1048\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mqml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_program\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_transform_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverride_shots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_shots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m res \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m# convert result to the interface in case the qfunc has no parameters\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pennylane/workflow/execution.py:684\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(tapes, device, gradient_fn, interface, transform_program, config, grad_on_execution, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform, device_vjp)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;66;03m# Exiting early if we do not need to deal with an interface boundary\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_interface_boundary_required:\n\u001b[0;32m--> 684\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43minner_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m post_processing(results)\n\u001b[1;32m    687\u001b[0m _grad_on_execution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pennylane/workflow/execution.py:283\u001b[0m, in \u001b[0;36m_make_inner_execute.<locals>.inner_execute\u001b[0;34m(tapes, **_)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_only:\n\u001b[1;32m    282\u001b[0m     tapes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(qml\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mconvert_to_numpy_parameters(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tapes)\n\u001b[0;32m--> 283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached_device_execution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pennylane/workflow/execution.py:361\u001b[0m, in \u001b[0;36mcache_execute.<locals>.wrapper\u001b[0;34m(tapes, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(cache, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache):\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# No caching. Simply execute the execution function\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# and return the results.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \n\u001b[1;32m    360\u001b[0m     \u001b[38;5;66;03m# must convert to list as new device interface returns tuples\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (res, []) \u001b[38;5;28;01mif\u001b[39;00m return_tuple \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[1;32m    364\u001b[0m execution_tapes \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pennylane/devices/modifiers/simulator_tracking.py:30\u001b[0m, in \u001b[0;36m_track_execute.<locals>.execute\u001b[0;34m(self, circuits, execution_config)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(untracked_execute)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, circuits, execution_config\u001b[38;5;241m=\u001b[39mDefaultExecutionConfig):\n\u001b[0;32m---> 30\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43muntracked_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(circuits, QuantumScript):\n\u001b[1;32m     32\u001b[0m         batch \u001b[38;5;241m=\u001b[39m (circuits,)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pennylane/devices/modifiers/single_tape_support.py:32\u001b[0m, in \u001b[0;36m_make_execute.<locals>.execute\u001b[0;34m(self, circuits, execution_config)\u001b[0m\n\u001b[1;32m     30\u001b[0m     is_single_circuit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     circuits \u001b[38;5;241m=\u001b[39m (circuits,)\n\u001b[0;32m---> 32\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m is_single_circuit \u001b[38;5;28;01melse\u001b[39;00m results\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pennylane/devices/default_qubit.py:553\u001b[0m, in \u001b[0;36mDefaultQubit.execute\u001b[0;34m(self, circuits, execution_config)\u001b[0m\n\u001b[1;32m    547\u001b[0m interface \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    548\u001b[0m     execution_config\u001b[38;5;241m.\u001b[39minterface\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m execution_config\u001b[38;5;241m.\u001b[39mgradient_method \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackprop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    551\u001b[0m )\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    554\u001b[0m         simulate(\n\u001b[1;32m    555\u001b[0m             c,\n\u001b[1;32m    556\u001b[0m             rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng,\n\u001b[1;32m    557\u001b[0m             prng_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prng_key,\n\u001b[1;32m    558\u001b[0m             debugger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debugger,\n\u001b[1;32m    559\u001b[0m             interface\u001b[38;5;241m=\u001b[39minterface,\n\u001b[1;32m    560\u001b[0m             state_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_cache,\n\u001b[1;32m    561\u001b[0m         )\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    565\u001b[0m vanilla_circuits \u001b[38;5;241m=\u001b[39m [convert_to_numpy_parameters(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits]\n\u001b[1;32m    566\u001b[0m seeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng\u001b[38;5;241m.\u001b[39mintegers(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m31\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vanilla_circuits))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pennylane/devices/default_qubit.py:554\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    547\u001b[0m interface \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    548\u001b[0m     execution_config\u001b[38;5;241m.\u001b[39minterface\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m execution_config\u001b[38;5;241m.\u001b[39mgradient_method \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackprop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    551\u001b[0m )\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m--> 554\u001b[0m         \u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m            \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprng_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prng_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdebugger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_debugger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m            \u001b[49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    565\u001b[0m vanilla_circuits \u001b[38;5;241m=\u001b[39m [convert_to_numpy_parameters(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits]\n\u001b[1;32m    566\u001b[0m seeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng\u001b[38;5;241m.\u001b[39mintegers(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m31\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vanilla_circuits))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pennylane/devices/qubit/simulate.py:260\u001b[0m, in \u001b[0;36msimulate\u001b[0;34m(circuit, rng, prng_key, debugger, interface, state_cache)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m circuit\u001b[38;5;241m.\u001b[39mshots \u001b[38;5;129;01mand\u001b[39;00m has_mid_circuit_measurements(circuit):\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m simulate_native_mcm(\n\u001b[1;32m    258\u001b[0m         circuit, rng\u001b[38;5;241m=\u001b[39mrng, prng_key\u001b[38;5;241m=\u001b[39mprng_key, debugger\u001b[38;5;241m=\u001b[39mdebugger, interface\u001b[38;5;241m=\u001b[39minterface\n\u001b[1;32m    259\u001b[0m     )\n\u001b[0;32m--> 260\u001b[0m state, is_state_batched \u001b[38;5;241m=\u001b[39m \u001b[43mget_final_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebugger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebugger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m     state_cache[circuit\u001b[38;5;241m.\u001b[39mhash] \u001b[38;5;241m=\u001b[39m state\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pennylane/devices/qubit/simulate.py:139\u001b[0m, in \u001b[0;36mget_final_state\u001b[0;34m(circuit, debugger, interface, mid_measurements)\u001b[0m\n\u001b[1;32m    137\u001b[0m is_state_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(prep \u001b[38;5;129;01mand\u001b[39;00m prep\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m circuit\u001b[38;5;241m.\u001b[39moperations[\u001b[38;5;28mbool\u001b[39m(prep) :]:\n\u001b[0;32m--> 139\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mapply_operation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_state_batched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdebugger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebugger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmid_measurements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmid_measurements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# Handle postselection on mid-circuit measurements\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op, qml\u001b[38;5;241m.\u001b[39mProjector):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/functools.py:909\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    907\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pennylane/devices/qubit/apply_operation.py:204\u001b[0m, in \u001b[0;36mapply_operation\u001b[0;34m(op, state, is_state_batched, debugger, **_)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;129m@singledispatch\u001b[39m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_operation\u001b[39m(\n\u001b[1;32m    154\u001b[0m     op: qml\u001b[38;5;241m.\u001b[39moperation\u001b[38;5;241m.\u001b[39mOperator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_,\n\u001b[1;32m    159\u001b[0m ):\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply and operator to a given state.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_operation_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebugger\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pennylane/devices/qubit/apply_operation.py:214\u001b[0m, in \u001b[0;36m_apply_operation_default\u001b[0;34m(op, state, is_state_batched, debugger)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The default behaviour of apply_operation, accessed through the standard dispatch\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03mof apply_operation, as well as conditionally in other dispatches.\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28mlen\u001b[39m(op\u001b[38;5;241m.\u001b[39mwires) \u001b[38;5;241m<\u001b[39m EINSUM_OP_WIRECOUNT_PERF_THRESHOLD\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m math\u001b[38;5;241m.\u001b[39mndim(state) \u001b[38;5;241m<\u001b[39m EINSUM_STATE_WIRECOUNT_PERF_THRESHOLD\n\u001b[1;32m    213\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m (op\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mand\u001b[39;00m is_state_batched):\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_operation_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_state_batched\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m apply_operation_tensordot(op, state, is_state_batched\u001b[38;5;241m=\u001b[39mis_state_batched)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pennylane/devices/qubit/apply_operation.py:102\u001b[0m, in \u001b[0;36mapply_operation_einsum\u001b[0;34m(op, state, is_state_batched)\u001b[0m\n\u001b[1;32m     99\u001b[0m         op\u001b[38;5;241m.\u001b[39m_batch_size \u001b[38;5;241m=\u001b[39m batch_size  \u001b[38;5;66;03m# pylint:disable=protected-access\u001b[39;00m\n\u001b[1;32m    100\u001b[0m reshaped_mat \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mreshape(mat, new_mat_shape)\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43meinsum_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreshaped_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pennylane/math/multi_dispatch.py:539\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(indices, like, optimize, *operands)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m like \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    538\u001b[0m     like \u001b[38;5;241m=\u001b[39m get_interface(\u001b[38;5;241m*\u001b[39moperands)\n\u001b[0;32m--> 539\u001b[0m operands \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoerce\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlike\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m like \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# torch einsum doesn't support the optimize keyword argument\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39meinsum(indices, \u001b[38;5;241m*\u001b[39moperands, like\u001b[38;5;241m=\u001b[39mlike)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/autoray/autoray.py:80\u001b[0m, in \u001b[0;36mdo\u001b[0;34m(fn, like, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Do function named ``fn`` on ``(*args, **kwargs)``, peforming single\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03mdispatch to retrieve ``fn`` based on whichever library defines the class of\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mthe ``args[0]``, or the ``like`` keyword argument if specified.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    <tf.Tensor: id=91, shape=(3, 3), dtype=float32>\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m backend \u001b[38;5;241m=\u001b[39m choose_backend(fn, \u001b[38;5;241m*\u001b[39margs, like\u001b[38;5;241m=\u001b[39mlike, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_lib_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pennylane/math/single_dispatch.py:629\u001b[0m, in \u001b[0;36m_coerce_types_torch\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    626\u001b[0m cast_type \u001b[38;5;241m=\u001b[39m complex_type \u001b[38;5;129;01mor\u001b[39;00m float_type \u001b[38;5;129;01mor\u001b[39;00m int_type\n\u001b[1;32m    627\u001b[0m cast_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(cast_type)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pennylane/math/single_dispatch.py:629\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    626\u001b[0m cast_type \u001b[38;5;241m=\u001b[39m complex_type \u001b[38;5;129;01mor\u001b[39;00m float_type \u001b[38;5;129;01mor\u001b[39;00m int_type\n\u001b[1;32m    627\u001b[0m cast_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(cast_type)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_type\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensors]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 1\n",
    "shots = None\n",
    "diff_method = 'best' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"ring\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = torch.nn.init.ones_\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval_global\n",
    "policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "value_circuit_measure = one_measure_expval_global\n",
    "value_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     input_scaling, input_init, weight_init, value_circuit_measure)\n",
    "\n",
    "n_actions = 2\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = True\n",
    "beta = 1\n",
    "increase_rate = 0.0005\n",
    "output_scaling = False\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                         post_processing, \n",
    "                         beta_scheduling, \n",
    "                         beta, increase_rate, \n",
    "                         output_scaling, \n",
    "                         output_init)\n",
    "\n",
    "policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "value_lr_list = [0.01, 0.1]\n",
    "value_params = list(value_circuit.parameters())\n",
    "value_optimizer = create_optimizer_with_lr(value_params, value_lr_list)\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "n_episodes = 5000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 10\n",
    "verbose = 1\n",
    "reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "#reinforce_update = ReinforceBatchAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "#reinforce_update = ReinforceSimpleBaselineAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "#reinforce_update = ReinforceDynamicBaselineAgent(policy, policy_optimizer, value_circuit, value_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../../data\"\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, data_directory, True)\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 1\n",
    "    n_layers = 1\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval_global\n",
    "    policy_circuit = UQC(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    #value_circuit_measure = one_measure_expval_global\n",
    "    #value_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "    #                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "    #                     input_scaling, input_init, weight_init, value_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = False\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "    value_lr_list = [0.01, 0.1]\n",
    "    #value_params = list(value_circuit.parameters())\n",
    "    #value_optimizer = create_optimizer_with_lr(value_params, value_lr_list)\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 5000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../../data\"\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, data_directory, True)\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 1\n",
    "    n_layers = 2\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval_global\n",
    "    policy_circuit = UQC(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    #value_circuit_measure = one_measure_expval_global\n",
    "    #value_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "    #                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "    #                     input_scaling, input_init, weight_init, value_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = False\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "    value_lr_list = [0.01, 0.1]\n",
    "    #value_params = list(value_circuit.parameters())\n",
    "    #value_optimizer = create_optimizer_with_lr(value_params, value_lr_list)\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 5000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../../data\"\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, data_directory, True)\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 1\n",
    "    n_layers = 1\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = torch.nn.init.ones_\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval_global\n",
    "    policy_circuit = TfqTutorial(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                                 entanglement, entanglement_pattern, entanglement_gate, input_scaling,\n",
    "                                 input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    #value_circuit_measure = one_measure_expval_global\n",
    "    #value_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "    #                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "    #                     input_scaling, input_init, weight_init, value_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = False\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "    value_lr_list = [0.01, 0.1]\n",
    "    #value_params = list(value_circuit.parameters())\n",
    "    #value_optimizer = create_optimizer_with_lr(value_params, value_lr_list)\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 5000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../../data\"\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, data_directory, True)\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 1\n",
    "    n_layers = 2\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval_global\n",
    "    policy_circuit = TfqTutorial(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                                 entanglement, entanglement_pattern, entanglement_gate, input_scaling,\n",
    "                                 input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    #value_circuit_measure = one_measure_expval_global\n",
    "    #value_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "    #                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "    #                     input_scaling, input_init, weight_init, value_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = False\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "    value_lr_list = [0.01, 0.1]\n",
    "    #value_params = list(value_circuit.parameters())\n",
    "    #value_optimizer = create_optimizer_with_lr(value_params, value_lr_list)\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 5000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../../data\"\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, data_directory, True)\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 1\n",
    "    n_layers = 4\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval_global\n",
    "    policy_circuit = AlternateEntanglement(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement,, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    #value_circuit_measure = one_measure_expval_global\n",
    "    #value_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "    #                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "    #                     input_scaling, input_init, weight_init, value_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = False\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "    value_lr_list = [0.01, 0.1]\n",
    "    #value_params = list(value_circuit.parameters())\n",
    "    #value_optimizer = create_optimizer_with_lr(value_params, value_lr_list)\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 5000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "import optuna\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(policy, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(policy, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "def objective_function(results):\n",
    "\n",
    "    results_mean = np.mean(results, axis=0)\n",
    "    area = np.abs(np.trapz(results_mean))\n",
    "    maximum_performance_area = float(len(results[0]) * 200)\n",
    "\n",
    "    # Create a metric called performance area and normalize it between 0 and 1\n",
    "    performance_area = area / maximum_performance_area\n",
    "    return performance_area\n",
    "\n",
    "'''\n",
    "def sum_and_average(results):\n",
    "\n",
    "    averages = np.mean(results, axis=1)\n",
    "    return np.mean(averages)\n",
    "'''\n",
    "\n",
    "def objective(trial):    \n",
    "\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling, beta, increase_rate = True, 1, 0.005\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr1 = trial.suggest_float(\"lr1\", 1e-5, 1e-1, log=True)\n",
    "    #lr2 = trial.suggest_float(\"lr2\", 1e-5, 1e-1, log=True)\n",
    "    #lr3 = trial.suggest_float(\"lr3\", 1e-5, 1e-1, log=True)\n",
    "    lr2, lr3 = 0.1, 0.1\n",
    "    lr_list= [lr1, lr2, lr3]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 10\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 1\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 5\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, trial.number) for i in range(num_agents))\n",
    "    performance_metric = objective_function(results)\n",
    "    #performance_metric = sum_and_average(results)\n",
    "    return performance_metric\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    print(f\"Best trial: {study.best_trial.value}\")\n",
    "    print(f\"Best parameters: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_output = F.softmax(torch.Tensor([0.1,0.9]) * 3, dim=0)\n",
    "print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
