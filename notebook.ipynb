{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pennylane as qml \n",
    "import torch\n",
    "\n",
    "def create_circuit(n_qubits,n_layers=None,circ = \"simplified_two_design\",fim=False, shots=None):\n",
    "\n",
    "    dev = qml.device(\"default.qubit.torch\", wires=n_qubits, shots=shots)\n",
    "\n",
    "    def RZRY(params):        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        #qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        #qml.AngleEmbedding(params,wires=range(n_qubits))\n",
    "        for q in range(n_qubits):\n",
    "            qml.Hadamard(wires=q)\n",
    "\n",
    "        for w in range(n_layers): \n",
    "            for q in range(n_qubits):\n",
    "                index = w * (2*n_qubits) + q * 2\n",
    "                qml.RZ(params[index],wires=q)\n",
    "                qml.RY(params[index + 1],wires=q)\n",
    "        \n",
    "        qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        return qml.probs(wires=range(n_qubits))\n",
    "\n",
    "    def S2D(init_params,params,measurement_qubits=0,prod_approx=False):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        \n",
    "        #qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        if not prod_approx:\n",
    "            return qml.probs(wires=list(range(measurement_qubits)))\n",
    "        else:\n",
    "            return [qml.probs(i) for i in range(measurement_qubits)]\n",
    "\n",
    "    def SU(params):\n",
    "        qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        \n",
    "        ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "        for i in range(2,n_qubits):\n",
    "            ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "        return qml.expval(ZZ)\n",
    "    \n",
    "    def simmpleRZRY(params,cnots=True):\n",
    "        qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RZ, wires=range(n_qubits), pattern=\"single\", parameters=params[0])\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params[1])\n",
    "        if cnots:\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "            return qml.expval(qml.PauliZ(n_qubits-1))\n",
    "        else:\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "            return qml.expval(ZZ)\n",
    "        \n",
    "    def RY(params,y=True,probs=False,prod=False, entanglement=None):\n",
    "        #qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params)\n",
    "        #qml.broadcast(qml.CZ, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "\n",
    "        if entanglement==\"all_to_all\":\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        if y==True:\n",
    "            #YY = qml.operation.Tensor(qml.PauliY(0), qml.PauliY(1))\n",
    "            YY = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                #YY = qml.operation.Tensor(YY, qml.PauliY(i))\n",
    "                YY.append(qml.PauliZ(i))\n",
    "            \n",
    "            #return [qml.expval(i) for i in YY]\n",
    "            return qml.expval(YY)\n",
    "\n",
    "        elif probs==False:\n",
    "\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            #ZZ = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))        \n",
    "                #ZZ.append(qml.PauliZ(i))        \n",
    "\n",
    "            #return [qml.expval(i) for i in ZZ]\n",
    "            return qml.expval(ZZ)\n",
    "\n",
    "        else:\n",
    "            if prod:\n",
    "                return [qml.probs(i) for i in range(n_qubits)]\n",
    "            else:\n",
    "                return qml.probs(wires=range(n_qubits))\n",
    "            \n",
    "        \n",
    "        \n",
    "    def GHZ(params,measurement_qubits=0):\n",
    "        qml.RY(params,wires=0)\n",
    "        qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "\n",
    "    def random_product_state(params,gate_sequence=None):\n",
    "                \n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(np.pi / 4, wires=i)\n",
    "\n",
    "        for ll in range(len(params)):\n",
    "\n",
    "            for i in range(n_qubits):\n",
    "                gate_sequence[\"{}{}\".format(ll,i)](params[ll][i], wires=i)\n",
    "\n",
    "            #for i in range(n_qubits - 1):\n",
    "                #qml.CZ(wires=[i, i + 1])\n",
    "    def SEL(params, measurement_qubits=0):\n",
    "        qml.StronglyEntanglingLayers(params, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    def RL(params, measurement_qubits=0):\n",
    "        qml.RandomLayers(params, ratio_imprim=0.8 ,imprimitive=qml.CZ, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    if circ == \"rzry\":\n",
    "        qcircuit = RZRY\n",
    "    elif circ == \"simplified_two_design\":\n",
    "        qcircuit = S2D\n",
    "    elif circ == \"special_unitary\":\n",
    "        qcircuit = SU\n",
    "    elif circ == \"simpleRZRY\":\n",
    "        qcircuit = simmpleRZRY\n",
    "    elif circ == \"RY\":\n",
    "        qcircuit = RY\n",
    "    elif circ == \"ghz\":\n",
    "        qcircuit = GHZ\n",
    "    elif circ == \"random_product_state\":\n",
    "        qcircuit = random_product_state\n",
    "    elif circ == \"SEL\":\n",
    "        qcircuit = SEL\n",
    "    elif circ == \"RL\":\n",
    "        qcircuit = RL\n",
    "    if not fim:\n",
    "        circuit = qml.QNode(qcircuit, dev,interface=\"torch\", diff_method=\"backprop\")\n",
    "    else:\n",
    "        circuit = qml.QNode(qcircuit, dev)\n",
    "\n",
    "    return circuit\n",
    "\n",
    "def compute_gradient(log_prob, w):\n",
    "    \"\"\"Compute gradient of the log probability with respect to weights.\n",
    "    \n",
    "    Args:\n",
    "    - log_prob (torch.Tensor): The log probability tensor.\n",
    "    - w (torch.Tensor): The weights tensor, with requires_grad=True.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The gradient of log_prob with respect to w, flattened.\n",
    "    \"\"\"\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()\n",
    "    log_prob.backward(retain_graph=True)\n",
    "    \n",
    "    if w.grad is None:\n",
    "        raise ValueError(\"The gradient for the given log_prob with respect to w is None.\")\n",
    "    \n",
    "    return w.grad.view(-1).detach().numpy()\n",
    "\n",
    "def policy(probs, policy_type=\"contiguous-like\", n_actions=2, n_qubits=1):\n",
    "\n",
    "    if policy_type == \"contiguous-like\":\n",
    "        return probs\n",
    "    elif policy_type == \"parity-like\":\n",
    "        policy = torch.zeros(n_actions)\n",
    "        for i in range(len(probs)):\n",
    "            a=[]\n",
    "            for m in range(int(np.log2(n_actions))):\n",
    "                if m==0:    \n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)\n",
    "                else:\n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)[:-m]\n",
    "                \n",
    "                a.append(bitstring.count(\"1\") % 2)\n",
    "            policy[int(\"\".join(str(x) for x in a),2)] += probs[i]\n",
    "\n",
    "        return policy    \n",
    "    \n",
    "def compute_policy_and_gradient(args):\n",
    "    n_qubits, shapes, type , n_actions, policy_type, clamp = args\n",
    "\n",
    "    if policy_type == \"parity-like\":\n",
    "        measure_qubits = n_qubits\n",
    "    else:\n",
    "        measure_qubits = int(np.log2(n_actions))\n",
    "\n",
    "    qc = create_circuit(n_qubits, circ=type, fim=False, shots=None)\n",
    "\n",
    "    if type == \"simplified_two_design\":\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_init = torch.tensor(weights[0], requires_grad=False)\n",
    "        weights_tensor_params = torch.tensor(weights[1], requires_grad=True)\n",
    "        \n",
    "        probs = qc(weights_tensor_init,weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    else:\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_params = torch.tensor(weights, requires_grad=True)\n",
    "\n",
    "        probs = qc(weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    pi = policy(probs, policy_type=policy_type, n_actions=n_actions, n_qubits=n_qubits)\n",
    "    if clamp is not None:\n",
    "        pi = torch.clamp(pi, clamp, 1)\n",
    "\n",
    "    dist = torch.distributions.Categorical(probs=pi)\n",
    "    \n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    gradient_no_clamp = np.linalg.norm(compute_gradient(log_prob, weights_tensor_params), 2)\n",
    "    return gradient_no_clamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, env, n_episodes=1000, max_t=1000, gamma=1.0, print_every=5):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    average_scores = []\n",
    "    runtime_sum = 0\n",
    "    for e in range(1, n_episodes):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Sample the action from current policy\n",
    "            if t==0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = policy.sample(state_tensor)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # Total expected reward\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "    #standardized returns\n",
    "        R=0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0,R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + np.finfo(np.float32).eps)\n",
    "\n",
    "        for log_prob, R in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        policy_sum = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "    # Backpropagation\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        policy_sum.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        runtime = end_time-start_time\n",
    "        \n",
    "        runtime_sum += runtime\n",
    "        if e % print_every == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tLast {}\\tEpisodes average reward: {:.2f}\\tRuntime: {:.2f}'.format(e, scores_deque[-1], print_every, np.mean(scores_deque), runtime_sum))\n",
    "            runtime_sum = 0\n",
    "        if np.mean(scores_deque) == 500:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores, policy.gradient_list, average_scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def save_training_data(self):\n",
    "        ''' \n",
    "        Saves training data into json files\n",
    "        '''\n",
    "        current_directory = os.path.dirname(__file__)\n",
    "        folder_name = f\"{str(self.env_name)}_{self.policy.policy.post_processing}_{self.policy.circuit.n_layers}\"\n",
    "        folder_path = os.path.join(current_directory, folder_name)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        episode_data = [self.scores_deque, \n",
    "                        self.runtime, \n",
    "                        self.loss.item(), \n",
    "                        tensor_to_list(self.policy.get_gradients()[0]), \n",
    "                        tensor_to_list(self.policy.get_gradients()[1])]\n",
    "\n",
    "        if folder_path is not None:\n",
    "            file_path = os.path.join(self.folder_path, f\"{self.file_name}.json\")\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, 'r') as f:\n",
    "                    existing_data = json.load(f)\n",
    "                existing_data.append(episode_data)\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump(existing_data, f, indent=4)\n",
    "            else:\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump([episode_data], f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import ray\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "def create_directory(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def tensor_to_list(tensor):\n",
    "    \"\"\"\n",
    "    Convert a tensor or numpy array to a nested list.\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, list):\n",
    "        return [tensor_to_list(t) for t in tensor]\n",
    "    elif isinstance(tensor, dict):\n",
    "        return {key: tensor_to_list(value) for key, value in tensor.items()}\n",
    "    elif isinstance(tensor, np.ndarray):\n",
    "        return tensor.tolist()\n",
    "    elif isinstance(tensor, torch.Tensor):\n",
    "        return tensor.tolist()\n",
    "    else:\n",
    "        return tensor\n",
    "    \n",
    "def create_optimizer_with_lr(params, lr_list, use_amsgrad=False):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': p, 'lr': lr} for p, lr in zip(params, lr_list)\n",
    "    ], amsgrad=use_amsgrad)\n",
    "    return optimizer\n",
    "\n",
    "def get_function_representation(func):\n",
    "    # Check if the object is callable\n",
    "    if callable(func):\n",
    "        # Handle functools.partial objects separately\n",
    "        if isinstance(func, partial):\n",
    "            # Get the original function wrapped by functools.partial\n",
    "            original_func = func.func\n",
    "            # Represent the original function with its module and name\n",
    "            original_func_rep = f\"{original_func.__module__}.{original_func.__name__}\"\n",
    "            # Optionally, include the arguments set by partial\n",
    "            return f\"partial({original_func_rep}, args={func.args}, keywords={func.keywords})\"\n",
    "        # Handle lambda functions\n",
    "        elif func.__name__ == \"<lambda>\":\n",
    "            return f\"{func.__module__}.<lambda>\" + (getattr(func, 'description', ''))\n",
    "        else:\n",
    "            # Handle regular functions\n",
    "            return f\"{func.__module__}.{func.__name__}\"\n",
    "    return \"Unknown Function Type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measures\n",
    "\n",
    "def measure_probs(qubits):\n",
    "    return qml.probs(wires=range(qubits)) \n",
    "\n",
    "def one_measure_expval(qubits):\n",
    "\n",
    "    pauli_string = qml.PauliZ(0)\n",
    "    for i in range(1, qubits):\n",
    "        pauli_string = pauli_string @ qml.PauliZ(i)\n",
    "    \n",
    "    expvals = []\n",
    "    expvals.append(qml.expval(pauli_string))\n",
    "\n",
    "    return expvals\n",
    "\n",
    "def two_measure_expval(qubits):\n",
    "\n",
    "    pauli_string = qml.PauliZ(0)\n",
    "    for i in range(1, qubits):\n",
    "        pauli_string = pauli_string @ qml.PauliZ(i)\n",
    "    \n",
    "    expvals = []\n",
    "    expvals.append(qml.expval(pauli_string))\n",
    "    expvals.append(qml.expval(-pauli_string))\n",
    "\n",
    "    return expvals\n",
    "\n",
    "def three_measure_expval(qubits):\n",
    "\n",
    "    z0 = qml.PauliZ(0)\n",
    "    z1 = qml.PauliZ(1)\n",
    "    z0z1 = z0 @ z1\n",
    "\n",
    "    expvals = []\n",
    "    expvals.append(qml.expval(z0))   \n",
    "    expvals.append(qml.expval(z0z1))  \n",
    "    expvals.append(qml.expval(z1))    \n",
    "\n",
    "    return expvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vqc operations\n",
    "\n",
    "def strong_entangling(n_qubits, layer, entanglement_gate):\n",
    "    for qubit in range(n_qubits):\n",
    "        target = (qubit + layer + 1) % n_qubits\n",
    "        if target != qubit:\n",
    "            qml.CZ(wires=[qubit, target])\n",
    "\n",
    "def entangle_gate_calc(pattern, n_qubits):\n",
    "\n",
    "    if pattern not in ['single', 'double', 'double_odd', 'chain', 'ring', 'pyramid', 'all_to_all']:\n",
    "        raise ValueError(\"Pattern must be one of 'single', 'double', 'double_odd', 'chain', 'ring', 'pyramid', or 'all_to_all'.\")\n",
    "    \n",
    "    if pattern == 'single':\n",
    "        # Applies a single-wire unitary to each qubit\n",
    "        n_gates = n_qubits\n",
    "    \n",
    "    elif pattern == 'double':\n",
    "        # Applies a two-wire unitary to floor(n_qubits / 2) pairs\n",
    "        n_gates = np.floor(n_qubits / 2).astype(int)\n",
    "    \n",
    "    elif pattern == 'double_odd':\n",
    "        # Applies a two-wire unitary to floor((n_qubits - 1) / 2) pairs, starting with the second wire\n",
    "        n_gates = np.floor((n_qubits - 1) / 2).astype(int)\n",
    "    \n",
    "    elif pattern == 'chain':\n",
    "        # Applies a two-wire unitary to all (n_qubits - 1) neighboring pairs\n",
    "        n_gates = n_qubits - 1\n",
    "    \n",
    "    elif pattern == 'ring':\n",
    "        # Applies a two-wire unitary to all n_qubits neighboring pairs, including last to first\n",
    "        n_gates = n_qubits\n",
    "    \n",
    "    elif pattern == 'pyramid':\n",
    "        # Applies gates in a declining pyramid shape to the right, where the number of pairs reduces by 1 each row\n",
    "        # Sum of first (n_qubits-1) integers: (n_qubits - 1) * (n_qubits) / 2\n",
    "        n_gates = (n_qubits - 1) * n_qubits // 2\n",
    "    \n",
    "    elif pattern == 'all_to_all':\n",
    "        # Applies a two-wire unitary to all possible pairs of wires\n",
    "        # Number of combinations of n_qubits taken 2 at a time: n_qubits * (n_qubits - 1) / 2\n",
    "        n_gates = n_qubits * (n_qubits - 1) // 2\n",
    "    return n_gates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JerbiModel(nn.Module):\n",
    "    '''\n",
    "    Creates a parametrized quantum circuit based on the work in https://doi.org/10.48550/arXiv.2103.05577.\n",
    "\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, \n",
    "                n_qubits,\n",
    "                n_layers, \n",
    "                shots = None, \n",
    "                diff_method = 'best', \n",
    "                entanglement = True,\n",
    "                entanglement_pattern = \"all_to_all\", \n",
    "                entanglement_gate = qml.CZ, \n",
    "                input_scaling = True, \n",
    "                input_init = None, \n",
    "                weight_init = None, \n",
    "                measure = two_measure_expval):\n",
    "        super(JerbiModel, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_scaling = input_scaling\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(\"lightning.qubit\", wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        if self.n_layers < 1:\n",
    "            raise ValueError(\"Number of layers can't take values below 1\")\n",
    "        \n",
    "        self.weight_shapes = {\n",
    "            \"input_params\": (self.n_layers, self.n_qubits, 2),\n",
    "            \"params\": (self.n_layers + 1, self.n_qubits, 2)\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"input_params\": self.input_init,\n",
    "            \"params\": self.weight_init,\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, params, input_params):\n",
    "            # In case the number of qubits is greater than the input size AND it is multiple of the input length, \n",
    "            # then tile input tensor 'multiplier' times\n",
    "            if self.n_qubits > len(inputs) and self.n_qubits % len(inputs) == 0:\n",
    "                multiplier = self.n_qubits // len(inputs)\n",
    "                inputs = torch.cat([inputs] * multiplier)\n",
    "\n",
    "            # If the number of qubits is not equal to the input length and not a multiple of the input length, raise an error\n",
    "            elif self.n_qubits != len(inputs) and self.n_qubits % len(inputs) != 0:\n",
    "                raise ValueError('Number of qubits cannot be divided by input lenght')\n",
    "\n",
    "            # Apply Hadamard gates to every qubits\n",
    "            qml.broadcast(qml.Hadamard, wires=range(self.n_qubits), pattern=\"single\")\n",
    "            \n",
    "            # Iterate through layers\n",
    "            for layer in range(self.n_layers):\n",
    "                # Iterate through qubits\n",
    "                for wire in range(self.n_qubits):\n",
    "                    # Parameterized block\n",
    "                    qml.RZ(params[layer][wire][0], wires=wire)\n",
    "                    qml.RY(params[layer][wire][1], wires=wire)\n",
    "\n",
    "                # Entanglement between qubits\n",
    "                if self.entanglement:\n",
    "                    if self.entanglement_pattern == 'strong_entangling':\n",
    "                        strong_entangling(self.n_qubits, layer, self.entanglement_gate)\n",
    "                    else:\n",
    "                        qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "                # If input scaling set to True, multiply inputs by weights\n",
    "                if self.input_scaling:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RY(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                        qml.RZ(input_params[layer][wire][1] * inputs[wire], wires=wire)\n",
    "                # If input scaling set to False, use only the inputs\n",
    "                else:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RY(inputs[wire], wires=wire)\n",
    "                        qml.RZ(inputs[wire], wires=wire)\n",
    "                \n",
    "            # Last parameterized layer\n",
    "            for wire in range(self.n_qubits):\n",
    "                qml.RZ(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)  \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        \n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        circuit = qml.draw_mpl(self.qnode)(inputs, \n",
    "                                        initialized_params[\"params\"], \n",
    "                                        initialized_params[\"input_params\"])\n",
    "        \n",
    "    def circuit_spectrum(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        res = qml.fourier.circuit_spectrum(self.qnode)(inputs, initialized_params[\"params\"], initialized_params[\"input_params\"])\n",
    "        for inp, freqs in res.items():\n",
    "            print(f\"{inp}: {freqs}\")       \n",
    "       \n",
    "    @classmethod\n",
    "    def info(cls):        \n",
    "        '''\n",
    "        Provides a summary of the JerbiModel class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        Creates the Parameterized Quantum Circuit based on the design in https://doi.org/10.48550/arXiv.2103.05577.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_qubits (int): \n",
    "            Number of qubits used in the quantum circuit.\n",
    "        \n",
    "        n_layers (int): \n",
    "            Number of layers in the quantum circuit. Each layer typically consists of parameterized rotations followed by entanglement gates.\n",
    "        \n",
    "        shots (int, optional): \n",
    "            Number of times the circuit gets executed (repeated measurements). If None, the circuit is executed with analytic calculations (no shot noise).\n",
    "        \n",
    "        diff_method (str): \n",
    "            Differentiation method used for training the model. Common options are 'best', 'parameter-shift', 'backprop', etc.\n",
    "\n",
    "        entanglement (bool):\n",
    "            If True, entanglement between qubits is implemented. The entanglement pattern and gate is defined in entanglement_pattern and entanglement_gate, respectively.\n",
    "        \n",
    "        entanglement_pattern (str): \n",
    "            Entanglement pattern used in the circuit, such as 'chain', 'ring', 'all_to_all', etc., as defined by qml.broadcast patterns.\n",
    "        \n",
    "        entanglement_gate (function): \n",
    "            Quantum gate used for entanglement, such as qml.CZ or qml.CNOT. This gate will be applied between qubits according to the specified entanglement pattern.\n",
    "        \n",
    "        input_scaling (bool): \n",
    "            If True, input parameters are scaled by additional learnable parameters (input_params). The input is multiplied by these parameters before being applied to the qubits.\n",
    "                \n",
    "        input_init (function): \n",
    "            Function to initialize the input scaling parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, and function defined by the user.\n",
    "        \n",
    "        weight_init (function): \n",
    "            Function to initialize the weights of the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.normal_, and function defined by the user.\n",
    "                \n",
    "        measure (function): \n",
    "            Measurement function that takes the number of qubits as an argument and returns the measurement result. Common choices are measure_probs, two_measure_expval, or any user-defined measurement function.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        JerbiModel: \n",
    "            An instance of the JerbiModel class representing the quantum neural network.\n",
    "        \"\"\"\n",
    "        print(info_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UQC_FullEnc(nn.Module):\n",
    "    '''\n",
    "    Creates a parameterized quantum circuit (Universal Quantum Classifier) based in https://doi.org/10.22331/q-2020-02-06-226.\n",
    "\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, \n",
    "                n_qubits, \n",
    "                n_layers, \n",
    "                state_dim, \n",
    "                shots = None, \n",
    "                diff_method = 'best', \n",
    "                entanglement = True,\n",
    "                entanglement_pattern = \"all_to_all\", \n",
    "                entanglement_gate = qml.CZ, \n",
    "                input_init = None,\n",
    "                weight_init = None,\n",
    "                bias_init = None,\n",
    "                measure = two_measure_expval):\n",
    "        super(UQC_FullEnc, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.state_dim = state_dim\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.bias_init = bias_init\n",
    "        self.measure = measure\n",
    "        self.input_scaling = None\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(\"lightning.qubit\", wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        self.weight_shapes = {\n",
    "            \"weights\": (self.n_layers, self.n_qubits, self.state_dim),\n",
    "            \"params\": (self.n_layers, self.n_qubits, 1),\n",
    "            \"bias\": (self.n_layers, self.n_qubits)\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"weights\": self.input_init,\n",
    "            \"params\": self.weight_init,\n",
    "            \"bias\": self.bias_init\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, weights, params, bias):\n",
    "\n",
    "            # Iterate through layers\n",
    "            for layer in range(self.n_layers):\n",
    "                # Iterate through qubits\n",
    "                for wire in range(self.n_qubits):\n",
    "                    # Compute the Hadamard product, sum with bias, and use it as the angle for R_X\n",
    "                    hadamard_product = torch.dot(inputs.clone().detach(), weights[layer][wire])\n",
    "                    angle = hadamard_product + bias[layer][wire]\n",
    "\n",
    "                    # Apply the Rx gate with computed angle\n",
    "\n",
    "                    qml.RZ(angle, wires=wire)\n",
    "\n",
    "                    qml.RY(params[layer][wire][0], wires=wire)\n",
    "                # Entanglement between qubits\n",
    "                if self.entanglement:\n",
    "                    if self.entanglement_pattern == 'strong_entangling':\n",
    "                        strong_entangling(self.n_qubits, layer)\n",
    "                    else:\n",
    "                        qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(self.qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        \n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.state_dim)], dtype=torch.float32)\n",
    "        \n",
    "        # Initialize all parameters using the provided initialization methods\n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        circuit = qml.draw_mpl(self.qnode)(inputs, \n",
    "                                        initialized_params[\"weights\"], \n",
    "                                        initialized_params[\"params\"], \n",
    "                                        initialized_params[\"bias\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UQC_PartialEnc(nn.Module):\n",
    "    '''\n",
    "    Creates a parameterized quantum circuit (Universal Quantum Classifier) based in https://doi.org/10.22331/q-2020-02-06-226.\n",
    "\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, \n",
    "                n_qubits, \n",
    "                n_layers, \n",
    "                state_dim, \n",
    "                shots = None, \n",
    "                diff_method = 'best', \n",
    "                entanglement = True,\n",
    "                entanglement_pattern = \"all_to_all\", \n",
    "                entanglement_gate = qml.CZ, \n",
    "                input_init = None,\n",
    "                weight_init = None,\n",
    "                bias_init = None,\n",
    "                measure = two_measure_expval):\n",
    "        super(UQC_PartialEnc, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.state_dim = state_dim\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.bias_init = bias_init\n",
    "        self.measure = measure\n",
    "        self.input_scaling = None\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(\"lightning.qubit\", wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        self.weight_shapes = {\n",
    "            \"weights\": (self.n_layers, self.n_qubits, int(self.state_dim/self.n_qubits)),\n",
    "            \"params\": (self.n_layers, self.n_qubits, 1),\n",
    "            \"bias\": (self.n_layers, self.n_qubits)\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"weights\": self.input_init,\n",
    "            \"params\": self.weight_init,\n",
    "            \"bias\": self.bias_init\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, weights, params, bias):\n",
    "\n",
    "            separate_inputs = np.array_split(inputs,self.n_qubits)\n",
    "            # Iterate through layers\n",
    "            for layer in range(self.n_layers):\n",
    "                # Iterate through qubits\n",
    "                for wire in range(self.n_qubits):\n",
    "                    # Compute the Hadamard product, sum with bias, and use it as the angle for R_X\n",
    "                    hadamard_product = torch.dot(separate_inputs[wire], weights[layer][wire])\n",
    "                    angle = hadamard_product + bias[layer][wire]\n",
    "\n",
    "                    # Apply the Rx gate with computed angle\n",
    "                    qml.RZ(angle, wires=wire)\n",
    "                    qml.RY(params[layer][wire][0], wires=wire)\n",
    "                # Entanglement between qubits\n",
    "                if self.entanglement:\n",
    "                    if self.entanglement_pattern == 'strong_entangling':\n",
    "                        strong_entangling(self.n_qubits, layer)\n",
    "                    else:\n",
    "                        qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(self.qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        \n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.state_dim)], dtype=torch.float32)\n",
    "        \n",
    "        # Initialize all parameters using the provided initialization methods\n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        circuit = qml.draw_mpl(self.qnode)(inputs, \n",
    "                                        initialized_params[\"weights\"], \n",
    "                                        initialized_params[\"params\"], \n",
    "                                        initialized_params[\"bias\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfqTutorial(nn.Module):\n",
    "    '''\n",
    "    Creates a parameterized quantum circuit based on the TensorFlow Quantum tutorial in https://www.tensorflow.org/quantum/tutorials/quantum_reinforcement_learning\n",
    "\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, \n",
    "                n_qubits,\n",
    "                n_layers, \n",
    "                shots = None, \n",
    "                diff_method = 'best', \n",
    "                entanglement = True,\n",
    "                entanglement_pattern = \"all_to_all\", \n",
    "                entanglement_gate = qml.CZ, \n",
    "                input_scaling = True, \n",
    "                input_init = None, \n",
    "                weight_init = None, \n",
    "                measure = two_measure_expval):\n",
    "        super(TfqTutorial, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_scaling = input_scaling\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(\"lightning.qubit\", wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        if self.n_layers < 1:\n",
    "            raise ValueError(\"Number of layers can't take values below 1\")\n",
    "        \n",
    "        self.weight_shapes = {\n",
    "            \"input_params\": (self.n_layers, self.n_qubits, 1),\n",
    "            \"params\": (self.n_layers + 1, self.n_qubits, 3)\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"input_params\": self.input_init,\n",
    "            \"params\": self.weight_init,\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, params, input_params):\n",
    "            # In case the number of qubits is greater than the input size AND it is multiple of the input length, \n",
    "            # then tile input tensor 'multiplier' times\n",
    "            if self.n_qubits > len(inputs) and self.n_qubits % len(inputs) == 0:\n",
    "                multiplier = self.n_qubits // len(inputs)\n",
    "                inputs = torch.cat([inputs] * multiplier)\n",
    "\n",
    "            # If the number of qubits is not equal to the input length and not a multiple of the input length, raise an error\n",
    "            elif self.n_qubits != len(inputs) and self.n_qubits % len(inputs) != 0:\n",
    "                raise ValueError('Number of qubits cannot be divided by input lenght')\n",
    "            \n",
    "            # Iterate through layers\n",
    "            for layer in range(self.n_layers):\n",
    "                # Iterate through qubits\n",
    "                for wire in range(self.n_qubits):\n",
    "                    # Parameterized block\n",
    "                    qml.RX(params[layer][wire][0], wires=wire)\n",
    "                    qml.RY(params[layer][wire][1], wires=wire)\n",
    "                    qml.RZ(params[layer][wire][2], wires=wire)\n",
    "\n",
    "                # Entanglement between qubits\n",
    "                if self.entanglement:\n",
    "                    if self.entanglement_pattern == 'strong_entangling':\n",
    "                        strong_entangling(self.n_qubits, layer)\n",
    "                    else:\n",
    "                        qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "                # If input scaling set to True, multiply inputs by weights\n",
    "                if self.input_scaling:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RX(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                # If input scaling set to False, use only the inputs\n",
    "                else:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RX(inputs[wire], wires=wire)\n",
    "                \n",
    "            # Last parameterized layer\n",
    "            for wire in range(self.n_qubits):\n",
    "                qml.RX(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "                qml.RZ(params[-1][wire][2], wires=wire)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)  \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        \n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        circuit = qml.draw_mpl(self.qnode)(inputs, \n",
    "                                        initialized_params[\"params\"], \n",
    "                                        initialized_params[\"input_params\"])\n",
    "        \n",
    "\n",
    "    def circuit_spectrum(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        res = qml.fourier.circuit_spectrum(self.qnode)(inputs, initialized_params[\"params\"], initialized_params[\"input_params\"])\n",
    "        for inp, freqs in res.items():\n",
    "            print(f\"{inp}: {freqs}\")       \n",
    "\n",
    "        \n",
    "    @classmethod\n",
    "    def info(cls):        \n",
    "        '''\n",
    "        Provides a summary of the JerbiModel class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        Creates the Jerbi policy (Parameterized Quantum Circuit) based on the design in https://doi.org/10.48550/arXiv.2103.05577.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_qubits (int): \n",
    "            Number of qubits used in the quantum circuit.\n",
    "        \n",
    "        n_layers (int): \n",
    "            Number of layers in the quantum circuit. Each layer typically consists of parameterized rotations followed by entanglement gates.\n",
    "        \n",
    "        shots (int, optional): \n",
    "            Number of times the circuit gets executed (repeated measurements). If None, the circuit is executed with analytic calculations (no shot noise).\n",
    "        \n",
    "        diff_method (str): \n",
    "            Differentiation method used for training the model. Common options are 'best', 'parameter-shift', 'backprop', etc.\n",
    "        \n",
    "        entanglement_pattern (str): \n",
    "            Entanglement pattern used in the circuit, such as 'chain', 'ring', 'all_to_all', etc., as defined by qml.broadcast patterns.\n",
    "        \n",
    "        entanglement_gate (function): \n",
    "            Quantum gate used for entanglement, such as qml.CZ or qml.CNOT. This gate will be applied between qubits according to the specified entanglement pattern.\n",
    "        \n",
    "        input_scaling (bool): \n",
    "            If True, input parameters are scaled by additional learnable parameters (input_params). The input is multiplied by these parameters before being applied to the qubits.\n",
    "                \n",
    "        input_init (function): \n",
    "            Function to initialize the input scaling parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, and function defined by the user.\n",
    "        \n",
    "        weight_init (function): \n",
    "            Function to initialize the weights of the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.normal_, and function defined by the user..\n",
    "                \n",
    "        measure (function): \n",
    "            Measurement function that takes the number of qubits as an argument and returns the measurement result. Common choices are measure_probs, measure_expval_pairs, or any user-defined measurement function.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        JerbiModel: \n",
    "            An instance of the JerbiModel class representing the quantum neural network.\n",
    "        \"\"\"\n",
    "        print(info_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyPostProcessing(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_qubits, n_actions, policy_type = 'raw_contiguous', \n",
    "                 beta_scheduling = False, beta = 1, increase_rate = 0.0005, \n",
    "                 output_scaling = False, output_init = torch.nn.init.ones_):\n",
    "        super(PolicyPostProcessing, self).__init__()\n",
    "\n",
    "        '''\n",
    "        Determines the type of policy used based on the arguments:\n",
    "\n",
    "            n_actions(int) = Number of actions\n",
    "            post_processing(str) = Type of policy ('raw_contiguous', 'raw_parity', 'softmax')\n",
    "            beta_scheduling(bool) = Inverse temperature parameter used in softmax (used if set to True)\n",
    "            beta(float) = Beta parameter or inverse temperature (used only for softmax)\n",
    "            increase_rate(float) = Amount added to beta at the end of each episode (used only for softmax)\n",
    "            output_scaling(bool) = Output parameters are used if True\n",
    "            output_init(function) = How the output parameters are initialized\n",
    "            \n",
    "        '''\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_actions = n_actions\n",
    "        self.policy_type = policy_type\n",
    "        self.beta_scheduling = beta_scheduling\n",
    "        self.beta = beta\n",
    "        self.increase_rate = increase_rate\n",
    "        self.output_scaling = output_scaling\n",
    "        self.output_init = output_init\n",
    "\n",
    "        if self.output_scaling == True:\n",
    "            self.output_params = nn.parameter.Parameter(torch.Tensor(self.n_actions), requires_grad=True)\n",
    "            self.output_init(self.output_params)\n",
    "        else:\n",
    "            self.register_parameter('w_input', None)\n",
    "\n",
    "    def forward(self,probs):\n",
    "        if self.policy_type == 'raw_contiguous':\n",
    "            policy = self.raw_contiguous(probs)\n",
    "        elif self.policy_type == 'raw_parity':\n",
    "            policy = self.raw_parity(probs)\n",
    "        elif self.policy_type == 'softmax':\n",
    "            policy = self.softmax(probs)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid post-processing method specified.\")\n",
    "        return policy\n",
    "\n",
    "    def raw_contiguous(self,probs):\n",
    "        \n",
    "        log_n_actions = np.log2(self.n_actions)\n",
    "        \n",
    "    # Ensure the number of actions does not exceed the number of basis states (determined by n_qubits)\n",
    "        if log_n_actions > self.n_qubits:\n",
    "            raise ValueError('Number of actions exceeds the number of basis states!')\n",
    "\n",
    "    # Flatten the probability distribution to handle it as a single-dimensional array\n",
    "        probs_flatten = probs.flatten()\n",
    "\n",
    "    # Calculate the size of each chunk that corresponds to an action\n",
    "        chunk_size = len(probs_flatten) // self.n_actions\n",
    "\n",
    "    # Calculate the remainder to handle cases where the division isn't exact\n",
    "        remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "    # Initialize an empty list to store the policy probabilities\n",
    "        policy = []\n",
    "\n",
    "    # Loop through each action to calculate the corresponding probability\n",
    "        for i in range(self.n_actions):\n",
    "        # Determine the start and end indices for the current chunk\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size\n",
    "\n",
    "        # Adjust the end index to accommodate the remainder\n",
    "            if i < remainder:\n",
    "                end += 1\n",
    "\n",
    "        # Sum the probabilities in the current chunk and append to the policy list\n",
    "            policy.append(sum(probs_flatten[start:end]))\n",
    "\n",
    "    # Convert the list to a tensor for compatibility with PyTorch\n",
    "        policy_tensor = torch.tensor(policy)\n",
    "        return policy_tensor\n",
    "        \n",
    "    def raw_parity(self,probs):\n",
    "        print(probs)\n",
    "\n",
    "        log_n_actions = np.log2(self.n_actions)\n",
    "        m = int(log_n_actions - 1)\n",
    "\n",
    "        if log_n_actions < 1.0 or not (np.floor(log_n_actions) == np.ceil(log_n_actions)):\n",
    "            raise NotImplementedError('Number of actions needs to be a power of two!')\n",
    "\n",
    "        # Ensure the number of actions does not exceed the number of basis states (determined by n_qubits)\n",
    "        if log_n_actions > self.n_qubits:\n",
    "            raise ValueError('Number of actions exceeds number of basis states!')\n",
    "        \n",
    "        # Flatten the probability distribution to handle bitstrings uniformly\n",
    "        probs_flatten = probs.flatten()\n",
    "        policy = torch.zeros(self.n_actions)\n",
    "\n",
    "        # Process each bitstring probability\n",
    "        for idx, prob in enumerate(probs_flatten):\n",
    "            # Convert the index to binary representation of bitstring\n",
    "            b_bin = '{:b}'.format(idx).zfill(self.n_qubits)\n",
    "            print(b_bin)\n",
    "            # Extract the initial bits and compute the parity bit\n",
    "            initial_bits = b_bin[:m]\n",
    "            parity_bits = b_bin[m:]\n",
    "            parity_bit = str(parity_bits.count('1') % 2)\n",
    "\n",
    "            # Form the action label from initial bits and parity bit\n",
    "            action_label = initial_bits + parity_bit\n",
    "\n",
    "            # Convert action label to integer to determine action index\n",
    "            action_index = int(action_label, 2) % self.n_actions\n",
    "\n",
    "            # Assign probability to the corresponding action in the policy vector\n",
    "            policy[action_index] += prob\n",
    "\n",
    "        print(policy)\n",
    "        return policy\n",
    "    \n",
    "    def softmax(self, probs):\n",
    "        \n",
    "        if self.output_scaling == True:\n",
    "            probs *= self.output_params\n",
    "\n",
    "        scaled_output = probs * self.beta\n",
    "        softmax_output = F.softmax(scaled_output, dim=0)\n",
    "        return softmax_output\n",
    "    \n",
    "    def beta_schedule(self):\n",
    "        if self.beta_scheduling == True:\n",
    "            if self.policy_type == 'softmax' or self.policy_type == 'softmax_probs':\n",
    "                self.beta += self.increase_rate\n",
    "\n",
    "    @classmethod\n",
    "    def info(cls):\n",
    "        '''\n",
    "        Provides a summary of the PolicyType class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        PolicyType Class:\n",
    "\n",
    "        Determines the type of policy used based on the arguments:\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_actions (int): \n",
    "            Number of actions available for the agent to choose from.\n",
    "        \n",
    "        policy_type (str): \n",
    "            Type of policy applied to the probability distribution:\n",
    "            - 'raw_contiguous': Sums up contiguous chunks of probabilities.\n",
    "            - 'raw_parity': Sums up probabilities in a circular manner based on parity.\n",
    "            - 'softmax': Applies a softmax function to the scaled probabilities.\n",
    "            - 'softmax_probs': Sums up contiguous chunks of probabilities and then applies softmax.\n",
    "        \n",
    "        beta_scheduling (bool): \n",
    "            If True, updates the inverse temperature parameter (beta) after each episode. Used only for 'softmax' and 'softmax_probs'.\n",
    "        \n",
    "        beta (float): \n",
    "            Inverse temperature parameter used for scaling probabilities in the softmax policy.\n",
    "        \n",
    "        increase_rate (float): \n",
    "            Amount added to beta at the end of each episode, if beta_scheduling is True.\n",
    "        \n",
    "        output_scaling (bool): \n",
    "            If True, scales the output probabilities by learnable parameters.\n",
    "        \n",
    "        output_init (function): \n",
    "            Initialization function for output parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, etc.\n",
    "        \n",
    "        Methods:\n",
    "        -------\n",
    "        sample(probs):\n",
    "            Selects an action based on the chosen post_processing method.\n",
    "        \n",
    "        raw_contiguous(probs):\n",
    "            Sums up contiguous chunks of probabilities and returns the resulting tensor.\n",
    "        \n",
    "        raw_parity(probs):\n",
    "            Sums up probabilities in a circular manner based on parity and returns the action, log probability, and policy tensor.\n",
    "        \n",
    "        softmax(probs):\n",
    "            Applies a softmax function to the scaled probabilities and returns the action, log probability, and policy tensor.\n",
    "        \n",
    "        softmax_probs(probs):\n",
    "            Sums up contiguous chunks of probabilities, applies softmax, and returns the action, log probability, and policy tensor.\n",
    "        \n",
    "        beta_schedule():\n",
    "            Updates the beta parameter if beta_scheduling is True. Only applicable for 'softmax' and 'softmax_probs' methods.\n",
    "        \"\"\"\n",
    "        print(info_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumPolicy(nn.Module):\n",
    "    \n",
    "    def __init__(self, circuit, post_processing):\n",
    "        super(QuantumPolicy, self).__init__()\n",
    "        self.circuit = circuit\n",
    "        self.post_processing = post_processing\n",
    "\n",
    "    def sample(self, inputs):\n",
    "        '''\n",
    "        Samples an action from the action probability distribution\n",
    "        '''\n",
    "        policy = self.forward(inputs)\n",
    "        dist = torch.distributions.Categorical(policy)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), policy\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Input state is fed to the circuit - its output is then fed to the post processing \n",
    "        '''\n",
    "        probs = self.circuit.forward(inputs)\n",
    "        probs_processed = self.post_processing.forward(probs)\n",
    "        return probs_processed\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        '''\n",
    "        Returns the values of each set of parameters\n",
    "        '''\n",
    "        parameters = []\n",
    "\n",
    "        circuit_parameters = [param.clone().detach().numpy().flatten() for param in self.circuit.parameters()]\n",
    "        parameters.extend(circuit_parameters)\n",
    "\n",
    "        policy_parameters = [param.clone().detach().numpy().flatten() for param in self.post_processing.parameters()]\n",
    "        parameters.extend(policy_parameters)\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    def get_gradients(self):\n",
    "        '''\n",
    "        Returns the gradient values of each set of parameters\n",
    "        '''\n",
    "        gradients = []\n",
    "\n",
    "        # Get gradients from circuit parameters\n",
    "        circuit_gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for param in self.circuit.parameters()]\n",
    "        gradients.extend(circuit_gradients)\n",
    "\n",
    "        # Get gradients from policy parameters\n",
    "        policy_gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for param in self.post_processing.parameters()]\n",
    "        gradients.extend(policy_gradients)\n",
    "\n",
    "        return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ajustar a verbosity level: nivel 1 rewards, nivel 2 rewards runtimes, nivel 3 rewards runtimes loss..\n",
    "\n",
    "colocar o print dos ultimos 100 episodios separadamente dos episodicos:\n",
    "Episode 100: reward\n",
    "Last 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceAgent:\n",
    "\n",
    "    def __init__(self, \n",
    "                policy, \n",
    "                policy_optimizer, \n",
    "                env_name, \n",
    "                n_episodes, \n",
    "                max_t, \n",
    "                gamma = 0.99, \n",
    "                baseline = True, \n",
    "                batch_size = 10, \n",
    "                normalize = False,\n",
    "                print_every = 100, \n",
    "                verbose = 1):\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy = policy.to(self.device)\n",
    "        self.policy_optimizer = policy_optimizer\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env_name = env_name\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.baseline = baseline\n",
    "        self.batch_size = batch_size\n",
    "        self.normalize = normalize\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.solved = False\n",
    "        self.scores = deque(maxlen=100)\n",
    "        self.batch_log_probs = []\n",
    "        self.batch_rewards = []\n",
    "        self.loss = torch.tensor(0.0)\n",
    "\n",
    "    def train(self, run_count=None, rundate = None, path=None, tensorboard=False):\n",
    "\n",
    "        if run_count is not None and path is not None:\n",
    "            data_path = create_directory(os.path.join(path, 'data'))\n",
    "            env_folder = create_directory(os.path.join(data_path, self.env_name))\n",
    "            experiment_folder_name = f\"{self.policy.circuit.__class__.__name__}_{self.policy.circuit.n_qubits}qubits_{self.policy.circuit.n_layers}layer_{rundate}\"\n",
    "            experiment_path = create_directory(os.path.join(env_folder, experiment_folder_name))\n",
    "            run_path = create_directory(os.path.join(experiment_path, f'run_{str(run_count)}'))\n",
    "            self.save_agent_data(experiment_path)\n",
    "        \n",
    "        if tensorboard:\n",
    "            writer = SummaryWriter(log_dir=run_path)\n",
    "\n",
    "        for i in range(1, self.n_episodes + 1):\n",
    "\n",
    "            start_time = time.time()\n",
    "            self.get_trajectory()\n",
    "\n",
    "            if np.mean(self.scores) > self.env.spec.reward_threshold:\n",
    "                self.solved = True\n",
    "\n",
    "            if i % self.batch_size == 0 and self.solved is False:\n",
    "                    self.update_policy()\n",
    "                    self.policy.post_processing.beta_schedule()\n",
    " \n",
    "            end_time = time.time()\n",
    "            self.runtime = end_time - start_time\n",
    "\n",
    "            if tensorboard:\n",
    "                self.writer_function(writer, i)\n",
    "            if run_count is not None and path is not None:\n",
    "                self.save_data(run_path, i)\n",
    "\n",
    "            if (i+1) % self.print_every == 0 and self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}s\\t Last {} Episodes average reward: {:.2f}\\t '.format(i+1, self.scores[-1], self.runtime, str(100), np.mean(self.scores)))\n",
    "            elif self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}s\\t '.format(i+1, self.scores[-1], self.runtime))\n",
    "\n",
    "        if tensorboard:\n",
    "            writer.close()\n",
    "\n",
    "    def get_trajectory(self):\n",
    "        '''\n",
    "        Gets a trajectory based on the running policy until it runs out of bounds or achieves maximum reward of an episode\n",
    "        '''\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(self.max_t):\n",
    "            if t == 0:\n",
    "                state_tensor = torch.tensor(self.normalize_state(state[0])).float().to(self.device)\n",
    "            else:\n",
    "                state_tensor = torch.tensor(self.normalize_state(state)).float().to(self.device)\n",
    "            action, log_prob, _ = self.policy.sample(state_tensor)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            \n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        self.scores.append(sum(self.rewards))\n",
    "        self.batch_log_probs.append(self.saved_log_probs)\n",
    "        self.batch_rewards.append(self.rewards)\n",
    "\n",
    "        del self.saved_log_probs, self.rewards\n",
    "\n",
    "        if self.solved is True:\n",
    "            self.batch_log_probs = []\n",
    "            self.batch_rewards = []\n",
    "      \n",
    "    def update_policy(self):\n",
    "        '''\n",
    "        Computes the loss and gradients and updates the policy via gradient methods\n",
    "        '''\n",
    "        all_returns = []\n",
    "        # Compute returns for each batch\n",
    "        for batch in self.batch_rewards:\n",
    "            R = 0\n",
    "            ep_return = []\n",
    "            for r in reversed(batch):\n",
    "                R = r + self.gamma * R\n",
    "                ep_return.insert(0, R)\n",
    "            \n",
    "            ep_return = torch.tensor(ep_return).to(self.device)\n",
    "            ep_return = (ep_return - ep_return.mean()) / (ep_return.std() + 1e-8)  # Standardize returns\n",
    "            all_returns.append(ep_return)\n",
    "        policy_loss = []\n",
    "        \n",
    "        if self.baseline:\n",
    "            baseline = np.mean([sum(lst) for lst in all_returns])\n",
    "            for log_probs, ep_returns in zip(self.batch_log_probs, all_returns):\n",
    "                for log_prob, ret in zip(log_probs, ep_returns):\n",
    "                    advantage = ret - baseline \n",
    "                    policy_loss.append(-log_prob * advantage)\n",
    "        else:\n",
    "            for log_probs, ep_returns in zip(self.batch_log_probs, all_returns):\n",
    "                for log_prob, ret in zip(log_probs, ep_returns):\n",
    "                    policy_loss.append(-log_prob * ret)\n",
    "\n",
    "        if policy_loss:\n",
    "            policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "            self.loss = torch.cat(policy_unsqueezed).mean()\n",
    "\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "        del all_returns\n",
    "        del policy_loss\n",
    "        del policy_unsqueezed \n",
    "\n",
    "        self.batch_log_probs = []\n",
    "        self.batch_rewards = []\n",
    "\n",
    "    def normalize_state(self, state):\n",
    "\n",
    "        if self.env_name in ('Acrobot-v0', 'Acrobot-v1'):\n",
    "            theta1 = np.arccos(state[0])\n",
    "            theta2 = np.arccos(state[2])\n",
    "            state = [theta1,theta2,state[4],state[5]]\n",
    "\n",
    "        if self.normalize == True:\n",
    "        # Compute the maximum absolute value of all features in the current state\n",
    "\n",
    "            max_abs_value = max(abs(value) for value in state)\n",
    "\n",
    "        # Normalize each feature by the maximum absolute value\n",
    "            state = np.array([value / max_abs_value for value in state])\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def save_agent_data(self,main_path):\n",
    "        '''\n",
    "        Stores the model parameters into a json file\n",
    "\n",
    "        '''\n",
    "        agent_variables = {\n",
    "            \"Model\": self.policy.circuit.__class__.__name__,\n",
    "            \"Number of Qubits\": self.policy.circuit.n_qubits,\n",
    "            \"Number of Layers\": self.policy.circuit.n_layers,\n",
    "            \"Shots\": self.policy.circuit.shots,\n",
    "            \"Differentiation Method\": self.policy.circuit.diff_method,\n",
    "            \"Entanglement\": self.policy.circuit.entanglement,\n",
    "            \"Entanglement Pattern\": self.policy.circuit.entanglement_pattern,\n",
    "            \"Entanglement Gate\": get_function_representation(self.policy.circuit.entanglement_gate),\n",
    "            \"Input Scaling\": self.policy.circuit.input_scaling,\n",
    "            \"Input Initialization\": get_function_representation(self.policy.circuit.input_init),\n",
    "            \"Weight Initialization\": get_function_representation(self.policy.circuit.weight_init),\n",
    "            \"Measure\": get_function_representation(self.policy.circuit.measure),\n",
    "            \"Policy Post Processing\": self.policy.post_processing.policy_type,\n",
    "            \"Optimizers\": str(self.policy_optimizer),\n",
    "            \"Envinronment\": str(self.env_name),\n",
    "            \"Gamma (discounting factor)\": self.gamma,\n",
    "            \"Batch Size\": str(self.batch_size),\n",
    "            \"Baseline\": str(self.baseline)\n",
    "        }\n",
    "        if self.policy.post_processing.policy_type in ['softmax', 'softmax_probs']:\n",
    "            agent_variables.update({\n",
    "                \"Softmax scheduling (in case policy is softmax)\": f\"{self.policy.post_processing.beta_scheduling}. Starting beta: {self.policy.post_processing.beta}. Increase rate: {self.policy.post_processing.increase_rate}\",\n",
    "                \"Softmax output scaling (in case policy is softmax)\": f\"{self.policy.post_processing.output_scaling}. Output Initialization: {get_function_representation(self.policy.post_processing.output_init)}\",\n",
    "            })\n",
    "        with open(os.path.join(main_path, \"agent_characteristics.json\"), \"w\") as f:\n",
    "            json.dump(agent_variables, f, indent=4)\n",
    "    \n",
    "    def save_data(self, run_path, iteration):\n",
    "        '''\n",
    "        Saves the data into a .npz file for each episode, where gradients are stored as a list of lists.\n",
    "        '''\n",
    "        data_file = os.path.join(run_path, \"run_data.npz\")\n",
    "        \n",
    "        # Load existing data if the file exists\n",
    "        if os.path.exists(data_file):\n",
    "            data = np.load(data_file, allow_pickle=True)\n",
    "            old_episode_reward = data['episode_reward'].tolist()\n",
    "            old_loss = data['loss'].tolist()\n",
    "            old_runtime = data['runtime'].tolist()\n",
    "            old_params_gradients = data['params_gradients'].tolist()\n",
    "        else:\n",
    "            old_episode_reward = []\n",
    "            old_loss = []\n",
    "            old_runtime = []\n",
    "            old_params_gradients = []\n",
    "\n",
    "        # Add new episode data\n",
    "        old_episode_reward.append(self.scores[-1])\n",
    "        old_runtime.append(self.runtime)\n",
    "\n",
    "        current_episode_gradients = []\n",
    "        if iteration % self.batch_size == 0 and iteration != 0 and self.solved is False:\n",
    "            old_loss.append(self.loss.item())\n",
    "        for name, param in self.policy.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_array = param.grad.cpu().numpy().flatten()\n",
    "                current_episode_gradients.append(grad_array)\n",
    "            else:\n",
    "                current_episode_gradients.append(None)\n",
    "\n",
    "        old_params_gradients.append(current_episode_gradients)\n",
    "            \n",
    "        # Save data to .npz file\n",
    "        np.savez_compressed(data_file,\n",
    "                            episode_reward=np.array(old_episode_reward),\n",
    "                            loss=np.array(old_loss),\n",
    "                            runtime=np.array(old_runtime),\n",
    "                            params_gradients=np.array(old_params_gradients, dtype=object))  # Use dtype=object to handle lists\n",
    "\n",
    "        # Clear old data lists to free up memory\n",
    "        del old_episode_reward[:]\n",
    "        del old_loss[:]\n",
    "        del old_runtime[:]\n",
    "        del old_params_gradients[:]\n",
    "\n",
    "    def writer_function(self, writer, iteration):\n",
    "        '''\n",
    "        Stores data into a tensorboard session\n",
    "        '''\n",
    "    #   Episode reward\n",
    "        writer.add_scalar(\"Episode Reward\", self.scores[-1], global_step=iteration)\n",
    "    #   Episode runtime\n",
    "        writer.add_scalar(\"Runtime\", self.runtime, global_step=iteration)\n",
    "    #   Loss\n",
    "        writer.add_scalar(\"Loss\", self.loss.item(), global_step=iteration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jerbi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 6\n",
    "n_layers = 4\n",
    "shots = None\n",
    "diff_method = 'best' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = torch.nn.init.ones_\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).normal_(-np.pi, np.pi)\n",
    "policy_circuit_measure = three_measure_expval\n",
    "policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "policy_circuit.visualize_circuit()\n",
    "#policy_circuit.circuit_spectrum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 5\n",
    "shots = None\n",
    "diff_method = 'adjoint' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval\n",
    "policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "\n",
    "n_actions = 2\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = False\n",
    "beta = 1\n",
    "increase_rate = 0.0005\n",
    "output_scaling = True\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                         post_processing, \n",
    "                         beta_scheduling, \n",
    "                         beta, increase_rate, \n",
    "                         output_scaling, \n",
    "                         output_init)\n",
    "\n",
    "policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "n_episodes = 1000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 100\n",
    "verbose = 1\n",
    "baseline = True\n",
    "batch_size = 10\n",
    "reinforce_update = ReinforceAgent(policy, \n",
    "                                  policy_optimizer, \n",
    "                                  env_name, \n",
    "                                  n_episodes, \n",
    "                                  max_t, \n",
    "                                  gamma, \n",
    "                                  baseline, \n",
    "                                  batch_size, \n",
    "                                  print_every, \n",
    "                                  verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 5\n",
    "n_layers = 5\n",
    "state_dim = 4\n",
    "shots = None\n",
    "diff_method = 'best' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "entanglement_training = False\n",
    "entanglement_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "input_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "bias_init = torch.nn.init.zeros_\n",
    "policy_circuit_measure = two_measure_expval\n",
    "policy_circuit = UQC_FullEnc(n_qubits,\n",
    "                            n_layers, \n",
    "                            state_dim, \n",
    "                            shots, \n",
    "                            diff_method, \n",
    "                            entanglement, \n",
    "                            entanglement_pattern, \n",
    "                            entanglement_gate, \n",
    "                            entanglement_training,\n",
    "                            entanglement_init,\n",
    "                            weight_init,\n",
    "                            weight_init,\n",
    "                            bias_init, \n",
    "                            policy_circuit_measure)\n",
    "\n",
    "policy_circuit.visualize_circuit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 5\n",
    "state_dim = 4\n",
    "shots = None\n",
    "diff_method = 'adjoint' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "bias_init = torch.nn.init.zeros_\n",
    "policy_circuit_measure = three_measure_expval\n",
    "policy_circuit = UQC_FullEnc(n_qubits,\n",
    "                            n_layers, \n",
    "                            state_dim, \n",
    "                            shots, \n",
    "                            diff_method, \n",
    "                            entanglement, \n",
    "                            entanglement_pattern, \n",
    "                            entanglement_gate, \n",
    "                            input_init,\n",
    "                            weight_init,\n",
    "                            bias_init, \n",
    "                            policy_circuit_measure)\n",
    "\n",
    "n_actions = 3\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = False\n",
    "beta = 1\n",
    "increase_rate = 0.0005\n",
    "output_scaling = True\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_post_process = PolicyPostProcessing( n_qubits,\n",
    "                                            n_actions, \n",
    "                                            post_processing, \n",
    "                                            beta_scheduling, \n",
    "                                            beta, increase_rate, \n",
    "                                            output_scaling, \n",
    "                                            output_init)\n",
    "\n",
    "policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [input_weights, weights, bias, enttanglement_weights, output_weights]\n",
    "policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "env_name = 'Acrobot-v1'\n",
    "n_episodes = 1000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 100\n",
    "verbose = 1\n",
    "baseline = True\n",
    "batch_size = 10\n",
    "normalize = True\n",
    "reinforce_update = ReinforceAgent(policy, \n",
    "                                  policy_optimizer, \n",
    "                                  env_name, \n",
    "                                  n_episodes, \n",
    "                                  max_t, \n",
    "                                  gamma, \n",
    "                                  baseline, \n",
    "                                  batch_size, \n",
    "                                  normalize,\n",
    "                                  print_every, \n",
    "                                  verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TQF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 5\n",
    "shots = None\n",
    "diff_method = 'adjoint' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"ring\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = torch.nn.init.ones_\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval\n",
    "policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "\n",
    "n_actions = 2\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = False\n",
    "beta = 1\n",
    "increase_rate = 0.0005\n",
    "output_scaling = True\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                         post_processing, \n",
    "                         beta_scheduling, \n",
    "                         beta, increase_rate, \n",
    "                         output_scaling, \n",
    "                         output_init)\n",
    "\n",
    "policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "n_episodes = 1000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 100\n",
    "verbose = 1\n",
    "baseline = True\n",
    "batch_size = 10\n",
    "reinforce_update = ReinforceAgent(policy, \n",
    "                                  policy_optimizer, \n",
    "                                  env_name, \n",
    "                                  n_episodes, \n",
    "                                  max_t, \n",
    "                                  gamma, \n",
    "                                  baseline, \n",
    "                                  batch_size, \n",
    "                                  print_every, \n",
    "                                  verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 1\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    bias_init = torch.nn.init.zeros_\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = UQC_FullEnc(n_qubits,\n",
    "                                n_layers, \n",
    "                                state_dim, \n",
    "                                shots, \n",
    "                                diff_method, \n",
    "                                entanglement, \n",
    "                                entanglement_pattern, \n",
    "                                entanglement_gate, \n",
    "                                input_init,\n",
    "                                weight_init,\n",
    "                                bias_init,\n",
    "                                policy_circuit_measure)\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = True\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing( n_qubits,\n",
    "                                                n_actions, \n",
    "                                                post_processing, \n",
    "                                                beta_scheduling, \n",
    "                                                beta, increase_rate, \n",
    "                                                output_scaling, \n",
    "                                                output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer = create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 8   \n",
    "\n",
    "    for run_index in range(1):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jerbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data reupp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 18:44:05,135\tINFO worker.py:1783 -- Started a local Ray instance.\n"
     ]
    },
    {
     "ename": "RayTaskError(IndexError)",
     "evalue": "\u001b[36mray::train_agents()\u001b[39m (pid=2396, ip=127.0.0.1)\n  File \"python\\ray\\_raylet.pyx\", line 1858, in ray._raylet.execute_task\n  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\2771910933.py\", line 57, in train_agents\n  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 52, in train\n  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 87, in get_trajectory\n  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 158, in normalize_state\nIndexError: index 4 is out of bounds for axis 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRayTaskError(IndexError)\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 73\u001b[0m\n\u001b[0;32m     68\u001b[0m start_agent_index \u001b[38;5;241m=\u001b[39m run_index \u001b[38;5;241m*\u001b[39m num_agents\n\u001b[0;32m     70\u001b[0m results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     71\u001b[0m     train_agents\u001b[38;5;241m.\u001b[39mremote(\u001b[38;5;28mstr\u001b[39m(start_agent_index \u001b[38;5;241m+\u001b[39m i), rundate) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_agents)]\n\u001b[1;32m---> 73\u001b[0m completed_results \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mget(results)\n\u001b[0;32m     74\u001b[0m all_results\u001b[38;5;241m.\u001b[39mextend(completed_results)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults for run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompleted_results\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\ray\\_private\\auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     20\u001b[0m     auto_init_ray()\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\ray\\_private\\client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\ray\\_private\\worker.py:2661\u001b[0m, in \u001b[0;36mget\u001b[1;34m(object_refs, timeout)\u001b[0m\n\u001b[0;32m   2655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2656\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid type of object refs, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(object_refs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, is given. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2657\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_refs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must either be an ObjectRef or a list of ObjectRefs. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2658\u001b[0m     )\n\u001b[0;32m   2660\u001b[0m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[1;32m-> 2661\u001b[0m values, debugger_breakpoint \u001b[38;5;241m=\u001b[39m worker\u001b[38;5;241m.\u001b[39mget_objects(object_refs, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m   2662\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[0;32m   2663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\ray\\_private\\worker.py:871\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[1;34m(self, object_refs, timeout, return_exceptions)\u001b[0m\n\u001b[0;32m    869\u001b[0m     global_worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mdump_object_store_memory_usage()\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[1;32m--> 871\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[1;31mRayTaskError(IndexError)\u001b[0m: \u001b[36mray::train_agents()\u001b[39m (pid=2396, ip=127.0.0.1)\n  File \"python\\ray\\_raylet.pyx\", line 1858, in ray._raylet.execute_task\n  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\2771910933.py\", line 57, in train_agents\n  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 52, in train\n  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 87, in get_trajectory\n  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 158, in normalize_state\nIndexError: index 4 is out of bounds for axis 0 with size 4"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 18:44:17,499\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::train_agents()\u001b[39m (pid=22460, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1858, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\2771910933.py\", line 57, in train_agents\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 52, in train\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 87, in get_trajectory\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 158, in normalize_state\n",
      "IndexError: index 4 is out of bounds for axis 0 with size 4\n",
      "2024-09-13 18:44:17,502\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::train_agents()\u001b[39m (pid=9732, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1858, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\2771910933.py\", line 57, in train_agents\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 52, in train\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 87, in get_trajectory\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 158, in normalize_state\n",
      "IndexError: index 4 is out of bounds for axis 0 with size 4\n",
      "2024-09-13 18:44:17,503\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::train_agents()\u001b[39m (pid=13220, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1858, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\2771910933.py\", line 57, in train_agents\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 52, in train\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 87, in get_trajectory\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 158, in normalize_state\n",
      "IndexError: index 4 is out of bounds for axis 0 with size 4\n",
      "2024-09-13 18:44:17,505\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::train_agents()\u001b[39m (pid=1668, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1858, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\2771910933.py\", line 57, in train_agents\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 52, in train\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 87, in get_trajectory\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 158, in normalize_state\n",
      "IndexError: index 4 is out of bounds for axis 0 with size 4\n",
      "2024-09-13 18:44:17,506\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::train_agents()\u001b[39m (pid=13468, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1858, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\2771910933.py\", line 57, in train_agents\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 52, in train\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 87, in get_trajectory\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 158, in normalize_state\n",
      "IndexError: index 4 is out of bounds for axis 0 with size 4\n",
      "2024-09-13 18:44:17,508\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::train_agents()\u001b[39m (pid=3500, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1858, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\2771910933.py\", line 57, in train_agents\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 52, in train\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 87, in get_trajectory\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 158, in normalize_state\n",
      "IndexError: index 4 is out of bounds for axis 0 with size 4\n",
      "2024-09-13 18:44:17,510\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::train_agents()\u001b[39m (pid=20136, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1858, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\2771910933.py\", line 57, in train_agents\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 52, in train\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 87, in get_trajectory\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 158, in normalize_state\n",
      "IndexError: index 4 is out of bounds for axis 0 with size 4\n"
     ]
    }
   ],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 8\n",
    "\n",
    "    for run_index in range(1):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No data reup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 1\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 8\n",
    "\n",
    "    for run_index in range(1):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 1\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = False\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 8\n",
    "\n",
    "    for run_index in range(1):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 1\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = False\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 8\n",
    "\n",
    "    for run_index in range(1):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 1\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = False\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = False\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 8\n",
    "\n",
    "    for run_index in range(1):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input output scalling on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "RayTaskError(IndexError)",
     "evalue": "\u001b[36mray::train_agents()\u001b[39m (pid=20136, ip=127.0.0.1)\n  File \"python\\ray\\_raylet.pyx\", line 1858, in ray._raylet.execute_task\n  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\235902030.py\", line 57, in train_agents\n  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 52, in train\n  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 87, in get_trajectory\n  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 158, in normalize_state\nIndexError: index 4 is out of bounds for axis 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRayTaskError(IndexError)\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 73\u001b[0m\n\u001b[0;32m     68\u001b[0m start_agent_index \u001b[38;5;241m=\u001b[39m run_index \u001b[38;5;241m*\u001b[39m num_agents\n\u001b[0;32m     70\u001b[0m results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     71\u001b[0m     train_agents\u001b[38;5;241m.\u001b[39mremote(\u001b[38;5;28mstr\u001b[39m(start_agent_index \u001b[38;5;241m+\u001b[39m i), rundate) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_agents)]\n\u001b[1;32m---> 73\u001b[0m completed_results \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mget(results)\n\u001b[0;32m     74\u001b[0m all_results\u001b[38;5;241m.\u001b[39mextend(completed_results)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults for run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompleted_results\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\ray\\_private\\auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     20\u001b[0m     auto_init_ray()\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\ray\\_private\\client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\ray\\_private\\worker.py:2661\u001b[0m, in \u001b[0;36mget\u001b[1;34m(object_refs, timeout)\u001b[0m\n\u001b[0;32m   2655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2656\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid type of object refs, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(object_refs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, is given. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2657\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_refs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must either be an ObjectRef or a list of ObjectRefs. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2658\u001b[0m     )\n\u001b[0;32m   2660\u001b[0m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[1;32m-> 2661\u001b[0m values, debugger_breakpoint \u001b[38;5;241m=\u001b[39m worker\u001b[38;5;241m.\u001b[39mget_objects(object_refs, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m   2662\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[0;32m   2663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\ray\\_private\\worker.py:871\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[1;34m(self, object_refs, timeout, return_exceptions)\u001b[0m\n\u001b[0;32m    869\u001b[0m     global_worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mdump_object_store_memory_usage()\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[1;32m--> 871\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[1;31mRayTaskError(IndexError)\u001b[0m: \u001b[36mray::train_agents()\u001b[39m (pid=20136, ip=127.0.0.1)\n  File \"python\\ray\\_raylet.pyx\", line 1858, in ray._raylet.execute_task\n  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\235902030.py\", line 57, in train_agents\n  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 52, in train\n  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 87, in get_trajectory\n  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 158, in normalize_state\nIndexError: index 4 is out of bounds for axis 0 with size 4"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 18:44:27,547\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::train_agents()\u001b[39m (pid=3500, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1858, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\235902030.py\", line 57, in train_agents\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 52, in train\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 87, in get_trajectory\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 158, in normalize_state\n",
      "IndexError: index 4 is out of bounds for axis 0 with size 4\n",
      "2024-09-13 18:44:27,550\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::train_agents()\u001b[39m (pid=13468, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1858, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\235902030.py\", line 57, in train_agents\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 52, in train\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 87, in get_trajectory\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 158, in normalize_state\n",
      "IndexError: index 4 is out of bounds for axis 0 with size 4\n",
      "2024-09-13 18:44:27,552\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::train_agents()\u001b[39m (pid=2396, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1858, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\235902030.py\", line 57, in train_agents\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 52, in train\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 87, in get_trajectory\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 158, in normalize_state\n",
      "IndexError: index 4 is out of bounds for axis 0 with size 4\n",
      "2024-09-13 18:44:27,553\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::train_agents()\u001b[39m (pid=1668, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1858, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\235902030.py\", line 57, in train_agents\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 52, in train\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 87, in get_trajectory\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 158, in normalize_state\n",
      "IndexError: index 4 is out of bounds for axis 0 with size 4\n",
      "2024-09-13 18:44:27,555\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::train_agents()\u001b[39m (pid=9732, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1858, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\235902030.py\", line 57, in train_agents\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 52, in train\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 87, in get_trajectory\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 158, in normalize_state\n",
      "IndexError: index 4 is out of bounds for axis 0 with size 4\n",
      "2024-09-13 18:44:27,556\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::train_agents()\u001b[39m (pid=22460, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1858, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\235902030.py\", line 57, in train_agents\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 52, in train\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 87, in get_trajectory\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 158, in normalize_state\n",
      "IndexError: index 4 is out of bounds for axis 0 with size 4\n",
      "2024-09-13 18:44:27,558\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::train_agents()\u001b[39m (pid=13220, ip=127.0.0.1)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1858, in ray._raylet.execute_task\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\235902030.py\", line 57, in train_agents\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 52, in train\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 87, in get_trajectory\n",
      "  File \"C:\\Users\\Bernardo\\AppData\\Local\\Temp\\ipykernel_14448\\3825245558.py\", line 158, in normalize_state\n",
      "IndexError: index 4 is out of bounds for axis 0 with size 4\n"
     ]
    }
   ],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_qubits,n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 8\n",
    "\n",
    "    for run_index in range(1):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input scaling off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = False\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_qubits,n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 8\n",
    "\n",
    "    for run_index in range(1):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output scaling off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = False\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_qubits,n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 8  \n",
    "\n",
    "    for run_index in range(1):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input and output scaling off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = False\n",
    "    input_init = torch.nn.init.ones_\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = False\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_qubits,n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 8 \n",
    "\n",
    "    for run_index in range(1):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beta schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = True\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_qubits,n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 8\n",
    "\n",
    "    for run_index in range(1):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No data reup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input output scalling on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 1\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_qubits,n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 8\n",
    "\n",
    "    for run_index in range(1):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input scalling off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 1\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = False\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_qubits,n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 8\n",
    "\n",
    "    for run_index in range(1):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output scalling off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 1\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = False\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_qubits,n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 8\n",
    "\n",
    "    for run_index in range(1):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input output scalling off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 1\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = False\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = False\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_qubits,n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 8\n",
    "\n",
    "    for run_index in range(1):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acrobot tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acrobot UQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    state_dim = 6\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = three_measure_expval\n",
    "    policy_circuit = UQC_FullEnc(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 3\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.01, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'Acrobot-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 3\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acrobot jerbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 6\n",
    "    n_layers = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = torch.nn.init.ones_\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = three_measure_expval\n",
    "    policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 3\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'Acrobot-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 3\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acrobot TFQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 6\n",
    "    n_layers = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"ring\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = torch.nn.init.ones_\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = three_measure_expval\n",
    "    policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 3\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'Acrobot-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 3\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn Off PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(100)\n",
    "\n",
    "import os\n",
    "os.system(\"shutdown /s /t 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "import optuna\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(policy, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(policy, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "def objective_function(results):\n",
    "\n",
    "    results_mean = np.mean(results, axis=0)\n",
    "    area = np.abs(np.trapz(results_mean))\n",
    "    maximum_performance_area = float(len(results[0]) * 200)\n",
    "\n",
    "    # Create a metric called performance area and normalize it between 0 and 1\n",
    "    performance_area = area / maximum_performance_area\n",
    "    return performance_area\n",
    "\n",
    "'''\n",
    "def sum_and_average(results):\n",
    "\n",
    "    averages = np.mean(results, axis=1)\n",
    "    return np.mean(averages)\n",
    "'''\n",
    "\n",
    "def objective(trial):    \n",
    "\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling, beta, increase_rate = True, 1, 0.005\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr1 = trial.suggest_float(\"lr1\", 1e-5, 1e-1, log=True)\n",
    "    #lr2 = trial.suggest_float(\"lr2\", 1e-5, 1e-1, log=True)\n",
    "    #lr3 = trial.suggest_float(\"lr3\", 1e-5, 1e-1, log=True)\n",
    "    lr2, lr3 = 0.1, 0.1\n",
    "    lr_list= [lr1, lr2, lr3]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 10\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 1\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 5\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, trial.number) for i in range(num_agents))\n",
    "    performance_metric = objective_function(results)\n",
    "    #performance_metric = sum_and_average(results)\n",
    "    return performance_metric\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    print(f\"Best trial: {study.best_trial.value}\")\n",
    "    print(f\"Best parameters: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_output = F.softmax(torch.Tensor([0.1,0.9]) * 3, dim=0)\n",
    "print(softmax_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
