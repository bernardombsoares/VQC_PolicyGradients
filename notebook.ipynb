{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQCPG Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pennylane as qml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from functools import partial\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "import ray\n",
    "\n",
    "import gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10, 5)\n",
    "\n",
    "libraries = {\n",
    "    \"numpy\": np.__version__,\n",
    "    \"pennylane\": qml.__version__,\n",
    "    \"torch\": torch.__version__,\n",
    "    \"functools\": \"built-in\",\n",
    "    \"collections\": \"built-in\",\n",
    "    \"os\": \"built-in\",\n",
    "    \"json\": \"built-in\",\n",
    "    \"warnings\": \"built-in\",\n",
    "    \"time\": \"built-in\",\n",
    "    \"datetime\": datetime.now().isoformat(),\n",
    "    \"ray\": ray.__version__,\n",
    "    \"gym\": gym.__version__,\n",
    "    \"matplotlib\": plt.matplotlib.__version__\n",
    "}\n",
    "\n",
    "# Print versions\n",
    "for lib, version in libraries.items():\n",
    "    if version:\n",
    "        print(f\"{lib}: {version}\")\n",
    "    else:\n",
    "        print(f\"{lib}: version information not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def tensor_to_list(tensor):\n",
    "    \"\"\"\n",
    "    Convert a tensor or numpy array to a nested list.\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, list):\n",
    "        return [tensor_to_list(t) for t in tensor]\n",
    "    elif isinstance(tensor, dict):\n",
    "        return {key: tensor_to_list(value) for key, value in tensor.items()}\n",
    "    elif isinstance(tensor, np.ndarray):\n",
    "        return tensor.tolist()\n",
    "    elif isinstance(tensor, torch.Tensor):\n",
    "        return tensor.tolist()\n",
    "    else:\n",
    "        return tensor\n",
    "    \n",
    "def create_optimizer_with_lr(params, lr_list, use_amsgrad=False):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': p, 'lr': lr} for p, lr in zip(params, lr_list)\n",
    "    ], amsgrad=use_amsgrad)\n",
    "    return optimizer\n",
    "\n",
    "def get_function_representation(func):\n",
    "    \"\"\"\n",
    "    Returns the full name of a function or partial function with arguments.\n",
    "    \"\"\"\n",
    "    if isinstance(func, partial):\n",
    "        func_name = f\"{func.func.__module__}.{func.func.__name__}\"\n",
    "        args = \", \".join(map(str, func.args)) if func.args else \"\"\n",
    "        kwargs = \", \".join(f\"{k}={v}\" for k, v in func.keywords.items()) if func.keywords else \"\"\n",
    "        return f\"{func_name}({args}{', ' if args and kwargs else ''}{kwargs})\"\n",
    "    elif callable(func):\n",
    "        return f\"{func.__module__}.{func.__name__}\"\n",
    "    else:\n",
    "        return str(func)\n",
    "\n",
    "def get_instance_variables(instance):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of instance variables with formatted function names if callable.\n",
    "    \"\"\"\n",
    "    variables = {}\n",
    "    for name, value in vars(instance).items():\n",
    "        if callable(value):\n",
    "            variables[name] = get_function_representation(value)\n",
    "        else:\n",
    "            variables[name] = value\n",
    "    return variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_probs(qubits):\n",
    "    '''\n",
    "    Returns a list with the probability of each computational basis state\n",
    "    '''\n",
    "    return qml.probs(wires=range(qubits)) \n",
    "\n",
    "def two_measure_expval(qubits):\n",
    "    '''\n",
    "    Computes and returns the expectation values of two observables for a given number of qubits.\n",
    "\n",
    "    For the specified number of qubits, this function constructs a PauliZ observable that acts\n",
    "    on all qubits. It returns two expectation values:\n",
    "    - The first expectation value is for the constructed PauliZ observable.\n",
    "    - The second expectation value is for the negative of the constructed PauliZ observable.\n",
    "    '''\n",
    "    \n",
    "    pauli_string = qml.PauliZ(0)\n",
    "    for i in range(1, qubits):\n",
    "        pauli_string = pauli_string @ qml.PauliZ(i)\n",
    "    \n",
    "    expvals = []\n",
    "    expvals.append(qml.expval(pauli_string))\n",
    "    expvals.append(qml.expval(-pauli_string))\n",
    "\n",
    "    return expvals\n",
    "\n",
    "def three_measure_expval(qubits):\n",
    "    '''\n",
    "    Computes and returns the expectation values of three observables based on the number of qubits.\n",
    "\n",
    "    This function defines and evaluates three observables depending on the input number of qubits:\n",
    "    - For 1 qubit: PauliZ, PauliX, and negative PauliZ.\n",
    "    - For 2 qubits: PauliZ on the first qubit, tensor product of PauliZ on both qubits, and PauliZ on the second qubit.\n",
    "    - For 3 or more qubits: PauliZ on the first qubit, a chain of PauliZ on the intermediate qubits, and PauliZ on the last qubit.\n",
    "    '''\n",
    "\n",
    "    expvals = []\n",
    "\n",
    "    if qubits == 1:\n",
    "        first_observable = qml.PauliZ(0)\n",
    "        middle_observable = qml.PauliX(0)\n",
    "        last_observable = -qml.PauliZ(0)\n",
    "    elif qubits == 2:\n",
    "        first_observable = qml.PauliZ(0)\n",
    "        middle_observable = qml.PauliZ(0) @ qml.PauliZ(1) \n",
    "        last_observable = qml.PauliZ(1)       \n",
    "    elif qubits >= 4:\n",
    "        first_observable = qml.PauliZ(0)\n",
    "        middle_observable = qml.PauliZ(1)\n",
    "        for i in range(2, qubits - 1):\n",
    "            middle_observable = middle_observable @ qml.PauliZ(i)\n",
    "        last_observable = qml.PauliZ(qubits - 1)                    \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported number of qubits: only 1, 3, or 4 qubits are supported\")\n",
    "\n",
    "    expvals.append(qml.expval(first_observable))\n",
    "    expvals.append(qml.expval(middle_observable))\n",
    "    expvals.append(qml.expval(last_observable))\n",
    "\n",
    "    return expvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jerbi Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JerbiModel(nn.Module):\n",
    "    '''\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                n_qubits,\n",
    "                n_layers, \n",
    "                device,\n",
    "                shots,\n",
    "                diff_method, \n",
    "                entanglement,\n",
    "                entanglement_pattern, \n",
    "                entanglement_gate, \n",
    "                input_scaling, \n",
    "                input_init, \n",
    "                weight_init, \n",
    "                measure):\n",
    "        super(JerbiModel, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_scaling = input_scaling\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        # Call the error handling function\n",
    "        self.handle_errors_and_warnings()\n",
    "\n",
    "        # Initialize the device\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(self.device, wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(self.device, wires=self.n_qubits, shots=self.shots)\n",
    "\n",
    "        # Weight initialization\n",
    "        self.weight_shapes = {\n",
    "            \"input_params\": (self.n_layers, self.n_qubits, 2),\n",
    "            \"params\": (self.n_layers + 1, self.n_qubits, 2)\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"input_params\": self.input_init,\n",
    "            \"params\": self.weight_init\n",
    "        }\n",
    "\n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, params, input_params):\n",
    "\n",
    "            # Apply Hadamard gates to all qubits\n",
    "            qml.broadcast(qml.Hadamard, wires=range(self.n_qubits), pattern=\"single\")\n",
    "\n",
    "            # Apply layers and entanglement\n",
    "            for layer in range(self.n_layers):\n",
    "                for wire in range(self.n_qubits):\n",
    "                    qml.RZ(params[layer][wire][0], wires=wire)\n",
    "                    qml.RY(params[layer][wire][1], wires=wire)\n",
    "\n",
    "                if self.entanglement:\n",
    "                    qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "                # Input scaling\n",
    "                if self.input_scaling is True:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RY(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                        qml.RZ(input_params[layer][wire][1] * inputs[wire], wires=wire)\n",
    "                else:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RY(inputs[wire], wires=wire)\n",
    "                        qml.RZ(inputs[wire], wires=wire)\n",
    "\n",
    "            # Final layer\n",
    "            for wire in range(self.n_qubits):\n",
    "                qml.RZ(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "        model = qml.qnn.TorchLayer(qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        Gives inputs to the circuit and outputs the respective output\n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "     \n",
    "    def visualize_circuit(self):\n",
    "        '''\n",
    "        Draws the circuit\n",
    "        '''\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        qml.draw_mpl(self.qnode)(inputs, \n",
    "                                initialized_params[\"params\"], \n",
    "                                initialized_params[\"input_params\"])\n",
    "\n",
    "    def handle_errors_and_warnings(self):\n",
    "        ''' \n",
    "        Handles the errors and warnings\n",
    "        '''\n",
    "        # Check if the number of layers is valid\n",
    "        if self.n_layers < 1:\n",
    "            raise ValueError(\"Number of layers must be at least 1.\") \n",
    "\n",
    "    def get_parameters(self):\n",
    "        # Extract relevant attributes for JSON serialization\n",
    "        return {\n",
    "            \"Number of Qubits\": self.n_qubits,\n",
    "            \"Number of Layers\": self.n_layers,\n",
    "            \"Device\": str(self.device),  # Convert to string representation\n",
    "            \"Shots\": self.shots,\n",
    "            \"Differentiation Method\": self.diff_method,\n",
    "            \"Entanglement\": self.entanglement,\n",
    "            \"Entanglement Pattern\": self.entanglement_pattern,\n",
    "            \"Entanglement Gate\": get_function_representation(self.entanglement_gate),  # Use the helper function to represent the gate\n",
    "            \"Input Scaling\": self.input_scaling,\n",
    "            \"Input Initialization\": get_function_representation(self.input_init),  # Use the helper function for the initializer\n",
    "            \"Parameters Initialization\": get_function_representation(self.weight_init),  # Use the helper function for the initializer\n",
    "            \"Measurement Function\": get_function_representation(self.measure)  # Use the helper function for the measurement function\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def info(cls):\n",
    "        '''\n",
    "        Provides a summary of the JerbiModel class, including its parameters and methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        \n",
    "        Creates a parametrized quantum circuit based on the 'Parametrized quantum policies for reinforcement learning' paper by Sofiene Jerbi.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_qubits (int): \n",
    "            Number of qubits used in the quantum circuit.\n",
    "        \n",
    "        n_layers (int): \n",
    "            Number of layers in the quantum circuit. Each layer typically consists of parameterized rotations followed by entanglement gates.\n",
    "        \n",
    "        device (str): \n",
    "            The quantum device used for simulation or execution (e.g., 'default_qubit', 'lightning.qubit', 'lightning.gpu').\n",
    "\n",
    "        shots (int, optional): \n",
    "            Number of times the circuit gets executed (repeated measurements). If None, the circuit is executed with analytic calculations (no shot noise).\n",
    "        \n",
    "        diff_method (str): \n",
    "            Differentiation method used for training the model. Common options are 'best', 'parameter-shift', 'backprop', etc.\n",
    "\n",
    "        entanglement (bool):\n",
    "            If True, entanglement between qubits is implemented. The entanglement pattern and gate are defined by `entanglement_pattern` and `entanglement_gate`, respectively.\n",
    "        \n",
    "        entanglement_pattern (str): \n",
    "            Entanglement pattern used in the circuit, such as 'chain', 'ring', 'all_to_all', etc., as defined by qml.broadcast patterns.\n",
    "        \n",
    "        entanglement_gate (function): \n",
    "            Quantum gate used for entanglement, such as qml.CZ or qml.CNOT. This gate is applied between qubits according to the specified entanglement pattern.\n",
    "        \n",
    "        input_scaling (bool): \n",
    "            If True, input parameters are scaled by additional learnable parameters (input_params). The input is multiplied by these parameters before being applied to the qubits.\n",
    "        \n",
    "        input_init (function): \n",
    "            Function to initialize the input scaling parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, or any function defined by the user.\n",
    "        \n",
    "        weight_init (function): \n",
    "            Function to initialize the weights of the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.normal_, or any function defined by the user.\n",
    "        \n",
    "        measure (function): \n",
    "            Measurement function that takes the number of qubits as an argument and returns the measurement result. Common choices are `measure_probs`, `two_measure_expval`, or any user-defined measurement function.\n",
    "\n",
    "        Methods:\n",
    "        --------\n",
    "        generate_circuit(self): \n",
    "            Generates and initializes the quantum circuit based on the parameters.\n",
    "        \n",
    "        forward(self, inputs): \n",
    "            Takes inputs and passes them through the quantum circuit to get the output.\n",
    "\n",
    "        visualize_circuit(self): \n",
    "            Visualizes the generated quantum circuit for the given number of qubits using the initial parameters. Useful for debugging or analyzing the circuit design.\n",
    "\n",
    "        handle_errors_and_warnings(self): \n",
    "            Handles common errors and warnings, such as invalid parameter values, unsupported devices, and incompatible differentiation methods.\n",
    "            \n",
    "        \"\"\"\n",
    "        return info_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFQ Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfqTutorial(nn.Module):\n",
    "    '''\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                n_qubits,\n",
    "                n_layers, \n",
    "                device,\n",
    "                shots, \n",
    "                diff_method, \n",
    "                entanglement,\n",
    "                entanglement_pattern, \n",
    "                entanglement_gate, \n",
    "                input_scaling, \n",
    "                input_init, \n",
    "                weight_init, \n",
    "                measure):\n",
    "        super(TfqTutorial, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_scaling = input_scaling\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        # Call the error handling function\n",
    "        self.handle_errors_and_warnings()\n",
    "\n",
    "        # Initialize the device\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(self.device, wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(self.device, wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        # Weight initialization\n",
    "        self.weight_shapes = {\n",
    "            \"input_params\": (self.n_layers, self.n_qubits, 1),\n",
    "            \"params\": (self.n_layers + 1, self.n_qubits, 3)\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"input_params\": self.input_init,\n",
    "            \"params\": self.weight_init,\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, params, input_params):\n",
    "            \n",
    "            # Apply layers and entanglement\n",
    "            for layer in range(self.n_layers):\n",
    "                for wire in range(self.n_qubits):\n",
    "                    qml.RX(params[layer][wire][0], wires=wire)\n",
    "                    qml.RY(params[layer][wire][1], wires=wire)\n",
    "                    qml.RZ(params[layer][wire][2], wires=wire)\n",
    "\n",
    "                if self.entanglement:\n",
    "                    qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "                # Input scaling\n",
    "                if self.input_scaling:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RX(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                else:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RX(inputs[wire], wires=wire)\n",
    "                \n",
    "            # Final layer\n",
    "            for wire in range(self.n_qubits):\n",
    "                qml.RX(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "                qml.RZ(params[-1][wire][2], wires=wire)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)  \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        Gives inputs to the circuit and outputs the respective output\n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        '''\n",
    "        Draws the circuit\n",
    "        '''\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        qml.draw_mpl(self.qnode)(inputs, \n",
    "                                initialized_params[\"params\"], \n",
    "                                initialized_params[\"input_params\"])\n",
    "\n",
    "    def handle_errors_and_warnings(self):\n",
    "        ''' \n",
    "        Handles the errors and warnings\n",
    "        '''\n",
    "        # Check if the number of layers is valid\n",
    "        if self.n_layers < 1:\n",
    "            raise ValueError(\"Number of layers must be at least 1.\")\n",
    "\n",
    "    def get_parameters(self):\n",
    "        # Extract relevant attributes for JSON serialization\n",
    "        return {\n",
    "            \"Number of Qubits\": self.n_qubits,\n",
    "            \"Number of Layers\": self.n_layers,\n",
    "            \"Device\": str(self.device),  # Convert to string representation\n",
    "            \"Shots\": self.shots,\n",
    "            \"Differentiation Method\": self.diff_method,\n",
    "            \"Entanglement\": self.entanglement,\n",
    "            \"Entanglement Pattern\": self.entanglement_pattern,\n",
    "            \"Entanglement Gate\": get_function_representation(self.entanglement_gate),  # Use the helper function to represent the gate\n",
    "            \"Input Scaling\": self.input_scaling,\n",
    "            \"Input Initialization\": get_function_representation(self.input_init),  # Use the helper function for the initializer\n",
    "            \"Parameters Initialization\": get_function_representation(self.weight_init),  # Use the helper function for the initializer\n",
    "            \"Measurement Function\": get_function_representation(self.measure)  # Use the helper function for the measurement function\n",
    "        }        \n",
    "    \n",
    "    @classmethod\n",
    "    def info(cls):\n",
    "        '''\n",
    "        Provides a summary of the TFQ class, including its parameters and methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        \n",
    "        Creates a parameterized quantum circuit based on the TensorFlow Quantum tutorial in https://www.tensorflow.org/quantum/tutorials/quantum_reinforcement_learning\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_qubits (int): \n",
    "            Number of qubits used in the quantum circuit.\n",
    "        \n",
    "        n_layers (int): \n",
    "            Number of layers in the quantum circuit. Each layer typically consists of parameterized rotations followed by entanglement gates.\n",
    "        \n",
    "        device (str): \n",
    "            The quantum device used for simulation or execution (e.g., 'default_qubit', 'lightning.qubit').\n",
    "\n",
    "        shots (int, optional): \n",
    "            Number of times the circuit gets executed (repeated measurements). If None, the circuit is executed with analytic calculations (no shot noise).\n",
    "        \n",
    "        diff_method (str): \n",
    "            Differentiation method used for training the model. Common options are 'best', 'parameter-shift', 'backprop', etc.\n",
    "\n",
    "        entanglement (bool):\n",
    "            If True, entanglement between qubits is implemented. The entanglement pattern and gate are defined by `entanglement_pattern` and `entanglement_gate`, respectively.\n",
    "        \n",
    "        entanglement_pattern (str): \n",
    "            Entanglement pattern used in the circuit, such as 'chain', 'ring', 'all_to_all', etc., as defined by qml.broadcast patterns.\n",
    "        \n",
    "        entanglement_gate (function): \n",
    "            Quantum gate used for entanglement, such as qml.CZ or qml.CNOT. This gate is applied between qubits according to the specified entanglement pattern.\n",
    "        \n",
    "        input_scaling (bool): \n",
    "            If True, input parameters are scaled by additional learnable parameters (input_params). The input is multiplied by these parameters before being applied to the qubits.\n",
    "        \n",
    "        input_init (function): \n",
    "            Function to initialize the input scaling parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, or any function defined by the user.\n",
    "        \n",
    "        weight_init (function): \n",
    "            Function to initialize the weights of the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.normal_, or any function defined by the user.\n",
    "        \n",
    "        measure (function): \n",
    "            Measurement function that takes the number of qubits as an argument and returns the measurement result. Common choices are `measure_probs`, `two_measure_expval`, `three_measure_expval`, or any user-defined measurement function.\n",
    "\n",
    "        Methods:\n",
    "        --------\n",
    "        generate_circuit(self): \n",
    "            Generates and initializes the quantum circuit based on the parameters.\n",
    "        \n",
    "        forward(self, inputs): \n",
    "            Takes inputs and passes them through the quantum circuit to get the output.\n",
    "\n",
    "        visualize_circuit(self): \n",
    "            Visualizes the generated quantum circuit for the given number of qubits using the initial parameters. Useful for debugging or analyzing the circuit design.\n",
    "\n",
    "        handle_errors_and_warnings(self): \n",
    "            Handles common errors and warnings, such as invalid parameter values, unsupported devices, and incompatible differentiation methods.\n",
    "            \n",
    "        \"\"\"\n",
    "        return info_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UQC(nn.Module):\n",
    "    '''\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                n_qubits, \n",
    "                n_layers, \n",
    "                state_dim,\n",
    "                device,\n",
    "                shots, \n",
    "                diff_method,\n",
    "                encoding_type,\n",
    "                entanglement,\n",
    "                entanglement_pattern, \n",
    "                entanglement_gate, \n",
    "                input_init,\n",
    "                weight_init,\n",
    "                bias_init,\n",
    "                measure):\n",
    "        super(UQC, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.state_dim = state_dim\n",
    "        self.device = device\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.encoding_type = encoding_type\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.bias_init = bias_init\n",
    "        self.measure = measure\n",
    "        self.input_scaling = None\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(self.device, wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(self.device, wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        if self.encoding_type == 'full':\n",
    "            self.weight_shapes = {\n",
    "                \"input_params\": (self.n_layers, self.n_qubits, self.state_dim),\n",
    "                \"params\": (self.n_layers, self.n_qubits, 1),\n",
    "                \"bias\": (self.n_layers, self.n_qubits)\n",
    "            }\n",
    "        elif self.encoding_type == 'partial':\n",
    "            self.weight_shapes = {\n",
    "            \"input_params\": (self.n_layers, self.n_qubits, int(self.state_dim/self.n_qubits)),\n",
    "            \"params\": (self.n_layers, self.n_qubits, 1),\n",
    "            \"bias\": (self.n_layers, self.n_qubits)\n",
    "            }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"input_params\": self.input_init,\n",
    "            \"params\": self.weight_init,\n",
    "            \"bias\": self.bias_init\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, input_params, params, bias):\n",
    "\n",
    "            for layer in range(self.n_layers):\n",
    "                for wire in range(self.n_qubits):\n",
    "                    if self.encoding_type == 'full':\n",
    "                        hadamard_product = torch.dot(inputs.clone().detach(), input_params[layer][wire])\n",
    "                        angle = hadamard_product + bias[layer][wire]\n",
    "                    elif self.encoding_type == 'partial':\n",
    "                        separate_inputs = np.array_split(inputs,self.n_qubits)\n",
    "                        hadamard_product = torch.dot(separate_inputs[wire], input_params[layer][wire])\n",
    "                        angle = hadamard_product + bias[layer][wire]\n",
    "\n",
    "                    qml.RZ(angle, wires=wire)\n",
    "                    \n",
    "                    qml.RY(params[layer][wire][0], wires=wire)\n",
    "                    \n",
    "                if self.entanglement:\n",
    "                    qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(self.qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        Gives inputs to the circuit and outputs the respective output\n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.state_dim)], dtype=torch.float32)\n",
    "        \n",
    "        # Initialize all parameters using the provided initialization methods\n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        qml.draw_mpl(self.qnode)(inputs, \n",
    "                                initialized_params[\"weights\"], \n",
    "                                initialized_params[\"params\"], \n",
    "                                initialized_params[\"bias\"])\n",
    "\n",
    "    def handle_errors_and_warnings(self):\n",
    "        ''' \n",
    "        Handles the errors and warnings\n",
    "        '''\n",
    "        # Check if the number of layers is valid\n",
    "        if self.n_layers < 1:\n",
    "            raise ValueError(\"Number of layers must be at least 1.\")\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        # Extract relevant attributes for JSON serialization\n",
    "        return {\n",
    "            \"Number of Qubits\": self.n_qubits,\n",
    "            \"Number of Layers\": self.n_layers,\n",
    "            \"State Dimension\": self.state_dim,\n",
    "            \"Device\": str(self.device),  # Convert to string representation\n",
    "            \"Shots\": self.shots,\n",
    "            \"Differentiation Method\": self.diff_method,\n",
    "            \"Encoding Type\": self.encoding_type,\n",
    "            \"Entanglement\": self.entanglement,\n",
    "            \"Entanglement Pattern\": self.entanglement_pattern,\n",
    "            \"Entanglement Gate\": get_function_representation(self.entanglement_gate),  # Use the helper function to represent the gate\n",
    "            \"Input Initialization\": get_function_representation(self.input_init),  # Use the helper function for the initializer\n",
    "            \"Parameters Initialization\": get_function_representation(self.weight_init),  # Use the helper function for the initializer\n",
    "            \"Bias Initialization\": get_function_representation(self.bias_init),  # Use the helper function for the initializer\n",
    "            \"Measurement Function\": get_function_representation(self.measure)  # Use the helper function for the measurement function\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def info(cls):\n",
    "        '''\n",
    "        Provides a summary of the UQC class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        \n",
    "        Creates a parameterized quantum circuit based on the 'Data re-uploading for a universal quantum classifier' paper by Adrián Pérez-Salinas.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_qubits (int): \n",
    "            Number of qubits used in the quantum circuit.\n",
    "\n",
    "        n_layers (int): \n",
    "            Number of layers in the quantum circuit. Each layer consists of parameterized rotations and entanglement gates.\n",
    "\n",
    "        state_dim (int): \n",
    "            Dimensionality of the state space, determining the size of the weights associated with each qubit.\n",
    "\n",
    "        device (str): \n",
    "            The quantum device to be used for execution, such as 'default.qubit', 'lightning.qubit', etc.\n",
    "\n",
    "        shots (int, optional): \n",
    "            Number of times the circuit gets executed (repeated measurements). If None, the circuit is executed with analytic calculations (no shot noise).\n",
    "\n",
    "        diff_method (str): \n",
    "            Differentiation method used for training the model. Common options include 'best', 'parameter-shift', 'adjoint', and 'backprop'.\n",
    "\n",
    "        encoding_type (str): \n",
    "            Type of encoding used for the input data. Can be 'full' for complete encoding or 'partial' for partial encoding, which changes the shape of weights.\n",
    "\n",
    "        entanglement (bool): \n",
    "            If True, entanglement between qubits is implemented. The entanglement pattern and gate are defined in entanglement_pattern and entanglement_gate, respectively.\n",
    "\n",
    "        entanglement_pattern (str): \n",
    "            Entanglement pattern used in the circuit, such as 'chain', 'ring', 'all_to_all', etc., as defined by qml.broadcast patterns.\n",
    "\n",
    "        entanglement_gate (function): \n",
    "            Quantum gate used for entanglement, such as qml.CZ or qml.CNOT. This gate is applied between qubits according to the specified entanglement pattern.\n",
    "\n",
    "        input_init (function): \n",
    "            Function to initialize the weights of the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.normal_, or a user-defined function.\n",
    "\n",
    "        weight_init (function): \n",
    "            Function to initialize the parameters of the quantum circuit, similar to input_init.\n",
    "\n",
    "        bias_init (function): \n",
    "            Function to initialize the bias terms in the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.zeros_, or a user-defined function.\n",
    "\n",
    "        measure (function): \n",
    "            Measurement function that takes the number of qubits as an argument and returns the measurement result. Common choices are measure_probs, two_measure_expval, or any user-defined measurement function.\n",
    "        \n",
    "        Methods:\n",
    "        --------\n",
    "        generate_circuit(self): \n",
    "            Generates and initializes the quantum circuit based on the parameters.\n",
    "        \n",
    "        forward(self, inputs): \n",
    "            Takes inputs and passes them through the quantum circuit to get the output.\n",
    "\n",
    "        visualize_circuit(self): \n",
    "            Visualizes the generated quantum circuit for the given number of qubits using the initial parameters. Useful for debugging or analyzing the circuit design.\n",
    "\n",
    "        handle_errors_and_warnings(self): \n",
    "            Handles common errors and warnings, such as invalid parameter values, unsupported devices, and incompatible differentiation methods.\n",
    "        \"\"\"\n",
    "        return info_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyPostProcessing(nn.Module):\n",
    "    '''\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 n_qubits,\n",
    "                 n_actions,\n",
    "                 policy_type, \n",
    "                 beta_scheduling, \n",
    "                 beta,\n",
    "                 increase_rate, \n",
    "                 output_scaling,\n",
    "                 output_init):\n",
    "        super(PolicyPostProcessing, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_actions = n_actions\n",
    "        self.policy_type = policy_type\n",
    "        self.beta_scheduling = beta_scheduling\n",
    "        self.beta = beta\n",
    "        self.increase_rate = increase_rate\n",
    "        self.output_scaling = output_scaling\n",
    "        self.output_init = output_init\n",
    "\n",
    "        if self.output_scaling == True:\n",
    "            self.output_params = nn.parameter.Parameter(torch.Tensor(self.n_actions), requires_grad=True)\n",
    "            self.output_init(self.output_params)\n",
    "        else:\n",
    "            self.register_parameter('w_input', None)\n",
    "\n",
    "    def forward(self,probs):\n",
    "        ''' \n",
    "        Takes the VQC output and applies the selected policy \n",
    "        '''\n",
    "        if self.policy_type == 'raw_contiguous':\n",
    "            policy = self.raw_contiguous(probs)\n",
    "        elif self.policy_type == 'raw_parity':\n",
    "            policy = self.raw_parity(probs)\n",
    "        elif self.policy_type == 'softmax':\n",
    "            policy = self.softmax(probs)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid post-processing method specified.\")\n",
    "        return policy\n",
    "\n",
    "    def raw_contiguous(self,probs):\n",
    "        ''' \n",
    "        Applies the Contiguous partition to the probabilities of the basis states (design to work with qml.probs)\n",
    "        '''\n",
    "        log_n_actions = int(np.log2(self.n_actions))\n",
    "        \n",
    "        # Ensure the number of actions does not exceed the number of basis states (determined by n_qubits)\n",
    "        if log_n_actions > self.n_qubits:\n",
    "            raise ValueError('Number of actions exceeds the number of basis states!')\n",
    "\n",
    "        # Split the probabilities in a contiguous manner\n",
    "        probs_split = torch.chunk(probs, self.n_actions)\n",
    "        policy = [torch.sum(prob) for prob in probs_split]\n",
    "        return(torch.stack(policy))\n",
    "\n",
    "    def raw_parity(self,probs):\n",
    "        ''' \n",
    "        Applies the Parity partition to the probabilities of the basis states (design to work with qml.probs)\n",
    "        '''\n",
    "        log_n_actions = int(np.log2(self.n_actions))\n",
    "\n",
    "        # Check if the number of actions is a power of 2\n",
    "        if log_n_actions < 1.0 or not (np.floor(log_n_actions) == np.ceil(log_n_actions)):\n",
    "            raise NotImplementedError('Number of actions needs to be a power of two!')\n",
    "\n",
    "        # Ensure the number of actions does not exceed the number of qubits\n",
    "        if log_n_actions > self.n_qubits:\n",
    "            raise ValueError('Number of actions exceeds number of basis states!')\n",
    "\n",
    "        # Flatten the probability distribution to handle it as a single-dimensional array\n",
    "        if log_n_actions == 1:\n",
    "            summed_tensors = []\n",
    "            even_tensor = probs[::2]  # Elements at even indices\n",
    "            odd_tensor = probs[1::2]  # Elements at odd indices\n",
    "            summed_tensors.append(torch.sum(even_tensor))\n",
    "            summed_tensors.append(torch.sum(odd_tensor))\n",
    "        else:\n",
    "            probs_split = list(torch.chunk(probs, self.n_actions//2))\n",
    "            summed_tensors = []\n",
    "\n",
    "            for tensor in probs_split:\n",
    "                even_tensor = tensor[::2]  # Even indexed elements\n",
    "                odd_tensor = tensor[1::2]  # Odd indexed elements\n",
    "                summed_tensors.append(torch.sum(even_tensor))\n",
    "                summed_tensors.append(torch.sum(odd_tensor))\n",
    "\n",
    "        return torch.stack(summed_tensors)\n",
    "    \n",
    "    def softmax(self, probs):\n",
    "        ''' \n",
    "        Applies a softmax to the expected values of some observable\n",
    "        '''\n",
    "        if self.output_scaling == True:\n",
    "            probs *= self.output_params\n",
    "\n",
    "        scaled_output = probs * self.beta\n",
    "        softmax_output = F.softmax(scaled_output, dim=0)\n",
    "        return softmax_output\n",
    "    \n",
    "    def beta_schedule(self):\n",
    "        '''\n",
    "        Increases the inverse temperature parameter by 'increase_rate'\n",
    "        '''\n",
    "        if self.beta_scheduling == True and self.policy_type == 'softmax':\n",
    "            self.beta += self.increase_rate\n",
    "\n",
    "    def get_parameters(self):\n",
    "        # Extract relevant attributes for JSON serialization\n",
    "        return {\n",
    "            \"Policy Type\": self.policy_type,\n",
    "            \"Beta Scheduling\": self.beta_scheduling,\n",
    "            \"Beta\": self.beta,\n",
    "            \"Increase Rate\": self.increase_rate,\n",
    "            \"Output Scaling\": self.output_scaling,\n",
    "            \"Output Initialization\": get_function_representation(self.output_init),\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def info(cls):\n",
    "        '''\n",
    "        Provides a summary of the PolicyType class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "\n",
    "        Processes the output of the circuit into one of the implemented policies (Born Contiguous, Born Parity, Softmax)\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_actions (int): \n",
    "            Number of actions available for the agent to choose from.\n",
    "        \n",
    "        policy_type (str): \n",
    "            Type of policy applied to the probability distribution:\n",
    "            - 'raw_contiguous': Applies the Born Contiguous-like policy.\n",
    "            - 'raw_parity': Applies the Born Parity-like policy.\n",
    "            - 'softmax': Applies the softmax policy to the expectation values.\n",
    "\n",
    "        beta_scheduling (bool): \n",
    "            If True, updates the inverse temperature parameter (beta) after each episode. Used only for the softmax policy.\n",
    "        \n",
    "        beta (float): \n",
    "            Inverse temperature parameter used for scaling probabilities in the softmax policy.\n",
    "        \n",
    "        increase_rate (float): \n",
    "            Amount added to beta at the end of each episode, if beta_scheduling is True.\n",
    "        \n",
    "        output_scaling (bool): \n",
    "            If True, scales the output probabilities by learnable parameters.\n",
    "        \n",
    "        output_init (function): \n",
    "            Initialization function for output parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, etc.\n",
    "        \n",
    "        Methods:\n",
    "        -------\n",
    "        forward(self, probs):\n",
    "            Selects an action based on the chosen post_processing method.\n",
    "        \n",
    "        raw_contiguous(self, probs):\n",
    "            Sums up contiguous chunks of probabilities and returns the probability of each action.\n",
    "        \n",
    "        raw_parity(self, probs):\n",
    "            Sums up probabilities based on parity and returns the probability of each action.\n",
    "        \n",
    "        softmax(self, probs):\n",
    "            Applies a softmax function to the scaled probabilities and returns the probability of each action.\n",
    "        \n",
    "        beta_schedule(self):\n",
    "            Updates the beta parameter if beta_scheduling is True. Only applicable for the softmax method.\n",
    "        \"\"\"\n",
    "        return info_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Policy (Circuit + Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumPolicy(nn.Module):\n",
    "    '''\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "    def __init__(self, circuit, post_processing):\n",
    "        super(QuantumPolicy, self).__init__()\n",
    "        self.circuit = circuit\n",
    "        self.post_processing = post_processing\n",
    "\n",
    "    def sample(self, inputs):\n",
    "        '''\n",
    "        Samples an action from the action probability distribution\n",
    "        '''\n",
    "        policy = self.forward(inputs)\n",
    "        dist = torch.distributions.Categorical(policy)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), policy\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Input state is fed to the circuit - its output is then fed to the post processing \n",
    "        '''\n",
    "        probs = self.circuit.forward(inputs)\n",
    "        probs_processed = self.post_processing.forward(probs)\n",
    "        return probs_processed\n",
    "\n",
    "    @classmethod\n",
    "    def info(cls):\n",
    "        '''\n",
    "        Provides a summary of the QuantumPolicy class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "\n",
    "        Combines a quantum circuit for generating action probabilities with a post-processing step to create a valid probability distribution for action selection.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        circuit (object): \n",
    "            A quantum circuit instance that processes input states and generates raw probabilities.\n",
    "        \n",
    "        post_processing (object): \n",
    "            A post-processing instance that transforms raw probabilities into a valid probability distribution.\n",
    "\n",
    "        Methods:\n",
    "        -------\n",
    "        sample(inputs):\n",
    "            Samples an action based on the computed probability distribution for a given input.\n",
    "        \n",
    "        forward(inputs):\n",
    "            Computes the forward pass of the policy network by processing the inputs through the circuit \n",
    "            and the post-processing module.\n",
    "        \"\"\"\n",
    "        return info_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceAgent:\n",
    "    '''\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                policy, \n",
    "                policy_optimizer, \n",
    "                env_name, \n",
    "                n_episodes, \n",
    "                max_t, \n",
    "                gamma, \n",
    "                baseline, \n",
    "                batch_size, \n",
    "                normalize,\n",
    "                print_every, \n",
    "                verbose):\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy = policy.to(self.device)\n",
    "        self.policy_optimizer = policy_optimizer\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env_name = env_name\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.baseline = baseline\n",
    "        self.batch_size = batch_size\n",
    "        self.normalize = normalize\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.solved = False\n",
    "        self.scores = deque(maxlen=100)\n",
    "        self.batch_log_probs = []\n",
    "        self.batch_rewards = []\n",
    "        self.loss = torch.tensor(0.0)\n",
    "\n",
    "    def train(self, run_count=None, rundate = None, path=None, tensorboard=False):\n",
    "\n",
    "        # Creates data saving files if specified\n",
    "        if run_count is not None and path is not None:\n",
    "            data_path = create_directory(os.path.join(path, 'data'))\n",
    "            env_folder = create_directory(os.path.join(data_path, self.env_name))\n",
    "            experiment_folder_name = f\"{self.policy.circuit.__class__.__name__}_{self.policy.circuit.n_qubits}qubits_{self.policy.circuit.n_layers}layer_{rundate}\"\n",
    "            experiment_path = create_directory(os.path.join(env_folder, experiment_folder_name))\n",
    "            run_path = create_directory(os.path.join(experiment_path, f'run_{str(run_count)}'))\n",
    "            self.save_agent_data(experiment_path)\n",
    "        \n",
    "\n",
    "        # Create TensorBoard session if specified\n",
    "        if tensorboard:\n",
    "            writer = SummaryWriter(log_dir=run_path)\n",
    "\n",
    "        for i in range(1, self.n_episodes + 1):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Get episode\n",
    "            self.get_trajectory()\n",
    "\n",
    "            # Check if environment is solved\n",
    "            self.env_solved_verification()\n",
    "\n",
    "            # Update parameters if the batch is full and environment is not solved\n",
    "            if i > 1 and i % self.batch_size == 0 and not self.solved:\n",
    "                self.update_policy()\n",
    "                self.policy.post_processing.beta_schedule()\n",
    " \n",
    "            end_time = time.time()\n",
    "\n",
    "            # Calculate the runtime\n",
    "            self.runtime = end_time - start_time\n",
    "\n",
    "            # Write in the TensorBoard session\n",
    "            if tensorboard:\n",
    "                self.writer_function(writer, i)\n",
    "\n",
    "            # Save the episode data\n",
    "            if run_count is not None and path is not None:\n",
    "                self.save_data(run_path, i)\n",
    "\n",
    "            # Print out episode data\n",
    "            if self.verbose == 1:\n",
    "                print('Episode {} reward: {:.2f}\\t Solved: {}'.format(i, self.scores[-1], self.solved))\n",
    "            if self.verbose >= 2:\n",
    "                print('Episode {} reward: {:.2f}\\t Solved: {}\\t Runtime: {:.2f}\\t Loss: {:.2f}'.format(i, self.scores[-1], self.solved, self.runtime, self.loss))\n",
    "            if i % self.print_every == 0 and i > 1:\n",
    "                print('Last {} Episodes average reward: {:.2f}\\t'.format(len(self.scores), np.mean(self.scores)))\n",
    "\n",
    "        # Save the final weights\n",
    "        self.save_final_weights()\n",
    "\n",
    "        # Close TensorBoard session\n",
    "        if tensorboard:\n",
    "            writer.close()\n",
    "\n",
    "    def get_trajectory(self):\n",
    "        '''\n",
    "        Gets a trajectory based on the running policy until it runs out of bounds or achieves maximum reward of an episode\n",
    "        '''\n",
    "        # Get an episode trajectory\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()[0]\n",
    "        for t in range(self.max_t):\n",
    "            state_tensor = torch.tensor(self.normalize_state(state)).float().to(self.device)\n",
    "            action, log_prob, _ = self.policy.sample(state_tensor)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            \n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Save the episode reward\n",
    "        self.scores.append(sum(self.rewards))\n",
    "\n",
    "        # Save data from the episode to the batch\n",
    "        self.batch_log_probs.append(self.saved_log_probs)\n",
    "        self.batch_rewards.append(self.rewards)\n",
    "\n",
    "        # Clear data in case the agent already solved the environment\n",
    "        if self.solved is True:\n",
    "            self.batch_log_probs = []\n",
    "            self.batch_rewards = []\n",
    "      \n",
    "    def update_policy(self):\n",
    "        '''\n",
    "        Computes the loss and gradients and updates the policy via gradient methods\n",
    "        '''\n",
    "        # Discounting of the rewards\n",
    "        all_returns = []\n",
    "        for batch in self.batch_rewards:\n",
    "            R = 0\n",
    "            ep_return = []\n",
    "            for r in reversed(batch):\n",
    "                R = r + self.gamma * R\n",
    "                ep_return.insert(0, R)\n",
    "            ep_return = torch.tensor(ep_return).to(self.device)\n",
    "\n",
    "            # Standardization of the discounted returns\n",
    "            ep_return = (ep_return - ep_return.mean()) / (ep_return.std() + 1e-8)\n",
    "\n",
    "            all_returns.append(ep_return)\n",
    "\n",
    "        # Calculate the policy loss\n",
    "        policy_loss = []     \n",
    "        if self.baseline:\n",
    "            baseline = np.mean([sum(lst) for lst in all_returns])\n",
    "            for log_probs, ep_returns in zip(self.batch_log_probs, all_returns):\n",
    "                for log_prob, ret in zip(log_probs, ep_returns):\n",
    "                    advantage = ret - baseline \n",
    "                    policy_loss.append(-log_prob * advantage)\n",
    "        else:\n",
    "            for log_probs, ep_returns in zip(self.batch_log_probs, all_returns):\n",
    "                for log_prob, ret in zip(log_probs, ep_returns):\n",
    "                    policy_loss.append(-log_prob * ret)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        self.loss = torch.cat(policy_unsqueezed).mean()\n",
    "\n",
    "        # Compute the gradients \n",
    "        self.policy_optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "        # Clear old data\n",
    "        del all_returns\n",
    "        del policy_loss\n",
    "        del policy_unsqueezed \n",
    "        self.batch_log_probs = []\n",
    "        self.batch_rewards = []\n",
    "\n",
    "    def normalize_state(self, state):\n",
    "        '''\n",
    "        Processes the input state by reducing its dimensionality and normalizing it\n",
    "        '''\n",
    "        # State-space reduction for the Acrobot\n",
    "        if self.env_name in ('Acrobot-v0', 'Acrobot-v1'):\n",
    "            theta1 = np.arccos(state[0])\n",
    "            theta2 = np.arccos(state[2])\n",
    "            state = [theta1,theta2,state[4],state[5]]\n",
    "\n",
    "\n",
    "        # Normalize each feature by the maximum absolute value at each step\n",
    "        if self.normalize == True:\n",
    "            max_abs_value = max(abs(value) for value in state)\n",
    "            state = np.array([value / max_abs_value for value in state])\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def env_solved_verification(self):\n",
    "        '''\n",
    "        Checks if the environment is solved\n",
    "        '''\n",
    "        # Acrobot-v1\n",
    "        if self.env_name in ('Acrobot-v1'):\n",
    "            if np.mean(self.scores) > -125:\n",
    "                self.solved = True\n",
    "        \n",
    "        # CartPole-v0 and CartPole-v1\n",
    "        elif self.env_name in ('CartPole-v0','CartPole-v1'):\n",
    "            if np.mean(self.scores) > self.env.spec.reward_threshold:\n",
    "                self.solved = True\n",
    "        \n",
    "        else:              \n",
    "            warnings.warn(f\"No reward threshold defined for environment {self.env_name}. \"\n",
    "                          \"Consider specifying a solved condition explicitly.\",\n",
    "                          UserWarning\n",
    "            )\n",
    "\n",
    "    def save_agent_data(self, main_path):\n",
    "        '''\n",
    "        Stores the most relevant model parameters into a .json file.\n",
    "        '''\n",
    "        # Use the get_parameters method to get Circuit Parameters, Policy Parameters and Agent Parameters\n",
    "        circuit_params = self.policy.circuit.get_parameters()  # Get circuit parameters\n",
    "        policy_params = self.policy.post_processing.get_parameters()  # Get policy parameters\n",
    "        agent_params = self.get_parameters()  # Get agent parameters\n",
    "        \n",
    "        # Create a structured dictionary\n",
    "        agent_variables = {\n",
    "            \"Circuit Parameters\": circuit_params,\n",
    "            \"Policy Parameters\": policy_params,\n",
    "            \"Agent Parameters\": agent_params\n",
    "        }\n",
    "\n",
    "        # Convert sets to lists\n",
    "        def convert_sets_to_lists(obj):\n",
    "            if isinstance(obj, set):\n",
    "                return list(obj)\n",
    "            elif isinstance(obj, dict):\n",
    "                return {key: convert_sets_to_lists(value) for key, value in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert_sets_to_lists(item) for item in obj]\n",
    "            else:\n",
    "                return obj\n",
    "\n",
    "        # Convert sets in agent_variables\n",
    "        agent_variables = convert_sets_to_lists(agent_variables)\n",
    "\n",
    "        # Save as JSON\n",
    "        with open(os.path.join(main_path, \"agent_characteristics.json\"), \"w\") as f:\n",
    "            json.dump(agent_variables, f, indent=4)\n",
    "\n",
    "    def save_data(self, run_path, iteration):\n",
    "        '''\n",
    "        Saves the data into a .npz file for each episode\n",
    "        '''\n",
    "        data_file = os.path.join(run_path, \"run_data.npz\")\n",
    "        \n",
    "        # Load existing data if the file exists\n",
    "        if os.path.exists(data_file):\n",
    "            data = np.load(data_file, allow_pickle=True)\n",
    "            old_episode_reward = data['episode_reward'].tolist()\n",
    "            old_loss = data['loss'].tolist()\n",
    "            old_runtime = data['runtime'].tolist()\n",
    "            old_gradients = data['gradients'].tolist()\n",
    "            old_params = data['params'].tolist()\n",
    "        else:\n",
    "            old_episode_reward = []\n",
    "            old_loss = []\n",
    "            old_runtime = []\n",
    "            old_gradients = []\n",
    "            old_params = []\n",
    "\n",
    "        # Add episode reward and runtime\n",
    "        old_episode_reward.append(self.scores[-1])\n",
    "        old_runtime.append(self.runtime)\n",
    "\n",
    "        # Stores the loss and parameter gradients when batch is full\n",
    "        current_episode_gradients = []\n",
    "        current_episode_params = []\n",
    "        if iteration % self.batch_size == 0 and iteration > 1 and self.solved is False:\n",
    "            old_loss.append(self.loss.item())\n",
    "            for name, param in self.policy.circuit.named_parameters():\n",
    "                # Get the parameter values and flatten them\n",
    "                param_array = param.cpu().detach().numpy().flatten()\n",
    "                \n",
    "                # Append the parameter values to the old_parameters list\n",
    "                current_episode_params.append(param_array)\n",
    "\n",
    "                # If the parameter has a gradient, get and flatten it\n",
    "                if param.grad is not None:\n",
    "                    grad_array = param.grad.cpu().numpy().flatten()\n",
    "                    current_episode_gradients.append(grad_array)\n",
    "\n",
    "            # Concatenate the parameters and gradients into a single array for each episode\n",
    "            flattened_parameters = np.concatenate(current_episode_params)\n",
    "            flattened_gradients = np.concatenate(current_episode_gradients)\n",
    "\n",
    "            old_params.append(flattened_parameters)\n",
    "            old_gradients.append(flattened_gradients)\n",
    "            \n",
    "        # Save data to .npz file\n",
    "        np.savez_compressed(data_file,\n",
    "                            episode_reward=np.array(old_episode_reward),\n",
    "                            loss=np.array(old_loss),\n",
    "                            runtime=np.array(old_runtime),\n",
    "                            gradients=np.array(old_gradients, dtype=object),\n",
    "                            params=np.array(old_params,dtype=object))\n",
    "\n",
    "        # Clear old data lists to free up memory\n",
    "        del old_episode_reward[:]\n",
    "        del old_loss[:]\n",
    "        del old_runtime[:]\n",
    "        del old_gradients[:]\n",
    "        del old_params[:]\n",
    "\n",
    "    def save_final_weights(self, run_path):\n",
    "        '''\n",
    "        Saves the final model weights into the .npz file at the end of training.\n",
    "        '''\n",
    "        data_file = os.path.join(run_path, \"run_data.npz\")\n",
    "\n",
    "        # Load existing data if the file exists\n",
    "        if os.path.exists(data_file):\n",
    "            data = dict(np.load(data_file, allow_pickle=True))\n",
    "        else:\n",
    "            data = {}\n",
    "\n",
    "        # Extract and save final weights\n",
    "        final_weights = {name: param.detach().cpu().numpy() for name, param in self.policy.named_parameters()}\n",
    "        data['final_weights'] = final_weights\n",
    "\n",
    "        # Save updated data with final weights to .npz file\n",
    "        np.savez_compressed(data_file, **data)\n",
    "    \n",
    "    def writer_function(self, writer, iteration):\n",
    "        '''\n",
    "        Stores data into a tensorboard session\n",
    "        '''\n",
    "        writer.add_scalar(\"Episode Reward\", self.scores[-1], global_step=iteration)\n",
    "        writer.add_scalar(\"Runtime\", self.runtime, global_step=iteration)\n",
    "        writer.add_scalar(\"Loss\", self.loss.item(), global_step=iteration)\n",
    "\n",
    "        gradients = []\n",
    "        for name, param in self.policy.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if name == 'input_params' or name == 'params':\n",
    "                    gradients.append(param.grad.view(-1))\n",
    "\n",
    "        # Concatenate all collected gradients into a single tensor and calculate L2 norm of the combined gradients\n",
    "        if gradients:\n",
    "            combined_gradients = torch.cat(gradients)\n",
    "            combined_grad_norm = torch.norm(combined_gradients).item()\n",
    "            \n",
    "            # Log the combined gradient norm\n",
    "            writer.add_scalar(\"Gradient Norm/Combined\", combined_grad_norm, global_step=iteration)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        # Extract specified attributes for JSON serialization\n",
    "        return {\n",
    "            \"Environment\": self.env_name,\n",
    "            \"Gamma (discounting factor)\": self.gamma,\n",
    "            \"Baseline\": self.baseline,\n",
    "            \"Batch Size\": self.batch_size,\n",
    "            \"Normalize\": self.normalize,\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def info(cls):\n",
    "        '''\n",
    "        Provides a summary of the ReinforceAgent class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "\n",
    "        Implements a Reinforcement Learning agent using the REINFORCE algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        policy (PolicyType): \n",
    "            An instance of the policy class that defines the action selection process based on the outputs of the VQC.\n",
    "        \n",
    "        policy_optimizer (torch.optim.Optimizer): \n",
    "            Optimizer used for updating the policy parameters during training.\n",
    "        \n",
    "        env_name (str): \n",
    "            Name of the environment to interact with, typically defined in OpenAI Gym.\n",
    "        \n",
    "        n_episodes (int): \n",
    "            Total number of episodes for training the agent.\n",
    "        \n",
    "        max_t (int): \n",
    "            Maximum number of time steps per episode.\n",
    "        \n",
    "        gamma (float): \n",
    "            Discount factor for future rewards, where 0 < gamma < 1.\n",
    "        \n",
    "        baseline (bool): \n",
    "            If True, applies a baseline to reduce variance in the policy gradient estimates.\n",
    "        \n",
    "        batch_size (int): \n",
    "            Number of episodes after which the policy parameters will be updated.\n",
    "        \n",
    "        normalize (bool): \n",
    "            If True, normalizes the state input for the agent.\n",
    "        \n",
    "        print_every (int): \n",
    "            Number of episodes after which to print the average reward.\n",
    "        \n",
    "        verbose (int): \n",
    "            Level of verbosity for outputting training details (0: none, 1: basic, 2: detailed).\n",
    "\n",
    "        Methods:\n",
    "        -------\n",
    "        train(self, run_count=None, rundate=None, path=None, tensorboard=False):\n",
    "            Trains the agent by collecting episodes and updating the policy using the REINFORCE algorithm.\n",
    "\n",
    "        get_trajectory(self):\n",
    "            Gathers a trajectory from the environment using the current policy until the episode ends.\n",
    "\n",
    "        update_policy(self):\n",
    "            Computes the loss, applies gradients, and updates the policy parameters.\n",
    "\n",
    "        normalize_state(self, state):\n",
    "            Normalizes and processes the input state to reduce dimensionality.\n",
    "\n",
    "        env_solved_verification(self):\n",
    "            Checks if the environment has been solved based on the average score.\n",
    "\n",
    "        save_agent_data(self, main_path):\n",
    "            Saves the agent's parameters and model characteristics to a JSON file.\n",
    "\n",
    "        save_data(self, run_path, iteration):\n",
    "            Saves episode data, including rewards, loss, and gradients, to a .npz file.\n",
    "\n",
    "        save_final_weights(self, run_path):\n",
    "            Saves the final model weights at the end of training to a .npz file.\n",
    "\n",
    "        writer_function(self, writer, iteration):\n",
    "            Logs episode data to TensorBoard for visualization.\n",
    "\n",
    "        get_parameters(self):\n",
    "            Returns a dictionary of important agent parameters for JSON serialization.\n",
    "        \"\"\"\n",
    "        return info_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jerbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 5\n",
    "device = 'lightning.qubit'\n",
    "shots = None\n",
    "diff_method = 'adjoint' \n",
    "entanglement = True\n",
    "entanglement_pattern = 'all_to_all'\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).normal_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval\n",
    "policy_circuit = JerbiModel(n_qubits, \n",
    "                            n_layers, \n",
    "                            device, \n",
    "                            shots, \n",
    "                            diff_method, \n",
    "                            entanglement, \n",
    "                            entanglement_pattern, \n",
    "                            entanglement_gate, \n",
    "                            input_scaling, \n",
    "                            input_init, \n",
    "                            weight_init, \n",
    "                            policy_circuit_measure)\n",
    "\n",
    "policy_circuit.visualize_circuit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 5\n",
    "device = 'lightning.qubit'\n",
    "shots = None\n",
    "diff_method = 'adjoint' \n",
    "entanglement = True\n",
    "entanglement_pattern = 'all_to_all'\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).normal_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval\n",
    "policy_circuit = JerbiModel(n_qubits, \n",
    "                            n_layers, \n",
    "                            device, \n",
    "                            shots, \n",
    "                            diff_method, \n",
    "                            entanglement, \n",
    "                            entanglement_pattern, \n",
    "                            entanglement_gate, \n",
    "                            input_scaling, \n",
    "                            input_init, \n",
    "                            weight_init, \n",
    "                            policy_circuit_measure)\n",
    "\n",
    "\n",
    "n_actions = 2\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = False\n",
    "beta = 1\n",
    "increase_rate = 0.0005\n",
    "output_scaling = True\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_post_process = PolicyPostProcessing(n_qubits, n_actions, \n",
    "                         post_processing, \n",
    "                         beta_scheduling, \n",
    "                         beta, increase_rate, \n",
    "                         output_scaling, \n",
    "                         output_init)\n",
    "\n",
    "policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "policy_lr_list= [0.1, 0.01, 0.1]  # [weights, input_weights, output_weights]\n",
    "policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "n_episodes = 1000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "baseline = True\n",
    "batch_size = 10\n",
    "normalize = True\n",
    "print_every = 100\n",
    "verbose = 1\n",
    "reinforce_update = ReinforceAgent(policy, \n",
    "                                  policy_optimizer, \n",
    "                                  env_name, \n",
    "                                  n_episodes, \n",
    "                                  max_t, \n",
    "                                  gamma, \n",
    "                                  baseline, \n",
    "                                  batch_size,\n",
    "                                  normalize,\n",
    "                                  print_every, \n",
    "                                  verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 5\n",
    "shots = None\n",
    "diff_method = 'adjoint' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).normal_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval\n",
    "policy_circuit = TfqTutorial(n_qubits,\n",
    "                            n_layers, \n",
    "                            shots, \n",
    "                            diff_method, \n",
    "                            entanglement, \n",
    "                            entanglement_pattern, \n",
    "                            entanglement_gate, \n",
    "                            input_scaling, \n",
    "                            input_init, \n",
    "                            weight_init, \n",
    "                            policy_circuit_measure)\n",
    "\n",
    "policy_circuit.visualize_circuit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 5\n",
    "shots = None\n",
    "diff_method = 'adjoint' \n",
    "entanglement = True\n",
    "entanglement_pattern = 'all_to_all'\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval\n",
    "policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "\n",
    "n_actions = 2\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = False\n",
    "beta = 1\n",
    "increase_rate = 0.0005\n",
    "output_scaling = True\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                         post_processing, \n",
    "                         beta_scheduling, \n",
    "                         beta, increase_rate, \n",
    "                         output_scaling, \n",
    "                         output_init)\n",
    "\n",
    "policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "n_episodes = 1000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "baseline = True\n",
    "batch_size = 10\n",
    "normalize = True\n",
    "print_every = 100\n",
    "verbose = 1\n",
    "reinforce_update = ReinforceAgent(policy, \n",
    "                                  policy_optimizer, \n",
    "                                  env_name, \n",
    "                                  n_episodes, \n",
    "                                  max_t, \n",
    "                                  gamma, \n",
    "                                  baseline, \n",
    "                                  batch_size,\n",
    "                                  normalize,\n",
    "                                  print_every, \n",
    "                                  verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UQC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 5\n",
    "n_layers = 5\n",
    "state_dim = 4\n",
    "device = 'lightning.qubit'\n",
    "shots = None\n",
    "diff_method = 'adjoint' \n",
    "encoding = 'full'\n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "bias_init = torch.nn.init.zeros_\n",
    "policy_circuit_measure = two_measure_expval\n",
    "policy_circuit = UQC(n_qubits,\n",
    "                    n_layers, \n",
    "                    state_dim,\n",
    "                    device,\n",
    "                    shots, \n",
    "                    diff_method,\n",
    "                    encoding,\n",
    "                    entanglement, \n",
    "                    entanglement_pattern, \n",
    "                    entanglement_gate,\n",
    "                    weight_init,\n",
    "                    weight_init,\n",
    "                    bias_init, \n",
    "                    policy_circuit_measure)\n",
    "\n",
    "policy_circuit.visualize_circuit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 5\n",
    "state_dim = 4\n",
    "device = 'lightning.qubit'\n",
    "shots = None\n",
    "diff_method = 'adjoint' \n",
    "encoding = 'full'\n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "bias_init = torch.nn.init.zeros_\n",
    "policy_circuit_measure = two_measure_expval\n",
    "policy_circuit = UQC(n_qubits,\n",
    "                    n_layers, \n",
    "                    state_dim,\n",
    "                    device,\n",
    "                    shots, \n",
    "                    diff_method,\n",
    "                    encoding,\n",
    "                    entanglement, \n",
    "                    entanglement_pattern, \n",
    "                    entanglement_gate,\n",
    "                    weight_init,\n",
    "                    weight_init,\n",
    "                    bias_init, \n",
    "                    policy_circuit_measure)\n",
    "\n",
    "n_actions = 2\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = False\n",
    "beta = 1\n",
    "increase_rate = 0.0005\n",
    "output_scaling = True\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_post_process = PolicyPostProcessing( n_qubits,\n",
    "                                            n_actions, \n",
    "                                            post_processing, \n",
    "                                            beta_scheduling, \n",
    "                                            beta, increase_rate, \n",
    "                                            output_scaling, \n",
    "                                            output_init)\n",
    "\n",
    "policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [input_weights, weights, bias, output_weights]\n",
    "policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "n_episodes = 1000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "baseline = True\n",
    "batch_size = 10\n",
    "normalize = True\n",
    "print_every = 100\n",
    "verbose = 1\n",
    "reinforce_update = ReinforceAgent(policy, \n",
    "                                  policy_optimizer, \n",
    "                                  env_name, \n",
    "                                  n_episodes, \n",
    "                                  max_t, \n",
    "                                  gamma, \n",
    "                                  baseline, \n",
    "                                  batch_size,\n",
    "                                  normalize,\n",
    "                                  print_every, \n",
    "                                  verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jerbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    device = 'lightning.qubit'\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = JerbiModel(n_qubits, n_layers,device, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_qubits, n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.1, 0.01, 0.1]  # [input_weights, weight, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    agents_per_run = 2\n",
    "    num_agents = 4\n",
    "\n",
    "    for run_index in range(agents_per_run):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    device = 'lightning.qubit'\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = TfqTutorial(n_qubits, n_layers,device, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_qubits, n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.1, 0.01, 0.1]  # [input_weights, weight, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    agents_per_run = 2\n",
    "    num_agents = 4\n",
    "\n",
    "    for run_index in range(agents_per_run):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    device = 'lightning.qubit'\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    encoding = 'full'\n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    bias_init = torch.nn.init.zeros_\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = UQC(n_qubits,\n",
    "                        n_layers, \n",
    "                        state_dim,\n",
    "                        device,\n",
    "                        shots, \n",
    "                        diff_method,\n",
    "                        encoding,\n",
    "                        entanglement, \n",
    "                        entanglement_pattern, \n",
    "                        entanglement_gate,\n",
    "                        input_init,\n",
    "                        weight_init,\n",
    "                        bias_init, \n",
    "                        policy_circuit_measure)\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing( n_qubits,\n",
    "                                                n_actions, \n",
    "                                                post_processing, \n",
    "                                                beta_scheduling, \n",
    "                                                beta, increase_rate, \n",
    "                                                output_scaling, \n",
    "                                                output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer = create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    agents_per_run = 2\n",
    "    num_agents = 4\n",
    "\n",
    "    for run_index in range(agents_per_run):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
