{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pennylane as qml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from functools import partial\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "import ray\n",
    "\n",
    "import gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def tensor_to_list(tensor):\n",
    "    \"\"\"\n",
    "    Convert a tensor or numpy array to a nested list.\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, list):\n",
    "        return [tensor_to_list(t) for t in tensor]\n",
    "    elif isinstance(tensor, dict):\n",
    "        return {key: tensor_to_list(value) for key, value in tensor.items()}\n",
    "    elif isinstance(tensor, np.ndarray):\n",
    "        return tensor.tolist()\n",
    "    elif isinstance(tensor, torch.Tensor):\n",
    "        return tensor.tolist()\n",
    "    else:\n",
    "        return tensor\n",
    "    \n",
    "def create_optimizer_with_lr(params, lr_list, use_amsgrad=False):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': p, 'lr': lr} for p, lr in zip(params, lr_list)\n",
    "    ], amsgrad=use_amsgrad)\n",
    "    return optimizer\n",
    "\n",
    "def get_function_representation(func):\n",
    "    \"\"\"\n",
    "    Returns the full name of a function or partial function with arguments.\n",
    "    \"\"\"\n",
    "    if isinstance(func, partial):\n",
    "        func_name = f\"{func.func.__module__}.{func.func.__name__}\"\n",
    "        args = \", \".join(map(str, func.args)) if func.args else \"\"\n",
    "        kwargs = \", \".join(f\"{k}={v}\" for k, v in func.keywords.items()) if func.keywords else \"\"\n",
    "        return f\"{func_name}({args}{', ' if args and kwargs else ''}{kwargs})\"\n",
    "    elif callable(func):\n",
    "        return f\"{func.__module__}.{func.__name__}\"\n",
    "    else:\n",
    "        return str(func)\n",
    "\n",
    "def get_instance_variables(instance):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of instance variables with formatted function names if callable.\n",
    "    \"\"\"\n",
    "    variables = {}\n",
    "    for name, value in vars(instance).items():\n",
    "        if callable(value):\n",
    "            variables[name] = get_function_representation(value)\n",
    "        else:\n",
    "            variables[name] = value\n",
    "    return variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_probs(qubits):\n",
    "    return qml.probs(wires=range(qubits)) \n",
    "\n",
    "def two_measure_expval(qubits):\n",
    "    pauli_string = qml.PauliZ(0)\n",
    "    for i in range(1, qubits):\n",
    "        pauli_string = pauli_string @ qml.PauliZ(i)\n",
    "    \n",
    "    expvals = []\n",
    "    expvals.append(qml.expval(pauli_string))\n",
    "    expvals.append(qml.expval(-pauli_string))\n",
    "\n",
    "    return expvals\n",
    "\n",
    "def three_measure_expval(qubits):\n",
    "    expvals = []\n",
    "\n",
    "    if qubits == 1:\n",
    "        first_observable = qml.PauliZ(0)\n",
    "        middle_observable = qml.PauliX(0)\n",
    "        last_observable = -qml.PauliZ(0)\n",
    "    elif qubits == 2:\n",
    "        first_observable = qml.PauliZ(0)\n",
    "        middle_observable = qml.PauliZ(0) @ qml.PauliZ(1) \n",
    "        last_observable = qml.PauliZ(1)       \n",
    "    elif qubits >= 4:\n",
    "        first_observable = qml.PauliZ(0)\n",
    "        middle_observable = qml.PauliZ(1)\n",
    "        for i in range(2, qubits - 1):\n",
    "            middle_observable = middle_observable @ qml.PauliZ(i)\n",
    "        last_observable = qml.PauliZ(qubits - 1)                    \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported number of qubits: only 1, 3, or 4 qubits are supported\")\n",
    "\n",
    "    expvals.append(qml.expval(first_observable))\n",
    "    expvals.append(qml.expval(middle_observable))\n",
    "    expvals.append(qml.expval(last_observable))\n",
    "\n",
    "    return expvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jerbi Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JerbiModel(nn.Module):\n",
    "    '''\n",
    "    Creates a parametrized quantum circuit based on the 'Parametrized quantum policies for reinforcement learning' paper by Sofiene Jerbi.\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                n_qubits,\n",
    "                n_layers, \n",
    "                device,\n",
    "                shots,\n",
    "                diff_method, \n",
    "                entanglement,\n",
    "                entanglement_pattern, \n",
    "                entanglement_gate, \n",
    "                input_scaling, \n",
    "                input_init, \n",
    "                weight_init, \n",
    "                measure):\n",
    "        super(JerbiModel, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_scaling = input_scaling\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        # Call the error handling function\n",
    "        self.handle_errors_and_warnings()\n",
    "\n",
    "        # Initialize the device\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(self.device, wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(self.device, wires=self.n_qubits, shots=self.shots)\n",
    "\n",
    "        # Weight initialization\n",
    "        self.weight_shapes = {\n",
    "            \"input_params\": (self.n_layers, self.n_qubits, 2),\n",
    "            \"params\": (self.n_layers + 1, self.n_qubits, 2)\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"input_params\": self.input_init,\n",
    "            \"params\": self.weight_init\n",
    "        }\n",
    "\n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, params, input_params):\n",
    "\n",
    "            # Apply Hadamard gates to all qubits\n",
    "            qml.broadcast(qml.Hadamard, wires=range(self.n_qubits), pattern=\"single\")\n",
    "\n",
    "            # Apply layers and entanglement\n",
    "            for layer in range(self.n_layers):\n",
    "                for wire in range(self.n_qubits):\n",
    "                    qml.RZ(params[layer][wire][0], wires=wire)\n",
    "                    qml.RY(params[layer][wire][1], wires=wire)\n",
    "\n",
    "                if self.entanglement:\n",
    "                    qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "                # Input scaling\n",
    "                if self.input_scaling is True:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RY(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                        qml.RZ(input_params[layer][wire][1] * inputs[wire], wires=wire)\n",
    "                else:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RY(inputs[wire], wires=wire)\n",
    "                        qml.RZ(inputs[wire], wires=wire)\n",
    "\n",
    "            # Final layer\n",
    "            for wire in range(self.n_qubits):\n",
    "                qml.RZ(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "        model = qml.qnn.TorchLayer(qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        Gives inputs to the circuit and outputs the respective output\n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "     \n",
    "    def visualize_circuit(self):\n",
    "        '''\n",
    "        Draws the circuit\n",
    "        '''\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        qml.draw_mpl(self.qnode)(inputs, \n",
    "                                initialized_params[\"params\"], \n",
    "                                initialized_params[\"input_params\"])\n",
    "\n",
    "    def handle_errors_and_warnings(self):\n",
    "        ''' \n",
    "        Handles the errors and warnings\n",
    "        '''\n",
    "        # Check if the number of layers is valid\n",
    "        if self.n_layers < 1:\n",
    "            raise ValueError(\"Number of layers must be at least 1.\") \n",
    "\n",
    "    def get_parameters(self):\n",
    "        # Extract relevant attributes for JSON serialization\n",
    "        return {\n",
    "            \"Number of Qubits\": self.n_qubits,\n",
    "            \"Number of Layers\": self.n_layers,\n",
    "            \"Device\": str(self.device),  # Convert to string representation\n",
    "            \"Shots\": self.shots,\n",
    "            \"Differentiation Method\": self.diff_method,\n",
    "            \"Entanglement\": self.entanglement,\n",
    "            \"Entanglement Pattern\": self.entanglement_pattern,\n",
    "            \"Entanglement Gate\": get_function_representation(self.entanglement_gate),  # Use the helper function to represent the gate\n",
    "            \"Input Scaling\": self.input_scaling,\n",
    "            \"Input Initialization\": get_function_representation(self.input_init),  # Use the helper function for the initializer\n",
    "            \"Parameters Initialization\": get_function_representation(self.weight_init),  # Use the helper function for the initializer\n",
    "            \"Measurement Function\": get_function_representation(self.measure)  # Use the helper function for the measurement function\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def info(cls):\n",
    "        '''\n",
    "        Provides a summary of the JerbiModel class, including its parameters and methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        JerbiModel: A Quantum Neural Network model that creates a Parameterized Quantum Circuit based on the design in https://doi.org/10.48550/arXiv.2103.05577.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_qubits (int): \n",
    "            Number of qubits used in the quantum circuit.\n",
    "        \n",
    "        n_layers (int): \n",
    "            Number of layers in the quantum circuit. Each layer typically consists of parameterized rotations followed by entanglement gates.\n",
    "        \n",
    "        device (str): \n",
    "            The quantum device used for simulation or execution (e.g., 'default_qubit', 'lightning.qubit', 'lightning.gpu').\n",
    "\n",
    "        shots (int, optional): \n",
    "            Number of times the circuit gets executed (repeated measurements). If None, the circuit is executed with analytic calculations (no shot noise).\n",
    "        \n",
    "        diff_method (str): \n",
    "            Differentiation method used for training the model. Common options are 'best', 'parameter-shift', 'backprop', etc.\n",
    "\n",
    "        entanglement (bool):\n",
    "            If True, entanglement between qubits is implemented. The entanglement pattern and gate are defined by `entanglement_pattern` and `entanglement_gate`, respectively.\n",
    "        \n",
    "        entanglement_pattern (str): \n",
    "            Entanglement pattern used in the circuit, such as 'chain', 'ring', 'all_to_all', etc., as defined by qml.broadcast patterns.\n",
    "        \n",
    "        entanglement_gate (function): \n",
    "            Quantum gate used for entanglement, such as qml.CZ or qml.CNOT. This gate is applied between qubits according to the specified entanglement pattern.\n",
    "        \n",
    "        input_scaling (bool): \n",
    "            If True, input parameters are scaled by additional learnable parameters (input_params). The input is multiplied by these parameters before being applied to the qubits.\n",
    "        \n",
    "        input_init (function): \n",
    "            Function to initialize the input scaling parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, or any function defined by the user.\n",
    "        \n",
    "        weight_init (function): \n",
    "            Function to initialize the weights of the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.normal_, or any function defined by the user.\n",
    "        \n",
    "        measure (function): \n",
    "            Measurement function that takes the number of qubits as an argument and returns the measurement result. Common choices are `measure_probs`, `two_measure_expval`, or any user-defined measurement function.\n",
    "\n",
    "        Methods:\n",
    "        --------\n",
    "        generate_circuit(self): \n",
    "            Generates and initializes the quantum circuit based on the parameters.\n",
    "        \n",
    "        forward(self, inputs): \n",
    "            Takes inputs and passes them through the quantum circuit to get the output.\n",
    "\n",
    "        visualize_circuit(self): \n",
    "            Visualizes the generated quantum circuit for the given number of qubits using the initial parameters. Useful for debugging or analyzing the circuit design.\n",
    "\n",
    "        handle_errors_and_warnings(self): \n",
    "            Handles common errors and warnings, such as invalid parameter values, unsupported devices, and incompatible differentiation methods.\n",
    "            \n",
    "        \"\"\"\n",
    "        return info_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFQ Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfqTutorial(nn.Module):\n",
    "    '''\n",
    "    Creates a parameterized quantum circuit based on the TensorFlow Quantum tutorial in https://www.tensorflow.org/quantum/tutorials/quantum_reinforcement_learning\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                n_qubits,\n",
    "                n_layers, \n",
    "                device,\n",
    "                shots, \n",
    "                diff_method, \n",
    "                entanglement,\n",
    "                entanglement_pattern, \n",
    "                entanglement_gate, \n",
    "                input_scaling, \n",
    "                input_init, \n",
    "                weight_init, \n",
    "                measure):\n",
    "        super(TfqTutorial, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_scaling = input_scaling\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        # Call the error handling function\n",
    "        self.handle_errors_and_warnings()\n",
    "\n",
    "        # Initialize the device\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(self.device, wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(self.device, wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        # Weight initialization\n",
    "        self.weight_shapes = {\n",
    "            \"input_params\": (self.n_layers, self.n_qubits, 1),\n",
    "            \"params\": (self.n_layers + 1, self.n_qubits, 3)\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"input_params\": self.input_init,\n",
    "            \"params\": self.weight_init,\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, params, input_params):\n",
    "            \n",
    "            # Apply layers and entanglement\n",
    "            for layer in range(self.n_layers):\n",
    "                for wire in range(self.n_qubits):\n",
    "                    qml.RX(params[layer][wire][0], wires=wire)\n",
    "                    qml.RY(params[layer][wire][1], wires=wire)\n",
    "                    qml.RZ(params[layer][wire][2], wires=wire)\n",
    "\n",
    "                if self.entanglement:\n",
    "                    qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "                # Input scaling\n",
    "                if self.input_scaling:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RX(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                else:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RX(inputs[wire], wires=wire)\n",
    "                \n",
    "            # Final layer\n",
    "            for wire in range(self.n_qubits):\n",
    "                qml.RX(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "                qml.RZ(params[-1][wire][2], wires=wire)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)  \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        Gives inputs to the circuit and outputs the respective output\n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        '''\n",
    "        Draws the circuit\n",
    "        '''\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        qml.draw_mpl(self.qnode)(inputs, \n",
    "                                initialized_params[\"params\"], \n",
    "                                initialized_params[\"input_params\"])\n",
    "\n",
    "    def handle_errors_and_warnings(self):\n",
    "        ''' \n",
    "        Handles the errors and warnings\n",
    "        '''\n",
    "        # Check if the number of layers is valid\n",
    "        if self.n_layers < 1:\n",
    "            raise ValueError(\"Number of layers must be at least 1.\")\n",
    "\n",
    "    def get_parameters(self):\n",
    "        # Extract relevant attributes for JSON serialization\n",
    "        return {\n",
    "            \"Number of Qubits\": self.n_qubits,\n",
    "            \"Number of Layers\": self.n_layers,\n",
    "            \"Device\": str(self.device),  # Convert to string representation\n",
    "            \"Shots\": self.shots,\n",
    "            \"Differentiation Method\": self.diff_method,\n",
    "            \"Entanglement\": self.entanglement,\n",
    "            \"Entanglement Pattern\": self.entanglement_pattern,\n",
    "            \"Entanglement Gate\": get_function_representation(self.entanglement_gate),  # Use the helper function to represent the gate\n",
    "            \"Input Scaling\": self.input_scaling,\n",
    "            \"Input Initialization\": get_function_representation(self.input_init),  # Use the helper function for the initializer\n",
    "            \"Parameters Initialization\": get_function_representation(self.weight_init),  # Use the helper function for the initializer\n",
    "            \"Measurement Function\": get_function_representation(self.measure)  # Use the helper function for the measurement function\n",
    "        }        \n",
    "    \n",
    "    @classmethod\n",
    "    def info(cls):\n",
    "        '''\n",
    "        Provides a summary of the TFQ class, including its parameters and methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        TFQ circuit: A Parameterized Quantum Circuit based on the design in the TensorFlow Quantum tutorial in https://www.tensorflow.org/quantum/tutorials/quantum_reinforcement_learning\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_qubits (int): \n",
    "            Number of qubits used in the quantum circuit.\n",
    "        \n",
    "        n_layers (int): \n",
    "            Number of layers in the quantum circuit. Each layer typically consists of parameterized rotations followed by entanglement gates.\n",
    "        \n",
    "        device (str): \n",
    "            The quantum device used for simulation or execution (e.g., 'default_qubit', 'lightning.qubit').\n",
    "\n",
    "        shots (int, optional): \n",
    "            Number of times the circuit gets executed (repeated measurements). If None, the circuit is executed with analytic calculations (no shot noise).\n",
    "        \n",
    "        diff_method (str): \n",
    "            Differentiation method used for training the model. Common options are 'best', 'parameter-shift', 'backprop', etc.\n",
    "\n",
    "        entanglement (bool):\n",
    "            If True, entanglement between qubits is implemented. The entanglement pattern and gate are defined by `entanglement_pattern` and `entanglement_gate`, respectively.\n",
    "        \n",
    "        entanglement_pattern (str): \n",
    "            Entanglement pattern used in the circuit, such as 'chain', 'ring', 'all_to_all', etc., as defined by qml.broadcast patterns.\n",
    "        \n",
    "        entanglement_gate (function): \n",
    "            Quantum gate used for entanglement, such as qml.CZ or qml.CNOT. This gate is applied between qubits according to the specified entanglement pattern.\n",
    "        \n",
    "        input_scaling (bool): \n",
    "            If True, input parameters are scaled by additional learnable parameters (input_params). The input is multiplied by these parameters before being applied to the qubits.\n",
    "        \n",
    "        input_init (function): \n",
    "            Function to initialize the input scaling parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, or any function defined by the user.\n",
    "        \n",
    "        weight_init (function): \n",
    "            Function to initialize the weights of the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.normal_, or any function defined by the user.\n",
    "        \n",
    "        measure (function): \n",
    "            Measurement function that takes the number of qubits as an argument and returns the measurement result. Common choices are `measure_probs`, `two_measure_expval`, `three_measure_expval`, or any user-defined measurement function.\n",
    "\n",
    "        Methods:\n",
    "        --------\n",
    "        generate_circuit(self): \n",
    "            Generates and initializes the quantum circuit based on the parameters.\n",
    "        \n",
    "        forward(self, inputs): \n",
    "            Takes inputs and passes them through the quantum circuit to get the output.\n",
    "\n",
    "        visualize_circuit(self): \n",
    "            Visualizes the generated quantum circuit for the given number of qubits using the initial parameters. Useful for debugging or analyzing the circuit design.\n",
    "\n",
    "        handle_errors_and_warnings(self): \n",
    "            Handles common errors and warnings, such as invalid parameter values, unsupported devices, and incompatible differentiation methods.\n",
    "            \n",
    "        \"\"\"\n",
    "        return info_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UQC(nn.Module):\n",
    "    '''\n",
    "    Creates a parameterized quantum circuit based on the 'Data re-uploading for a universal quantum classifier' paper by Adrián Pérez-Salinas.\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                n_qubits, \n",
    "                n_layers, \n",
    "                state_dim,\n",
    "                device,\n",
    "                shots, \n",
    "                diff_method,\n",
    "                encoding_type,\n",
    "                entanglement,\n",
    "                entanglement_pattern, \n",
    "                entanglement_gate, \n",
    "                input_init,\n",
    "                weight_init,\n",
    "                bias_init,\n",
    "                measure):\n",
    "        super(UQC, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.state_dim = state_dim\n",
    "        self.device = device\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.encoding_type = encoding_type\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.bias_init = bias_init\n",
    "        self.measure = measure\n",
    "        self.input_scaling = None\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(self.device, wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(self.device, wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        if self.encoding_type == 'full':\n",
    "            self.weight_shapes = {\n",
    "                \"input_params\": (self.n_layers, self.n_qubits, self.state_dim),\n",
    "                \"params\": (self.n_layers, self.n_qubits, 1),\n",
    "                \"bias\": (self.n_layers, self.n_qubits)\n",
    "            }\n",
    "        elif self.encoding_type == 'partial':\n",
    "            self.weight_shapes = {\n",
    "            \"input_params\": (self.n_layers, self.n_qubits, int(self.state_dim/self.n_qubits)),\n",
    "            \"params\": (self.n_layers, self.n_qubits, 1),\n",
    "            \"bias\": (self.n_layers, self.n_qubits)\n",
    "            }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"input_params\": self.input_init,\n",
    "            \"params\": self.weight_init,\n",
    "            \"bias\": self.bias_init\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, input_params, params, bias):\n",
    "\n",
    "            for layer in range(self.n_layers):\n",
    "                for wire in range(self.n_qubits):\n",
    "                    if self.encoding_type == 'full':\n",
    "                        hadamard_product = torch.dot(inputs.clone().detach(), input_params[layer][wire])\n",
    "                        angle = hadamard_product + bias[layer][wire]\n",
    "                    elif self.encoding_type == 'partial':\n",
    "                        separate_inputs = np.array_split(inputs,self.n_qubits)\n",
    "                        hadamard_product = torch.dot(separate_inputs[wire], input_params[layer][wire])\n",
    "                        angle = hadamard_product + bias[layer][wire]\n",
    "\n",
    "                    qml.RZ(angle, wires=wire)\n",
    "                    \n",
    "                    qml.RY(params[layer][wire][0], wires=wire)\n",
    "                    \n",
    "                if self.entanglement:\n",
    "                    qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(self.qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        Gives inputs to the circuit and outputs the respective output\n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.state_dim)], dtype=torch.float32)\n",
    "        \n",
    "        # Initialize all parameters using the provided initialization methods\n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        qml.draw_mpl(self.qnode)(inputs, \n",
    "                                initialized_params[\"weights\"], \n",
    "                                initialized_params[\"params\"], \n",
    "                                initialized_params[\"bias\"])\n",
    "\n",
    "    def handle_errors_and_warnings(self):\n",
    "        ''' \n",
    "        Handles the errors and warnings\n",
    "        '''\n",
    "        # Check if the number of layers is valid\n",
    "        if self.n_layers < 1:\n",
    "            raise ValueError(\"Number of layers must be at least 1.\")\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        # Extract relevant attributes for JSON serialization\n",
    "        return {\n",
    "            \"Number of Qubits\": self.n_qubits,\n",
    "            \"Number of Layers\": self.n_layers,\n",
    "            \"State Dimension\": self.state_dim,\n",
    "            \"Device\": str(self.device),  # Convert to string representation\n",
    "            \"Shots\": self.shots,\n",
    "            \"Differentiation Method\": self.diff_method,\n",
    "            \"Encoding Type\": self.encoding_type,\n",
    "            \"Entanglement\": self.entanglement,\n",
    "            \"Entanglement Pattern\": self.entanglement_pattern,\n",
    "            \"Entanglement Gate\": get_function_representation(self.entanglement_gate),  # Use the helper function to represent the gate\n",
    "            \"Input Initialization\": get_function_representation(self.input_init),  # Use the helper function for the initializer\n",
    "            \"Parameters Initialization\": get_function_representation(self.weight_init),  # Use the helper function for the initializer\n",
    "            \"Bias Initialization\": get_function_representation(self.bias_init),  # Use the helper function for the initializer\n",
    "            \"Measurement Function\": get_function_representation(self.measure)  # Use the helper function for the measurement function\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def info(cls):\n",
    "        '''\n",
    "        Provides a summary of the UQC class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        Creates a parameterized quantum circuit based on the work in 'Data re-uploading for a universal quantum classifier' by Adrián Pérez-Salinas.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_qubits (int): \n",
    "            Number of qubits used in the quantum circuit.\n",
    "\n",
    "        n_layers (int): \n",
    "            Number of layers in the quantum circuit. Each layer consists of parameterized rotations and entanglement gates.\n",
    "\n",
    "        state_dim (int): \n",
    "            Dimensionality of the state space, determining the size of the weights associated with each qubit.\n",
    "\n",
    "        device (str): \n",
    "            The quantum device to be used for execution, such as 'default.qubit', 'lightning.qubit', etc.\n",
    "\n",
    "        shots (int, optional): \n",
    "            Number of times the circuit gets executed (repeated measurements). If None, the circuit is executed with analytic calculations (no shot noise).\n",
    "\n",
    "        diff_method (str): \n",
    "            Differentiation method used for training the model. Common options include 'best', 'parameter-shift', 'adjoint', and 'backprop'.\n",
    "\n",
    "        encoding_type (str): \n",
    "            Type of encoding used for the input data. Can be 'full' for complete encoding or 'partial' for partial encoding, which changes the shape of weights.\n",
    "\n",
    "        entanglement (bool): \n",
    "            If True, entanglement between qubits is implemented. The entanglement pattern and gate are defined in entanglement_pattern and entanglement_gate, respectively.\n",
    "\n",
    "        entanglement_pattern (str): \n",
    "            Entanglement pattern used in the circuit, such as 'chain', 'ring', 'all_to_all', etc., as defined by qml.broadcast patterns.\n",
    "\n",
    "        entanglement_gate (function): \n",
    "            Quantum gate used for entanglement, such as qml.CZ or qml.CNOT. This gate is applied between qubits according to the specified entanglement pattern.\n",
    "\n",
    "        input_init (function): \n",
    "            Function to initialize the weights of the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.normal_, or a user-defined function.\n",
    "\n",
    "        weight_init (function): \n",
    "            Function to initialize the parameters of the quantum circuit, similar to input_init.\n",
    "\n",
    "        bias_init (function): \n",
    "            Function to initialize the bias terms in the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.zeros_, or a user-defined function.\n",
    "\n",
    "        measure (function): \n",
    "            Measurement function that takes the number of qubits as an argument and returns the measurement result. Common choices are measure_probs, two_measure_expval, or any user-defined measurement function.\n",
    "        \n",
    "        Methods:\n",
    "        --------\n",
    "        generate_circuit(self): \n",
    "            Generates and initializes the quantum circuit based on the parameters.\n",
    "        \n",
    "        forward(self, inputs): \n",
    "            Takes inputs and passes them through the quantum circuit to get the output.\n",
    "\n",
    "        visualize_circuit(self): \n",
    "            Visualizes the generated quantum circuit for the given number of qubits using the initial parameters. Useful for debugging or analyzing the circuit design.\n",
    "\n",
    "        handle_errors_and_warnings(self): \n",
    "            Handles common errors and warnings, such as invalid parameter values, unsupported devices, and incompatible differentiation methods.\n",
    "        \"\"\"\n",
    "        return info_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyPostProcessing(nn.Module):\n",
    "    '''\n",
    "    Processes the output of the circuit into one of the implemented policies (Born Contiguous, Born Parity, Softmax)\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 n_qubits,\n",
    "                 n_actions,\n",
    "                 policy_type, \n",
    "                 beta_scheduling, \n",
    "                 beta,\n",
    "                 increase_rate, \n",
    "                 output_scaling,\n",
    "                 output_init):\n",
    "        super(PolicyPostProcessing, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_actions = n_actions\n",
    "        self.policy_type = policy_type\n",
    "        self.beta_scheduling = beta_scheduling\n",
    "        self.beta = beta\n",
    "        self.increase_rate = increase_rate\n",
    "        self.output_scaling = output_scaling\n",
    "        self.output_init = output_init\n",
    "\n",
    "        if self.output_scaling == True:\n",
    "            self.output_params = nn.parameter.Parameter(torch.Tensor(self.n_actions), requires_grad=True)\n",
    "            self.output_init(self.output_params)\n",
    "        else:\n",
    "            self.register_parameter('w_input', None)\n",
    "\n",
    "    def forward(self,probs):\n",
    "        if self.policy_type == 'raw_contiguous':\n",
    "            policy = self.raw_contiguous(probs)\n",
    "        elif self.policy_type == 'raw_parity':\n",
    "            policy = self.raw_parity(probs)\n",
    "        elif self.policy_type == 'softmax':\n",
    "            policy = self.softmax(probs)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid post-processing method specified.\")\n",
    "        return policy\n",
    "\n",
    "    def raw_contiguous(self,probs):\n",
    "\n",
    "        log_n_actions = int(np.log2(self.n_actions))\n",
    "        \n",
    "        # Ensure the number of actions does not exceed the number of basis states (determined by n_qubits)\n",
    "        if log_n_actions > self.n_qubits:\n",
    "            raise ValueError('Number of actions exceeds the number of basis states!')\n",
    "\n",
    "        # Split the probabilities in a contiguous manner\n",
    "        probs_split = torch.chunk(probs, self.n_actions)\n",
    "        policy = [torch.sum(prob) for prob in probs_split]\n",
    "        return(torch.stack(policy))\n",
    "\n",
    "    def raw_parity(self,probs):\n",
    "\n",
    "        log_n_actions = int(np.log2(self.n_actions))\n",
    "\n",
    "        # Check if the number of actions is a power of 2\n",
    "        if log_n_actions < 1.0 or not (np.floor(log_n_actions) == np.ceil(log_n_actions)):\n",
    "            raise NotImplementedError('Number of actions needs to be a power of two!')\n",
    "\n",
    "        # Ensure the number of actions does not exceed the number of qubits\n",
    "        if log_n_actions > self.n_qubits:\n",
    "            raise ValueError('Number of actions exceeds number of basis states!')\n",
    "\n",
    "        # Flatten the probability distribution to handle it as a single-dimensional array\n",
    "        if log_n_actions == 1:\n",
    "            summed_tensors = []\n",
    "            even_tensor = probs[::2]  # Elements at even indices\n",
    "            odd_tensor = probs[1::2]  # Elements at odd indices\n",
    "            summed_tensors.append(torch.sum(even_tensor))\n",
    "            summed_tensors.append(torch.sum(odd_tensor))\n",
    "        else:\n",
    "            probs_split = list(torch.chunk(probs, self.n_actions//2))\n",
    "            summed_tensors = []\n",
    "\n",
    "            for tensor in probs_split:\n",
    "                even_tensor = tensor[::2]  # Even indexed elements\n",
    "                odd_tensor = tensor[1::2]  # Odd indexed elements\n",
    "                summed_tensors.append(torch.sum(even_tensor))\n",
    "                summed_tensors.append(torch.sum(odd_tensor))\n",
    "\n",
    "        return torch.stack(summed_tensors)\n",
    "    \n",
    "    def softmax(self, probs):\n",
    "        \n",
    "        if self.output_scaling == True:\n",
    "            probs *= self.output_params\n",
    "\n",
    "        scaled_output = probs * self.beta\n",
    "        softmax_output = F.softmax(scaled_output, dim=0)\n",
    "        return softmax_output\n",
    "    \n",
    "    def beta_schedule(self):\n",
    "        if self.beta_scheduling == True and self.policy_type == 'softmax':\n",
    "            self.beta += self.increase_rate\n",
    "\n",
    "    def get_parameters(self):\n",
    "        # Extract relevant attributes for JSON serialization\n",
    "        return {\n",
    "            \"Policy Type\": self.policy_type,\n",
    "            \"Beta Scheduling\": self.beta_scheduling,\n",
    "            \"Beta\": self.beta,\n",
    "            \"Increase Rate\": self.increase_rate,\n",
    "            \"Output Scaling\": self.output_scaling,\n",
    "            \"Output Initialization\": get_function_representation(self.output_init),\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def info(cls):\n",
    "        '''\n",
    "        Provides a summary of the PolicyType class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        PolicyType Class:\n",
    "\n",
    "        Responsible for processing the outputs of the variational quantum circuit.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_actions (int): \n",
    "            Number of actions available for the agent to choose from.\n",
    "        \n",
    "        policy_type (str): \n",
    "            Type of policy applied to the probability distribution:\n",
    "            - 'raw_contiguous': Applies the Born Contiguous-like policy.\n",
    "            - 'raw_parity': Applies the Born Parity-like policy.\n",
    "            - 'softmax': Applies the softmax policy to the expectation values.\n",
    "\n",
    "        beta_scheduling (bool): \n",
    "            If True, updates the inverse temperature parameter (beta) after each episode. Used only for the softmax policy.\n",
    "        \n",
    "        beta (float): \n",
    "            Inverse temperature parameter used for scaling probabilities in the softmax policy.\n",
    "        \n",
    "        increase_rate (float): \n",
    "            Amount added to beta at the end of each episode, if beta_scheduling is True.\n",
    "        \n",
    "        output_scaling (bool): \n",
    "            If True, scales the output probabilities by learnable parameters.\n",
    "        \n",
    "        output_init (function): \n",
    "            Initialization function for output parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, etc.\n",
    "        \n",
    "        Methods:\n",
    "        -------\n",
    "        forward(self, probs):\n",
    "            Selects an action based on the chosen post_processing method.\n",
    "        \n",
    "        raw_contiguous(self, probs):\n",
    "            Sums up contiguous chunks of probabilities and returns the probability of each action.\n",
    "        \n",
    "        raw_parity(self, probs):\n",
    "            Sums up probabilities based on parity and returns the probability of each action.\n",
    "        \n",
    "        softmax(self, probs):\n",
    "            Applies a softmax function to the scaled probabilities and returns the probability of each action.\n",
    "        \n",
    "        beta_schedule(self):\n",
    "            Updates the beta parameter if beta_scheduling is True. Only applicable for the softmax method.\n",
    "        \"\"\"\n",
    "        return info_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Policy (Circuit + Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumPolicy(nn.Module):\n",
    "    \n",
    "    def __init__(self, circuit, post_processing):\n",
    "        super(QuantumPolicy, self).__init__()\n",
    "        self.circuit = circuit\n",
    "        self.post_processing = post_processing\n",
    "\n",
    "    def sample(self, inputs):\n",
    "        '''\n",
    "        Samples an action from the action probability distribution\n",
    "        '''\n",
    "        policy = self.forward(inputs)\n",
    "        dist = torch.distributions.Categorical(policy)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), policy\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Input state is fed to the circuit - its output is then fed to the post processing \n",
    "        '''\n",
    "        probs = self.circuit.forward(inputs)\n",
    "        probs_processed = self.post_processing.forward(probs)\n",
    "        return probs_processed\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        '''\n",
    "        Returns the values of each set of parameters\n",
    "        '''\n",
    "        parameters = []\n",
    "\n",
    "        circuit_parameters = [param.clone().detach().numpy().flatten() for param in self.circuit.parameters()]\n",
    "        parameters.extend(circuit_parameters)\n",
    "\n",
    "        policy_parameters = [param.clone().detach().numpy().flatten() for param in self.post_processing.parameters()]\n",
    "        parameters.extend(policy_parameters)\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    def get_gradients(self):\n",
    "        '''\n",
    "        Returns the gradient values of each set of parameters\n",
    "        '''\n",
    "        gradients = []\n",
    "\n",
    "        # Get gradients from circuit parameters\n",
    "        circuit_gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for param in self.circuit.parameters()]\n",
    "        gradients.extend(circuit_gradients)\n",
    "\n",
    "        # Get gradients from policy parameters\n",
    "        policy_gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for param in self.post_processing.parameters()]\n",
    "        gradients.extend(policy_gradients)\n",
    "\n",
    "        return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceAgent:\n",
    "\n",
    "    def __init__(self, \n",
    "                policy, \n",
    "                policy_optimizer, \n",
    "                env_name, \n",
    "                n_episodes, \n",
    "                max_t, \n",
    "                gamma, \n",
    "                baseline, \n",
    "                batch_size, \n",
    "                normalize,\n",
    "                print_every, \n",
    "                verbose):\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy = policy.to(self.device)\n",
    "        self.policy_optimizer = policy_optimizer\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env_name = env_name\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.baseline = baseline\n",
    "        self.batch_size = batch_size\n",
    "        self.normalize = normalize\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.solved = False\n",
    "        self.scores = deque(maxlen=100)\n",
    "        self.batch_log_probs = []\n",
    "        self.batch_rewards = []\n",
    "        self.loss = torch.tensor(0.0)\n",
    "\n",
    "    def train(self, run_count=None, rundate = None, path=None, tensorboard=False):\n",
    "\n",
    "        # Creates data saving files if specified\n",
    "        if run_count is not None and path is not None:\n",
    "            data_path = create_directory(os.path.join(path, 'data'))\n",
    "            env_folder = create_directory(os.path.join(data_path, self.env_name))\n",
    "            experiment_folder_name = f\"{self.policy.circuit.__class__.__name__}_{self.policy.circuit.n_qubits}qubits_{self.policy.circuit.n_layers}layer_{rundate}\"\n",
    "            experiment_path = create_directory(os.path.join(env_folder, experiment_folder_name))\n",
    "            run_path = create_directory(os.path.join(experiment_path, f'run_{str(run_count)}'))\n",
    "            self.save_agent_data(experiment_path)\n",
    "        \n",
    "\n",
    "        # Create TensorBoard session if specified\n",
    "        if tensorboard:\n",
    "            writer = SummaryWriter(log_dir=run_path)\n",
    "\n",
    "        for i in range(1, self.n_episodes + 1):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Get episode\n",
    "            self.get_trajectory()\n",
    "\n",
    "            # Check if environment is solved\n",
    "            self.env_solved_verification()\n",
    "\n",
    "            # Update parameters if the batch is full and environment is not solved\n",
    "            if i % self.batch_size == 0 and self.solved is False:\n",
    "                self.update_policy()\n",
    "                self.policy.post_processing.beta_schedule()\n",
    " \n",
    "            end_time = time.time()\n",
    "\n",
    "            # Calculate the runtime\n",
    "            self.runtime = end_time - start_time\n",
    "\n",
    "            # Write in the TensorBoard session\n",
    "            if tensorboard:\n",
    "                self.writer_function(writer, i)\n",
    "\n",
    "            # Save the episode data\n",
    "            if run_count is not None and path is not None:\n",
    "                self.save_data(run_path, i)\n",
    "\n",
    "            # Print out episode data\n",
    "            if self.verbose == 1:\n",
    "                print('Episode {} reward: {:.2f}\\t Solved: {}'.format(i, self.scores[-1], self.solved))\n",
    "            if self.verbose == 2:\n",
    "                print('Episode {} reward: {:.2f}\\t Solved: {}\\t Runtime: {:.2f}\\t Loss: {:.2f}'.format(i, self.scores[-1], self.solved, self.runtime, self.loss))\n",
    "            if i % self.print_every == 0:\n",
    "                print('Last {} Episodes average reward: {:.2f}\\t'.format(len(self.scores), np.mean(self.scores)))\n",
    "\n",
    "\n",
    "        # Close TensorBoard session\n",
    "        if tensorboard:\n",
    "            writer.close()\n",
    "\n",
    "    def get_trajectory(self):\n",
    "        '''\n",
    "        Gets a trajectory based on the running policy until it runs out of bounds or achieves maximum reward of an episode\n",
    "        '''\n",
    "        # Get an episode trajectory\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()[0]\n",
    "        for t in range(self.max_t):\n",
    "            state_tensor = torch.tensor(self.normalize_state(state)).float().to(self.device)\n",
    "            action, log_prob, _ = self.policy.sample(state_tensor)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            \n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Save the episode reward\n",
    "        self.scores.append(sum(self.rewards))\n",
    "\n",
    "        # Save data from the episode to the batch\n",
    "        self.batch_log_probs.append(self.saved_log_probs)\n",
    "        self.batch_rewards.append(self.rewards)\n",
    "\n",
    "        # Clear data in case the agent already solved the environment\n",
    "        if self.solved is True:\n",
    "            self.batch_log_probs = []\n",
    "            self.batch_rewards = []\n",
    "      \n",
    "    def update_policy(self):\n",
    "        '''\n",
    "        Computes the loss and gradients and updates the policy via gradient methods\n",
    "        '''\n",
    "        # Discounting of the rewards\n",
    "        all_returns = []\n",
    "        for batch in self.batch_rewards:\n",
    "            R = 0\n",
    "            ep_return = []\n",
    "            for r in reversed(batch):\n",
    "                R = r + self.gamma * R\n",
    "                ep_return.insert(0, R)\n",
    "            ep_return = torch.tensor(ep_return).to(self.device)\n",
    "\n",
    "            # Standardization of the discounted returns\n",
    "            ep_return = (ep_return - ep_return.mean()) / (ep_return.std() + 1e-8)\n",
    "\n",
    "            all_returns.append(ep_return)\n",
    "\n",
    "        # Calculate the policy loss\n",
    "        policy_loss = []     \n",
    "        if self.baseline:\n",
    "            baseline = np.mean([sum(lst) for lst in all_returns])\n",
    "            for log_probs, ep_returns in zip(self.batch_log_probs, all_returns):\n",
    "                for log_prob, ret in zip(log_probs, ep_returns):\n",
    "                    advantage = ret - baseline \n",
    "                    policy_loss.append(-log_prob * advantage)\n",
    "        else:\n",
    "            for log_probs, ep_returns in zip(self.batch_log_probs, all_returns):\n",
    "                for log_prob, ret in zip(log_probs, ep_returns):\n",
    "                    policy_loss.append(-log_prob * ret)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        self.loss = torch.cat(policy_unsqueezed).mean()\n",
    "\n",
    "        # Compute the gradients \n",
    "        self.policy_optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "        # Clear old data\n",
    "        del all_returns\n",
    "        del policy_loss\n",
    "        del policy_unsqueezed \n",
    "        self.batch_log_probs = []\n",
    "        self.batch_rewards = []\n",
    "\n",
    "    def normalize_state(self, state):\n",
    "        '''\n",
    "        Processes the input state by reducing its dimensionality and normalizing it\n",
    "        '''\n",
    "        # State-space reduction for the Acrobot\n",
    "        if self.env_name in ('Acrobot-v0', 'Acrobot-v1'):\n",
    "            theta1 = np.arccos(state[0])\n",
    "            theta2 = np.arccos(state[2])\n",
    "            state = [theta1,theta2,state[4],state[5]]\n",
    "\n",
    "\n",
    "        # Normalize each feature by the maximum absolute value at each step\n",
    "        if self.normalize == True:\n",
    "            max_abs_value = max(abs(value) for value in state)\n",
    "            state = np.array([value / max_abs_value for value in state])\n",
    "        \n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def env_solved_verification(self):\n",
    "        '''\n",
    "        Checks if the environment is solved\n",
    "        '''\n",
    "        # Acrobot-v1\n",
    "        if self.env_name in ('Acrobot-v1'):\n",
    "            if np.mean(self.scores) > -125:\n",
    "                self.solved = True\n",
    "        \n",
    "        # CartPole-v0 and CartPole-v1\n",
    "        elif self.env_name in ('CartPole-v0','CartPole-v1'):\n",
    "            if np.mean(self.scores) > self.env.spec.reward_threshold:\n",
    "                self.solved = True\n",
    "        \n",
    "        else:              \n",
    "            warnings.warn(f\"No reward threshold defined for environment {self.env_name}. \"\n",
    "                          \"Consider specifying a solved condition explicitly.\",\n",
    "                          UserWarning\n",
    "            )\n",
    "\n",
    "    def save_agent_data(self, main_path):\n",
    "        '''\n",
    "        Stores the most relevant model parameters into a .json file.\n",
    "        '''\n",
    "        # Use the get_parameters method to get Circuit Parameters and Policy Parameters\n",
    "        circuit_params = self.policy.circuit.get_parameters()  # Get circuit parameters\n",
    "        policy_params = self.policy.post_processing.get_parameters()  # Get policy parameters\n",
    "        \n",
    "        # Use the get_parameters method to get selected Agent Parameters\n",
    "        agent_params = self.get_parameters()  # Call the get_parameters method for agent parameters\n",
    "        \n",
    "        # Create a structured dictionary\n",
    "        agent_variables = {\n",
    "            \"Circuit Parameters\": circuit_params,\n",
    "            \"Policy Parameters\": policy_params,\n",
    "            \"Agent Parameters\": agent_params\n",
    "        }\n",
    "\n",
    "        # Convert sets to lists\n",
    "        def convert_sets_to_lists(obj):\n",
    "            if isinstance(obj, set):\n",
    "                return list(obj)\n",
    "            elif isinstance(obj, dict):\n",
    "                return {key: convert_sets_to_lists(value) for key, value in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert_sets_to_lists(item) for item in obj]\n",
    "            else:\n",
    "                return obj\n",
    "\n",
    "        # Convert sets in agent_variables\n",
    "        agent_variables = convert_sets_to_lists(agent_variables)\n",
    "\n",
    "        # Save as JSON\n",
    "        with open(os.path.join(main_path, \"agent_characteristics.json\"), \"w\") as f:\n",
    "            json.dump(agent_variables, f, indent=4)\n",
    "\n",
    "    def save_data(self, run_path, iteration):\n",
    "        '''\n",
    "        Saves the data into a .npz file for each episode\n",
    "        '''\n",
    "        data_file = os.path.join(run_path, \"run_data.npz\")\n",
    "        \n",
    "        # Load existing data if the file exists\n",
    "        if os.path.exists(data_file):\n",
    "            data = np.load(data_file, allow_pickle=True)\n",
    "            old_episode_reward = data['episode_reward'].tolist()\n",
    "            old_loss = data['loss'].tolist()\n",
    "            old_runtime = data['runtime'].tolist()\n",
    "            old_params_gradients = data['params_gradients'].tolist()\n",
    "        else:\n",
    "            old_episode_reward = []\n",
    "            old_loss = []\n",
    "            old_runtime = []\n",
    "            old_params_gradients = []\n",
    "\n",
    "        # Add episode reward and runtime\n",
    "        old_episode_reward.append(self.scores[-1])\n",
    "        old_runtime.append(self.runtime)\n",
    "\n",
    "\n",
    "        # Stores the loss and parameter gradients when batch is full\n",
    "        current_episode_gradients = []\n",
    "        if iteration % self.batch_size == 0 and iteration != 0 and self.solved is False:\n",
    "            old_loss.append(self.loss.item())\n",
    "            for name, param in self.policy.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    grad_array = param.grad.cpu().numpy().flatten()\n",
    "                    current_episode_gradients.append(grad_array)\n",
    "            old_params_gradients.append(current_episode_gradients)\n",
    "            \n",
    "        # Save data to .npz file\n",
    "        np.savez_compressed(data_file,\n",
    "                            episode_reward=np.array(old_episode_reward),\n",
    "                            loss=np.array(old_loss),\n",
    "                            runtime=np.array(old_runtime),\n",
    "                            params_gradients=np.array(old_params_gradients, dtype=object))  # Use dtype=object to handle lists\n",
    "\n",
    "        # Clear old data lists to free up memory\n",
    "        del old_episode_reward[:]\n",
    "        del old_loss[:]\n",
    "        del old_runtime[:]\n",
    "        del old_params_gradients[:]\n",
    "\n",
    "    def writer_function(self, writer, iteration):\n",
    "        '''\n",
    "        Stores data into a tensorboard session\n",
    "        '''\n",
    "        writer.add_scalar(\"Episode Reward\", self.scores[-1], global_step=iteration)\n",
    "        writer.add_scalar(\"Runtime\", self.runtime, global_step=iteration)\n",
    "        writer.add_scalar(\"Loss\", self.loss.item(), global_step=iteration)\n",
    "         # Initialize a list to store gradients\n",
    "        gradients = []\n",
    "\n",
    "        # Retrieve and collect gradients for 'input_params' and 'params'\n",
    "        for name, param in self.policy.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if name == 'input_params' or name == 'params':\n",
    "                    gradients.append(param.grad.view(-1))  # Flatten and add to list\n",
    "\n",
    "        # Concatenate all collected gradients into a single tensor\n",
    "        if gradients:\n",
    "            combined_gradients = torch.cat(gradients)\n",
    "            combined_grad_norm = torch.norm(combined_gradients).item()  # Calculate L2 norm of the combined gradients\n",
    "            \n",
    "            # Log the combined gradient norm\n",
    "            writer.add_scalar(\"Gradient Norm/Combined\", combined_grad_norm, global_step=iteration)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        # Extract specified attributes for JSON serialization\n",
    "        return {\n",
    "            \"Environment\": self.env_name,\n",
    "            \"Gamma (discounting factor)\": self.gamma,\n",
    "            \"Baseline\": self.baseline,\n",
    "            \"Batch Size\": self.batch_size,\n",
    "            \"Normalize\": self.normalize,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jerbi Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 5\n",
    "device = 'lightning.qubit'\n",
    "shots = None\n",
    "diff_method = 'adjoint' \n",
    "entanglement = True\n",
    "entanglement_pattern = 'all_to_all'\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).normal_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval\n",
    "policy_circuit = JerbiModel(n_qubits, \n",
    "                            n_layers, \n",
    "                            device, \n",
    "                            shots, \n",
    "                            diff_method, \n",
    "                            entanglement, \n",
    "                            entanglement_pattern, \n",
    "                            entanglement_gate, \n",
    "                            input_scaling, \n",
    "                            input_init, \n",
    "                            weight_init, \n",
    "                            policy_circuit_measure)\n",
    "\n",
    "policy_circuit.visualize_circuit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Episode 1 reward: 14.00\t Solved: False\n",
      "100\n",
      "Episode 2 reward: 13.00\t Solved: False\n",
      "100\n",
      "Episode 3 reward: 11.00\t Solved: False\n",
      "100\n",
      "Episode 4 reward: 14.00\t Solved: False\n",
      "100\n",
      "Episode 5 reward: 11.00\t Solved: False\n",
      "100\n",
      "Episode 6 reward: 10.00\t Solved: False\n",
      "100\n",
      "Episode 7 reward: 8.00\t Solved: False\n",
      "100\n",
      "Episode 8 reward: 9.00\t Solved: False\n",
      "100\n",
      "Episode 9 reward: 10.00\t Solved: False\n",
      "100\n",
      "Episode 10 reward: 19.00\t Solved: False\n",
      "100\n",
      "Episode 11 reward: 15.00\t Solved: False\n",
      "100\n",
      "Episode 12 reward: 11.00\t Solved: False\n",
      "100\n",
      "Episode 13 reward: 12.00\t Solved: False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 67\u001b[0m\n\u001b[0;32m     55\u001b[0m verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     56\u001b[0m reinforce_update \u001b[38;5;241m=\u001b[39m ReinforceAgent(policy, \n\u001b[0;32m     57\u001b[0m                                   policy_optimizer, \n\u001b[0;32m     58\u001b[0m                                   env_name, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m                                   print_every, \n\u001b[0;32m     66\u001b[0m                                   verbose)\n\u001b[1;32m---> 67\u001b[0m reinforce_update\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[1;32mIn[16], line 56\u001b[0m, in \u001b[0;36mReinforceAgent.train\u001b[1;34m(self, run_count, rundate, path, tensorboard)\u001b[0m\n\u001b[0;32m     53\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Get episode\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_trajectory()\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Check if environment is solved\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_solved_verification()\n",
      "Cell \u001b[1;32mIn[16], line 103\u001b[0m, in \u001b[0;36mReinforceAgent.get_trajectory\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_t):\n\u001b[0;32m    102\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_state(state))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 103\u001b[0m     action, log_prob, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39msample(state_tensor)\n\u001b[0;32m    104\u001b[0m     state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaved_log_probs\u001b[38;5;241m.\u001b[39mappend(log_prob)\n",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m, in \u001b[0;36mQuantumPolicy.sample\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    Samples an action from the action probability distribution\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(inputs)\n\u001b[0;32m     13\u001b[0m     dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(policy)\n\u001b[0;32m     14\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "Cell \u001b[1;32mIn[8], line 21\u001b[0m, in \u001b[0;36mQuantumPolicy.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    Input state is fed to the circuit - its output is then fed to the post processing \u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcircuit\u001b[38;5;241m.\u001b[39mforward(inputs)\n\u001b[0;32m     22\u001b[0m     probs_processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_processing\u001b[38;5;241m.\u001b[39mforward(probs)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m probs_processed\n",
      "Cell \u001b[1;32mIn[4], line 98\u001b[0m, in \u001b[0;36mJerbiModel.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     95\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m''' \u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    Gives inputs to the circuit and outputs the respective output\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcircuit(inputs)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\qnn\\torch.py:402\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    399\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(inputs, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    401\u001b[0m \u001b[38;5;66;03m# calculate the forward pass as usual\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate_qnode(inputs)\n\u001b[0;32m    404\u001b[0m \u001b[38;5;66;03m# reshape to the correct number of batch dims\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_batch_dim:\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\qnn\\torch.py:423\u001b[0m, in \u001b[0;36mTorchLayer._evaluate_qnode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluates the QNode for a single input datapoint.\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m    tensor: output datapoint\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    419\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_arg: x},\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{arg: weight\u001b[38;5;241m.\u001b[39mto(x) \u001b[38;5;28;01mfor\u001b[39;00m arg, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnode_weights\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m    422\u001b[0m }\n\u001b[1;32m--> 423\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnode(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mtype(x\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\workflow\\qnode.py:1002\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    999\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshots\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_device_shots(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_device)\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;66;03m# construct the tape\u001b[39;00m\n\u001b[1;32m-> 1002\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstruct(args, kwargs)\n\u001b[0;32m   1004\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1005\u001b[0m using_custom_cache \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(cache, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitem__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1007\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cache, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__setitem__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1008\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cache, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__delitem__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1009\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\workflow\\qnode.py:888\u001b[0m, in \u001b[0;36mQNode.construct\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterface \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mget_interface(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m qml\u001b[38;5;241m.\u001b[39mqueuing\u001b[38;5;241m.\u001b[39mAnnotatedQueue() \u001b[38;5;28;01mas\u001b[39;00m q:\n\u001b[1;32m--> 888\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qfunc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape \u001b[38;5;241m=\u001b[39m QuantumScript\u001b[38;5;241m.\u001b[39mfrom_queue(q, shots)\n\u001b[0;32m    892\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtape\u001b[38;5;241m.\u001b[39mget_parameters(trainable_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[4], line 75\u001b[0m, in \u001b[0;36mJerbiModel.generate_circuit.<locals>.qnode\u001b[1;34m(inputs, params, input_params)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_scaling \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m wire \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_qubits):\n\u001b[1;32m---> 75\u001b[0m         qml\u001b[38;5;241m.\u001b[39mRY(input_params[layer][wire][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m inputs[wire], wires\u001b[38;5;241m=\u001b[39mwire)\n\u001b[0;32m     76\u001b[0m         qml\u001b[38;5;241m.\u001b[39mRZ(input_params[layer][wire][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m inputs[wire], wires\u001b[38;5;241m=\u001b[39mwire)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\ops\\qubit\\parametric_ops_single_qubit.py:171\u001b[0m, in \u001b[0;36mRY.__init__\u001b[1;34m(self, phi, wires, id)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerator\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m PauliY(wires\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwires)\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, phi, wires, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(phi, wires\u001b[38;5;241m=\u001b[39mwires, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_matrix\u001b[39m(theta):  \u001b[38;5;66;03m# pylint: disable=arguments-differ\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 5\n",
    "device = 'lightning.qubit'\n",
    "shots = None\n",
    "diff_method = 'adjoint' \n",
    "entanglement = True\n",
    "entanglement_pattern = 'all_to_all'\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).normal_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval\n",
    "policy_circuit = JerbiModel(n_qubits, \n",
    "                            n_layers, \n",
    "                            device, \n",
    "                            shots, \n",
    "                            diff_method, \n",
    "                            entanglement, \n",
    "                            entanglement_pattern, \n",
    "                            entanglement_gate, \n",
    "                            input_scaling, \n",
    "                            input_init, \n",
    "                            weight_init, \n",
    "                            policy_circuit_measure)\n",
    "\n",
    "\n",
    "n_actions = 2\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = False\n",
    "beta = 1\n",
    "increase_rate = 0.0005\n",
    "output_scaling = True\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_post_process = PolicyPostProcessing(n_qubits, n_actions, \n",
    "                         post_processing, \n",
    "                         beta_scheduling, \n",
    "                         beta, increase_rate, \n",
    "                         output_scaling, \n",
    "                         output_init)\n",
    "\n",
    "policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "policy_lr_list= [0.1, 0.01, 0.1]  # [weights, input_weights, output_weights]\n",
    "policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "n_episodes = 1000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "baseline = True\n",
    "batch_size = 10\n",
    "normalize = True\n",
    "print_every = 100\n",
    "verbose = 1\n",
    "reinforce_update = ReinforceAgent(policy, \n",
    "                                  policy_optimizer, \n",
    "                                  env_name, \n",
    "                                  n_episodes, \n",
    "                                  max_t, \n",
    "                                  gamma, \n",
    "                                  baseline, \n",
    "                                  batch_size,\n",
    "                                  normalize,\n",
    "                                  print_every, \n",
    "                                  verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 5\n",
    "shots = None\n",
    "diff_method = 'adjoint' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).normal_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval\n",
    "policy_circuit = TfqTutorial(n_qubits,\n",
    "                            n_layers, \n",
    "                            shots, \n",
    "                            diff_method, \n",
    "                            entanglement, \n",
    "                            entanglement_pattern, \n",
    "                            entanglement_gate, \n",
    "                            input_scaling, \n",
    "                            input_init, \n",
    "                            weight_init, \n",
    "                            policy_circuit_measure)\n",
    "\n",
    "policy_circuit.visualize_circuit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 5\n",
    "shots = None\n",
    "diff_method = 'adjoint' \n",
    "entanglement = True\n",
    "entanglement_pattern = 'all_to_all'\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval\n",
    "policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "\n",
    "n_actions = 2\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = False\n",
    "beta = 1\n",
    "increase_rate = 0.0005\n",
    "output_scaling = True\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                         post_processing, \n",
    "                         beta_scheduling, \n",
    "                         beta, increase_rate, \n",
    "                         output_scaling, \n",
    "                         output_init)\n",
    "\n",
    "policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "n_episodes = 1000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 100\n",
    "verbose = 1\n",
    "baseline = True\n",
    "batch_size = 10\n",
    "reinforce_update = ReinforceAgent(policy, \n",
    "                                  policy_optimizer, \n",
    "                                  env_name, \n",
    "                                  n_episodes, \n",
    "                                  max_t, \n",
    "                                  gamma, \n",
    "                                  baseline, \n",
    "                                  batch_size, \n",
    "                                  print_every, \n",
    "                                  verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UQC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 5\n",
    "n_layers = 5\n",
    "state_dim = 4\n",
    "device = 'lightning.qubit'\n",
    "shots = None\n",
    "diff_method = 'adjoint' \n",
    "encoding = 'full'\n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "bias_init = torch.nn.init.zeros_\n",
    "policy_circuit_measure = two_measure_expval\n",
    "policy_circuit = UQC(n_qubits,\n",
    "                    n_layers, \n",
    "                    state_dim,\n",
    "                    device,\n",
    "                    shots, \n",
    "                    diff_method,\n",
    "                    encoding,\n",
    "                    entanglement, \n",
    "                    entanglement_pattern, \n",
    "                    entanglement_gate,\n",
    "                    weight_init,\n",
    "                    weight_init,\n",
    "                    bias_init, \n",
    "                    policy_circuit_measure)\n",
    "\n",
    "policy_circuit.visualize_circuit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 5\n",
    "state_dim = 4\n",
    "device = 'lightning.qubit'\n",
    "shots = None\n",
    "diff_method = 'adjoint' \n",
    "encoding = 'full'\n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "bias_init = torch.nn.init.zeros_\n",
    "policy_circuit_measure = two_measure_expval\n",
    "policy_circuit = UQC(n_qubits,\n",
    "                    n_layers, \n",
    "                    state_dim,\n",
    "                    device,\n",
    "                    shots, \n",
    "                    diff_method,\n",
    "                    encoding,\n",
    "                    entanglement, \n",
    "                    entanglement_pattern, \n",
    "                    entanglement_gate,\n",
    "                    weight_init,\n",
    "                    weight_init,\n",
    "                    bias_init, \n",
    "                    policy_circuit_measure)\n",
    "\n",
    "n_actions = 2\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = False\n",
    "beta = 1\n",
    "increase_rate = 0.0005\n",
    "output_scaling = True\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_post_process = PolicyPostProcessing( n_qubits,\n",
    "                                            n_actions, \n",
    "                                            post_processing, \n",
    "                                            beta_scheduling, \n",
    "                                            beta, increase_rate, \n",
    "                                            output_scaling, \n",
    "                                            output_init)\n",
    "\n",
    "policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [input_weights, weights, bias, output_weights]\n",
    "policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "n_episodes = 1000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 100\n",
    "verbose = 1\n",
    "baseline = True\n",
    "batch_size = 10\n",
    "normalize = True\n",
    "reinforce_update = ReinforceAgent(policy, \n",
    "                                  policy_optimizer, \n",
    "                                  env_name, \n",
    "                                  n_episodes, \n",
    "                                  max_t, \n",
    "                                  gamma, \n",
    "                                  baseline, \n",
    "                                  batch_size, \n",
    "                                  normalize,\n",
    "                                  print_every, \n",
    "                                  verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jerbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    device = 'lightning.qubit'\n",
    "    shots = None\n",
    "    diff_method = 'adjoint' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = JerbiModel(n_qubits, n_layers,device, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_qubits, n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.1, 0.01, 0.1]  # [input_weights, weight, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = True\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 4\n",
    "\n",
    "    for run_index in range(2):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 5\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    device = 'default.qubit'\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    encoding = 'full'\n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    bias_init = torch.nn.init.zeros_\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = UQC(n_qubits,\n",
    "                        n_layers, \n",
    "                        state_dim,\n",
    "                        device,\n",
    "                        shots, \n",
    "                        diff_method,\n",
    "                        encoding,\n",
    "                        entanglement, \n",
    "                        entanglement_pattern, \n",
    "                        entanglement_gate,\n",
    "                        input_init,\n",
    "                        weight_init,\n",
    "                        bias_init, \n",
    "                        policy_circuit_measure)\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_contiguous'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing( n_qubits,\n",
    "                                                n_actions, \n",
    "                                                post_processing, \n",
    "                                                beta_scheduling, \n",
    "                                                beta, increase_rate, \n",
    "                                                output_scaling, \n",
    "                                                output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer = create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = False\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 3   \n",
    "\n",
    "    for run_index in range(2):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_agents(file_name, rundate):\n",
    "#   Path settings\n",
    "    current_dir = os.getcwd()\n",
    "    two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "\n",
    "#   VQC settings\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    device = 'default.qubit'\n",
    "    shots = None\n",
    "    diff_method = 'backprop' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_init = partial(torch.nn.init.normal_, mean=0.0, std=0.01)\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    bias_init = torch.nn.init.zeros_\n",
    "    policy_circuit_measure = measure_probs\n",
    "    policy_circuit = UQC_FullEnc(n_qubits,\n",
    "                                n_layers, \n",
    "                                state_dim, \n",
    "                                device,\n",
    "                                shots, \n",
    "                                diff_method, \n",
    "                                entanglement, \n",
    "                                entanglement_pattern, \n",
    "                                entanglement_gate, \n",
    "                                input_init,\n",
    "                                weight_init,\n",
    "                                bias_init,\n",
    "                                policy_circuit_measure)\n",
    "    \n",
    "#   Post processing settings\n",
    "    n_actions = 2\n",
    "    post_processing = 'raw_parity'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing( n_qubits,\n",
    "                                                n_actions, \n",
    "                                                post_processing, \n",
    "                                                beta_scheduling, \n",
    "                                                beta, increase_rate, \n",
    "                                                output_scaling, \n",
    "                                                output_init)\n",
    "#   Circuit + Post processing\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "#   Gradient learning rates\n",
    "    policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer = create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "#   Agent and environment settings\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    normalize = False\n",
    "    reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, normalize, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    all_results = []\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    num_agents = 3  \n",
    "\n",
    "    for run_index in range(2):  \n",
    "        start_agent_index = run_index * num_agents\n",
    "\n",
    "        results = [\n",
    "            train_agents.remote(str(start_agent_index + i), rundate) for i in range(num_agents)]\n",
    "\n",
    "        completed_results = ray.get(results)\n",
    "        all_results.extend(completed_results)\n",
    "        print(f\"Results for run {run_index}: {completed_results}\")\n",
    "\n",
    "    # Shutdown Ray after all tasks are complete\n",
    "    ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
