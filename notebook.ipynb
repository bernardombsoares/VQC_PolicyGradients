{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pennylane as qml \n",
    "import torch\n",
    "\n",
    "def create_circuit(n_qubits,n_layers=None,circ = \"simplified_two_design\",fim=False, shots=None):\n",
    "\n",
    "    dev = qml.device(\"default.qubit.torch\", wires=n_qubits, shots=shots)\n",
    "\n",
    "    def RZRY(params):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        #qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        #qml.AngleEmbedding(params,wires=range(n_qubits))\n",
    "        for q in range(n_qubits):\n",
    "            qml.Hadamard(wires=q)\n",
    "\n",
    "        for w in range(n_layers): \n",
    "            for q in range(n_qubits):\n",
    "                index = w * (2*n_qubits) + q * 2\n",
    "                qml.RZ(params[index],wires=q)\n",
    "                qml.RY(params[index + 1],wires=q)\n",
    "        \n",
    "        qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        return qml.probs(wires=range(n_qubits))\n",
    "\n",
    "    def S2D(init_params,params,measurement_qubits=0,prod_approx=False):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        \n",
    "        #qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        if not prod_approx:\n",
    "            return qml.probs(wires=list(range(measurement_qubits)))\n",
    "        else:\n",
    "            return [qml.probs(i) for i in range(measurement_qubits)]\n",
    "\n",
    "    def SU(params):\n",
    "        qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        \n",
    "        ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "        for i in range(2,n_qubits):\n",
    "            ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "        return qml.expval(ZZ)\n",
    "    \n",
    "    def simmpleRZRY(params,cnots=True):\n",
    "        qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RZ, wires=range(n_qubits), pattern=\"single\", parameters=params[0])\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params[1])\n",
    "        if cnots:\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "            return qml.expval(qml.PauliZ(n_qubits-1))\n",
    "        else:\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "            return qml.expval(ZZ)\n",
    "        \n",
    "    def RY(params,y=True,probs=False,prod=False, entanglement=None):\n",
    "        #qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params)\n",
    "        #qml.broadcast(qml.CZ, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "\n",
    "        if entanglement==\"all_to_all\":\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        if y==True:\n",
    "            #YY = qml.operation.Tensor(qml.PauliY(0), qml.PauliY(1))\n",
    "            YY = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                #YY = qml.operation.Tensor(YY, qml.PauliY(i))\n",
    "                YY.append(qml.PauliZ(i))\n",
    "            \n",
    "            #return [qml.expval(i) for i in YY]\n",
    "            return qml.expval(YY)\n",
    "\n",
    "        elif probs==False:\n",
    "\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            #ZZ = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))        \n",
    "                #ZZ.append(qml.PauliZ(i))        \n",
    "\n",
    "            #return [qml.expval(i) for i in ZZ]\n",
    "            return qml.expval(ZZ)\n",
    "\n",
    "        else:\n",
    "            if prod:\n",
    "                return [qml.probs(i) for i in range(n_qubits)]\n",
    "            else:\n",
    "                return qml.probs(wires=range(n_qubits))\n",
    "            \n",
    "        \n",
    "        \n",
    "    def GHZ(params,measurement_qubits=0):\n",
    "        qml.RY(params,wires=0)\n",
    "        qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "\n",
    "    def random_product_state(params,gate_sequence=None):\n",
    "                \n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(np.pi / 4, wires=i)\n",
    "\n",
    "        for ll in range(len(params)):\n",
    "\n",
    "            for i in range(n_qubits):\n",
    "                gate_sequence[\"{}{}\".format(ll,i)](params[ll][i], wires=i)\n",
    "\n",
    "            #for i in range(n_qubits - 1):\n",
    "                #qml.CZ(wires=[i, i + 1])\n",
    "    def SEL(params, measurement_qubits=0):\n",
    "        qml.StronglyEntanglingLayers(params, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    def RL(params, measurement_qubits=0):\n",
    "        qml.RandomLayers(params, ratio_imprim=0.8 ,imprimitive=qml.CZ, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    if circ == \"rzry\":\n",
    "        qcircuit = RZRY\n",
    "    elif circ == \"simplified_two_design\":\n",
    "        qcircuit = S2D\n",
    "    elif circ == \"special_unitary\":\n",
    "        qcircuit = SU\n",
    "    elif circ == \"simpleRZRY\":\n",
    "        qcircuit = simmpleRZRY\n",
    "    elif circ == \"RY\":\n",
    "        qcircuit = RY\n",
    "    elif circ == \"ghz\":\n",
    "        qcircuit = GHZ\n",
    "    elif circ == \"random_product_state\":\n",
    "        qcircuit = random_product_state\n",
    "    elif circ == \"SEL\":\n",
    "        qcircuit = SEL\n",
    "    elif circ == \"RL\":\n",
    "        qcircuit = RL\n",
    "    if not fim:\n",
    "        circuit = qml.QNode(qcircuit, dev,interface=\"torch\", diff_method=\"backprop\")\n",
    "    else:\n",
    "        circuit = qml.QNode(qcircuit, dev)\n",
    "\n",
    "    return circuit\n",
    "\n",
    "def compute_gradient(log_prob, w):\n",
    "    \"\"\"Compute gradient of the log probability with respect to weights.\n",
    "    \n",
    "    Args:\n",
    "    - log_prob (torch.Tensor): The log probability tensor.\n",
    "    - w (torch.Tensor): The weights tensor, with requires_grad=True.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The gradient of log_prob with respect to w, flattened.\n",
    "    \"\"\"\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()\n",
    "    log_prob.backward(retain_graph=True)\n",
    "    \n",
    "    if w.grad is None:\n",
    "        raise ValueError(\"The gradient for the given log_prob with respect to w is None.\")\n",
    "    \n",
    "    return w.grad.view(-1).detach().numpy()\n",
    "\n",
    "def policy(probs, policy_type=\"contiguous-like\", n_actions=2, n_qubits=1):\n",
    "\n",
    "    if policy_type == \"contiguous-like\":\n",
    "        return probs\n",
    "    elif policy_type == \"parity-like\":\n",
    "        policy = torch.zeros(n_actions)\n",
    "        for i in range(len(probs)):\n",
    "            a=[]\n",
    "            for m in range(int(np.log2(n_actions))):\n",
    "                if m==0:    \n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)\n",
    "                else:\n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)[:-m]\n",
    "                \n",
    "                a.append(bitstring.count(\"1\") % 2)\n",
    "            policy[int(\"\".join(str(x) for x in a),2)] += probs[i]\n",
    "\n",
    "        return policy    \n",
    "    \n",
    "def compute_policy_and_gradient(args):\n",
    "    n_qubits, shapes, type , n_actions, policy_type, clamp = args\n",
    "\n",
    "    if policy_type == \"parity-like\":\n",
    "        measure_qubits = n_qubits\n",
    "    else:\n",
    "        measure_qubits = int(np.log2(n_actions))\n",
    "\n",
    "    qc = create_circuit(n_qubits, circ=type, fim=False, shots=None)\n",
    "\n",
    "    if type == \"simplified_two_design\":\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_init = torch.tensor(weights[0], requires_grad=False)\n",
    "        weights_tensor_params = torch.tensor(weights[1], requires_grad=True)\n",
    "        \n",
    "        probs = qc(weights_tensor_init,weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    else:\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_params = torch.tensor(weights, requires_grad=True)\n",
    "\n",
    "        probs = qc(weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    pi = policy(probs, policy_type=policy_type, n_actions=n_actions, n_qubits=n_qubits)\n",
    "    if clamp is not None:\n",
    "        pi = torch.clamp(pi, clamp, 1)\n",
    "\n",
    "    dist = torch.distributions.Categorical(probs=pi)\n",
    "    \n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    gradient_no_clamp = np.linalg.norm(compute_gradient(log_prob, weights_tensor_params), 2)\n",
    "    return gradient_no_clamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, env, n_episodes=1000, max_t=1000, gamma=1.0, print_every=5):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    average_scores = []\n",
    "    runtime_sum = 0\n",
    "    for e in range(1, n_episodes):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Sample the action from current policy\n",
    "            if t==0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = policy.sample(state_tensor)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # Total expected reward\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "    #standardized returns\n",
    "        R=0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0,R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + np.finfo(np.float32).eps)\n",
    "\n",
    "        for log_prob, R in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        policy_sum = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "    # Backpropagation\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        policy_sum.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        runtime = end_time-start_time\n",
    "        \n",
    "        runtime_sum += runtime\n",
    "        if e % print_every == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tLast {}\\tEpisodes average reward: {:.2f}\\tRuntime: {:.2f}'.format(e, scores_deque[-1], print_every, np.mean(scores_deque), runtime_sum))\n",
    "            runtime_sum = 0\n",
    "        if np.mean(scores_deque) == 500:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores, policy.gradient_list, average_scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def save_training_data(self):\n",
    "        ''' \n",
    "        Saves training data into json files\n",
    "        '''\n",
    "        current_directory = os.path.dirname(__file__)\n",
    "        folder_name = f\"{str(self.env_name)}_{self.policy.policy.post_processing}_{self.policy.circuit.n_layers}\"\n",
    "        folder_path = os.path.join(current_directory, folder_name)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        episode_data = [self.scores_deque, \n",
    "                        self.runtime, \n",
    "                        self.loss.item(), \n",
    "                        tensor_to_list(self.policy.get_gradients()[0]), \n",
    "                        tensor_to_list(self.policy.get_gradients()[1])]\n",
    "\n",
    "        if folder_path is not None:\n",
    "            file_path = os.path.join(self.folder_path, f\"{self.file_name}.json\")\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, 'r') as f:\n",
    "                    existing_data = json.load(f)\n",
    "                existing_data.append(episode_data)\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump(existing_data, f, indent=4)\n",
    "            else:\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump([episode_data], f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "def create_directory(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def tensor_to_list(tensor):\n",
    "    \"\"\"\n",
    "    Convert a tensor or numpy array to a nested list.\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, list):\n",
    "        return [tensor_to_list(t) for t in tensor]\n",
    "    elif isinstance(tensor, dict):\n",
    "        return {key: tensor_to_list(value) for key, value in tensor.items()}\n",
    "    elif isinstance(tensor, np.ndarray):\n",
    "        return tensor.tolist()\n",
    "    elif isinstance(tensor, torch.Tensor):\n",
    "        return tensor.tolist()\n",
    "    else:\n",
    "        return tensor\n",
    "    \n",
    "def create_optimizer_with_lr(params, lr_list, use_amsgrad=False):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': p, 'lr': lr} for p, lr in zip(params, lr_list)\n",
    "    ], amsgrad=use_amsgrad)\n",
    "    return optimizer\n",
    "\n",
    "def get_function_representation(func):\n",
    "    if callable(func):\n",
    "        # Check if the function is a lambda\n",
    "        if func.__name__ == \"<lambda>\":\n",
    "            # Optionally, check if the function has a custom description attribute\n",
    "            return f\"{func.__module__}.<lambda>\" + (getattr(func, 'description', ''))\n",
    "        else:\n",
    "            return f\"{func.__module__}.{func.__name__}\"\n",
    "    return \"Unknown Function Type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measures\n",
    "\n",
    "def measure_probs(qubits):\n",
    "    return qml.probs(wires=range(qubits)) \n",
    "\n",
    "def one_measure_expval_global(qubits):\n",
    "\n",
    "    pauli_string = qml.PauliZ(0)\n",
    "    for i in range(1, qubits):\n",
    "        pauli_string = pauli_string @ qml.PauliZ(i)\n",
    "    \n",
    "    expvals = []\n",
    "    expvals.append(qml.expval(pauli_string))\n",
    "\n",
    "    return expvals\n",
    "\n",
    "def two_measure_expval_global(qubits):\n",
    "\n",
    "    pauli_string = qml.PauliZ(0)\n",
    "    for i in range(1, qubits):\n",
    "        pauli_string = pauli_string @ qml.PauliZ(i)\n",
    "    \n",
    "    expvals = []\n",
    "    expvals.append(qml.expval(pauli_string))\n",
    "    expvals.append(qml.expval(-pauli_string))\n",
    "\n",
    "    return expvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vqc operations\n",
    "\n",
    "def strong_entangling(n_qubits, layer, entanglement_gate):\n",
    "    for qubit in range(n_qubits):\n",
    "        target = (qubit + layer + 1) % n_qubits\n",
    "        if target != qubit:\n",
    "            qml.CZ(wires=[qubit, target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JerbiModel(nn.Module):\n",
    "    '''\n",
    "    Creates a parametrized quantum circuit based on the work in https://doi.org/10.48550/arXiv.2103.05577.\n",
    "\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers, shots = None, diff_method = 'best', entanglement = True,\n",
    "                entanglement_pattern = \"all_to_all\", entanglement_gate = qml.CNOT, input_scaling = True, \n",
    "                input_init = torch.nn.init.ones_, weight_init = torch.nn.init.normal_, measure = two_measure_expval_global):\n",
    "        super(JerbiModel, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_scaling = input_scaling\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        if self.n_layers < 1:\n",
    "            raise ValueError(\"Number of layers can't take values below 1\")\n",
    "        \n",
    "        self.weight_shapes = {\n",
    "            \"params\": (self.n_layers + 1, self.n_qubits, 2),\n",
    "            \"input_params\": (self.n_layers, self.n_qubits, 2),\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"params\": self.weight_init,\n",
    "            \"input_params\": self.input_init,\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, params, input_params):\n",
    "            # In case the number of qubits is greater than the input size AND it is multiple of the input length, \n",
    "            # then tile input tensor 'multiplier' times\n",
    "            if self.n_qubits > len(inputs) and self.n_qubits % len(inputs) == 0:\n",
    "                multiplier = self.n_qubits // len(inputs)\n",
    "                inputs = torch.cat([inputs] * multiplier)\n",
    "\n",
    "            # If the number of qubits is not equal to the input length and not a multiple of the input length, raise an error\n",
    "            elif self.n_qubits != len(inputs) and self.n_qubits % len(inputs) != 0:\n",
    "                raise ValueError('Number of qubits cannot be divided by input lenght')\n",
    "\n",
    "            # Apply Hadamard gates to every qubits\n",
    "            qml.broadcast(qml.Hadamard, wires=range(self.n_qubits), pattern=\"single\")\n",
    "            \n",
    "            # Iterate through layers\n",
    "            for layer in range(self.n_layers):\n",
    "                # Iterate through qubits\n",
    "                for wire in range(self.n_qubits):\n",
    "                    # Parameterized block\n",
    "                    qml.RZ(params[layer][wire][0], wires=wire)\n",
    "                    qml.RY(params[layer][wire][1], wires=wire)\n",
    "\n",
    "                # Entanglement between qubits\n",
    "                if self.entanglement:\n",
    "                    if self.entanglement_pattern == 'strong_entangling':\n",
    "                        strong_entangling(self.n_qubits, layer, self.entanglement_gate)\n",
    "                    else:\n",
    "                        qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "                # If input scaling set to True, multiply inputs by weights\n",
    "                if self.input_scaling:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RY(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                        qml.RZ(input_params[layer][wire][1] * inputs[wire], wires=wire)\n",
    "                # If input scaling set to False, use only the inputs\n",
    "                else:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RY(inputs[wire], wires=wire)\n",
    "                        qml.RZ(inputs[wire], wires=wire)\n",
    "                \n",
    "            # Last parameterized layer\n",
    "            for wire in range(self.n_qubits):\n",
    "                qml.RZ(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)  \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        \n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        circuit = qml.draw_mpl(self.qnode)(inputs, \n",
    "                                        initialized_params[\"params\"], \n",
    "                                        initialized_params[\"input_params\"])\n",
    "        \n",
    "\n",
    "    def circuit_spectrum(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        res = qml.fourier.circuit_spectrum(self.qnode)(inputs, initialized_params[\"params\"], initialized_params[\"input_params\"])\n",
    "        for inp, freqs in res.items():\n",
    "            print(f\"{inp}: {freqs}\")       \n",
    "\n",
    "        \n",
    "    @classmethod\n",
    "    def info(cls):        \n",
    "        '''\n",
    "        Provides a summary of the JerbiModel class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        Creates the Jerbi policy (Parameterized Quantum Circuit) based on the design in https://doi.org/10.48550/arXiv.2103.05577.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_qubits (int): \n",
    "            Number of qubits used in the quantum circuit.\n",
    "        \n",
    "        n_layers (int): \n",
    "            Number of layers in the quantum circuit. Each layer typically consists of parameterized rotations followed by entanglement gates.\n",
    "        \n",
    "        shots (int, optional): \n",
    "            Number of times the circuit gets executed (repeated measurements). If None, the circuit is executed with analytic calculations (no shot noise).\n",
    "        \n",
    "        diff_method (str): \n",
    "            Differentiation method used for training the model. Common options are 'best', 'parameter-shift', 'backprop', etc.\n",
    "        \n",
    "        entanglement_pattern (str): \n",
    "            Entanglement pattern used in the circuit, such as 'chain', 'ring', 'all_to_all', etc., as defined by qml.broadcast patterns.\n",
    "        \n",
    "        entanglement_gate (function): \n",
    "            Quantum gate used for entanglement, such as qml.CZ or qml.CNOT. This gate will be applied between qubits according to the specified entanglement pattern.\n",
    "        \n",
    "        input_scaling (bool): \n",
    "            If True, input parameters are scaled by additional learnable parameters (input_params). The input is multiplied by these parameters before being applied to the qubits.\n",
    "                \n",
    "        input_init (function): \n",
    "            Function to initialize the input scaling parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, and function defined by the user.\n",
    "        \n",
    "        weight_init (function): \n",
    "            Function to initialize the weights of the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.normal_, and function defined by the user..\n",
    "                \n",
    "        measure (function): \n",
    "            Measurement function that takes the number of qubits as an argument and returns the measurement result. Common choices are measure_probs, measure_expval_pairs, or any user-defined measurement function.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        JerbiModel: \n",
    "            An instance of the JerbiModel class representing the quantum neural network.\n",
    "        \"\"\"\n",
    "        print(info_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UQC(nn.Module):\n",
    "    '''\n",
    "    Creates a parameterized quantum circuit (Universal Quantum Classifier) based in https://doi.org/10.22331/q-2020-02-06-226.\n",
    "\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers, state_dim, shots = None, diff_method = 'best', entanglement = True,\n",
    "                entanglement_pattern = \"all_to_all\", entanglement_gate = qml.CNOT, \n",
    "                weight_init = torch.nn.init.normal_, measure = two_measure_expval_global):\n",
    "        super(UQC, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.state_dim = state_dim\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        self.weight_shapes = {\n",
    "            \"weights\": (self.n_layers, self.n_qubits, self.state_dim),\n",
    "            \"params\": (self.n_layers, self.n_qubits, 1),\n",
    "            \"bias\": (self.n_layers, self.n_qubits)\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"weights\": self.weight_init,\n",
    "            \"params\": self.weight_init,\n",
    "            \"bias\": torch.nn.init.ones_\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, weights, params, bias):\n",
    "\n",
    "            # Iterate through layers\n",
    "            for layer in range(self.n_layers):\n",
    "                # Iterate through qubits\n",
    "                for wire in range(self.n_qubits):\n",
    "                    # Compute the Hadamard product, sum with bias, and use it as the angle for R_X\n",
    "                    hadamard_product = torch.dot(inputs.clone().detach(), weights[layer][wire])\n",
    "                    angle = hadamard_product + bias[layer][wire]\n",
    "\n",
    "                    # Apply the Rx gate with computed angle\n",
    "                    qml.RX(angle, wires=wire)\n",
    "\n",
    "                    qml.RZ(params[layer][wire][0], wires=wire)\n",
    "                # Entanglement between qubits\n",
    "                if self.entanglement:\n",
    "                    if self.entanglement_pattern == 'strong_entangling':\n",
    "                        strong_entangling(self.n_qubits, layer)\n",
    "                    else:\n",
    "                        qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(self.qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        \n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.state_dim)], dtype=torch.float32)\n",
    "        \n",
    "        # Initialize all parameters using the provided initialization methods\n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        circuit = qml.draw_mpl(self.qnode)(inputs, \n",
    "                                        initialized_params[\"weights\"], \n",
    "                                        initialized_params[\"params\"], \n",
    "                                        initialized_params[\"bias\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfqTutorial(nn.Module):\n",
    "    '''\n",
    "    Creates a parameterized quantum circuit based on the TensorFlow Quantum tutorial in https://www.tensorflow.org/quantum/tutorials/quantum_reinforcement_learning\n",
    "\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers, shots = None, diff_method = 'best', entanglement = True,\n",
    "                entanglement_pattern = \"ring\", entanglement_gate = qml.CNOT, input_scaling = True, \n",
    "                input_init = torch.nn.init.ones_, weight_init = torch.nn.init.normal_, measure = two_measure_expval_global):\n",
    "        super(TfqTutorial, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_scaling = input_scaling\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        if self.n_layers < 1:\n",
    "            raise ValueError(\"Number of layers can't take values below 1\")\n",
    "        \n",
    "        self.weight_shapes = {\n",
    "            \"params\": (self.n_layers + 1, self.n_qubits, 3),\n",
    "            \"input_params\": (self.n_layers, self.n_qubits, 1),\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"params\": self.weight_init,\n",
    "            \"input_params\": self.input_init,\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, params, input_params):\n",
    "            # In case the number of qubits is greater than the input size AND it is multiple of the input length, \n",
    "            # then tile input tensor 'multiplier' times\n",
    "            if self.n_qubits > len(inputs) and self.n_qubits % len(inputs) == 0:\n",
    "                multiplier = self.n_qubits // len(inputs)\n",
    "                inputs = torch.cat([inputs] * multiplier)\n",
    "\n",
    "            # If the number of qubits is not equal to the input length and not a multiple of the input length, raise an error\n",
    "            elif self.n_qubits != len(inputs) and self.n_qubits % len(inputs) != 0:\n",
    "                raise ValueError('Number of qubits cannot be divided by input lenght')\n",
    "            \n",
    "            # Iterate through layers\n",
    "            for layer in range(self.n_layers):\n",
    "                # Iterate through qubits\n",
    "                for wire in range(self.n_qubits):\n",
    "                    # Parameterized block\n",
    "                    qml.RX(params[layer][wire][0], wires=wire)\n",
    "                    qml.RY(params[layer][wire][1], wires=wire)\n",
    "                    qml.RZ(params[layer][wire][2], wires=wire)\n",
    "\n",
    "                # Entanglement between qubits\n",
    "                if self.entanglement:\n",
    "                    if self.entanglement_pattern == 'strong_entangling':\n",
    "                        strong_entangling(self.n_qubits, layer)\n",
    "                    else:\n",
    "                        qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "                # If input scaling set to True, multiply inputs by weights\n",
    "                if self.input_scaling:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RX(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                # If input scaling set to False, use only the inputs\n",
    "                else:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RX(inputs[wire], wires=wire)\n",
    "                \n",
    "            # Last parameterized layer\n",
    "            for wire in range(self.n_qubits):\n",
    "                qml.RX(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "                qml.RZ(params[-1][wire][2], wires=wire)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)  \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        \n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        circuit = qml.draw_mpl(self.qnode)(inputs, \n",
    "                                        initialized_params[\"params\"], \n",
    "                                        initialized_params[\"input_params\"])\n",
    "        \n",
    "\n",
    "    def circuit_spectrum(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        res = qml.fourier.circuit_spectrum(self.qnode)(inputs, initialized_params[\"params\"], initialized_params[\"input_params\"])\n",
    "        for inp, freqs in res.items():\n",
    "            print(f\"{inp}: {freqs}\")       \n",
    "\n",
    "        \n",
    "    @classmethod\n",
    "    def info(cls):        \n",
    "        '''\n",
    "        Provides a summary of the JerbiModel class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        Creates the Jerbi policy (Parameterized Quantum Circuit) based on the design in https://doi.org/10.48550/arXiv.2103.05577.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_qubits (int): \n",
    "            Number of qubits used in the quantum circuit.\n",
    "        \n",
    "        n_layers (int): \n",
    "            Number of layers in the quantum circuit. Each layer typically consists of parameterized rotations followed by entanglement gates.\n",
    "        \n",
    "        shots (int, optional): \n",
    "            Number of times the circuit gets executed (repeated measurements). If None, the circuit is executed with analytic calculations (no shot noise).\n",
    "        \n",
    "        diff_method (str): \n",
    "            Differentiation method used for training the model. Common options are 'best', 'parameter-shift', 'backprop', etc.\n",
    "        \n",
    "        entanglement_pattern (str): \n",
    "            Entanglement pattern used in the circuit, such as 'chain', 'ring', 'all_to_all', etc., as defined by qml.broadcast patterns.\n",
    "        \n",
    "        entanglement_gate (function): \n",
    "            Quantum gate used for entanglement, such as qml.CZ or qml.CNOT. This gate will be applied between qubits according to the specified entanglement pattern.\n",
    "        \n",
    "        input_scaling (bool): \n",
    "            If True, input parameters are scaled by additional learnable parameters (input_params). The input is multiplied by these parameters before being applied to the qubits.\n",
    "                \n",
    "        input_init (function): \n",
    "            Function to initialize the input scaling parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, and function defined by the user.\n",
    "        \n",
    "        weight_init (function): \n",
    "            Function to initialize the weights of the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.normal_, and function defined by the user..\n",
    "                \n",
    "        measure (function): \n",
    "            Measurement function that takes the number of qubits as an argument and returns the measurement result. Common choices are measure_probs, measure_expval_pairs, or any user-defined measurement function.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        JerbiModel: \n",
    "            An instance of the JerbiModel class representing the quantum neural network.\n",
    "        \"\"\"\n",
    "        print(info_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RZRYRZ(nn.Module):\n",
    "    '''\n",
    "    Creates a parametrized quantum circuit based on the work in https://doi.org/10.48550/arXiv.2103.05577.\n",
    "\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers, shots = None, diff_method = 'best', entanglement = True,\n",
    "                entanglement_pattern = \"all_to_all\", entanglement_gate = qml.CNOT, input_scaling = True, \n",
    "                input_init = torch.nn.init.ones_, weight_init = torch.nn.init.normal_, measure = two_measure_expval_global):\n",
    "        super(RZRYRZ, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_scaling = input_scaling\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        if self.n_layers < 1:\n",
    "            raise ValueError(\"Number of layers can't take values below 1\")\n",
    "        \n",
    "        self.weight_shapes = {\n",
    "            \"params\": (self.n_layers + 1, self.n_qubits, 3),\n",
    "            \"input_params\": (self.n_layers, self.n_qubits, 3),\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"params\": self.weight_init,\n",
    "            \"input_params\": self.input_init,\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, params, input_params):\n",
    "            # In case the number of qubits is greater than the input size AND it is multiple of the input length, \n",
    "            # then tile input tensor 'multiplier' times\n",
    "            if self.n_qubits > len(inputs) and self.n_qubits % len(inputs) == 0:\n",
    "                multiplier = self.n_qubits // len(inputs)\n",
    "                inputs = torch.cat([inputs] * multiplier)\n",
    "\n",
    "            # If the number of qubits is not equal to the input length and not a multiple of the input length, raise an error\n",
    "            elif self.n_qubits != len(inputs) and self.n_qubits % len(inputs) != 0:\n",
    "                raise ValueError('Number of qubits cannot be divided by input lenght')\n",
    "\n",
    "            # Apply Hadamard gates to every qubits\n",
    "            qml.broadcast(qml.Hadamard, wires=range(self.n_qubits), pattern=\"single\")\n",
    "            \n",
    "            # Iterate through layers\n",
    "            for layer in range(self.n_layers):\n",
    "                # Iterate through qubits\n",
    "                for wire in range(self.n_qubits):\n",
    "                    # Parameterized block\n",
    "                    qml.RZ(params[layer][wire][0], wires=wire)\n",
    "                    qml.RY(params[layer][wire][1], wires=wire)\n",
    "                    qml.RZ(params[layer][wire][2], wires=wire)\n",
    "\n",
    "                # Entanglement between qubits\n",
    "                if self.entanglement:\n",
    "                    if self.entanglement_pattern == 'strong_entangling':\n",
    "                        strong_entangling(self.n_qubits, layer)\n",
    "                    else:\n",
    "                        qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "                # If input scaling set to True, multiply inputs by weights\n",
    "                if self.input_scaling:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RZ(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                        qml.RY(input_params[layer][wire][1] * inputs[wire], wires=wire)\n",
    "                        qml.RZ(input_params[layer][wire][2] * inputs[wire], wires=wire)\n",
    "\n",
    "                # If input scaling set to False, use only the inputs\n",
    "                else:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RZ(inputs[wire], wires=wire)\n",
    "                        qml.RY(inputs[wire], wires=wire)\n",
    "                        qml.RZ(inputs[wire], wires=wire)\n",
    "\n",
    "                \n",
    "            # Last parameterized layer\n",
    "            for wire in range(self.n_qubits):\n",
    "                qml.RZ(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "                qml.RZ(params[-1][wire][2], wires=wire)\n",
    "                \n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)  \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        \n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        circuit = qml.draw_mpl(self.qnode)(inputs, \n",
    "                                        initialized_params[\"params\"], \n",
    "                                        initialized_params[\"input_params\"])\n",
    "        \n",
    "\n",
    "    def circuit_spectrum(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        res = qml.fourier.circuit_spectrum(self.qnode)(inputs, initialized_params[\"params\"], initialized_params[\"input_params\"])\n",
    "        for inp, freqs in res.items():\n",
    "            print(f\"{inp}: {freqs}\")       \n",
    "\n",
    "        \n",
    "    @classmethod\n",
    "    def info(cls):        \n",
    "        '''\n",
    "        Provides a summary of the JerbiModel class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        Creates the Jerbi policy (Parameterized Quantum Circuit) based on the design in https://doi.org/10.48550/arXiv.2103.05577.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_qubits (int): \n",
    "            Number of qubits used in the quantum circuit.\n",
    "        \n",
    "        n_layers (int): \n",
    "            Number of layers in the quantum circuit. Each layer typically consists of parameterized rotations followed by entanglement gates.\n",
    "        \n",
    "        shots (int, optional): \n",
    "            Number of times the circuit gets executed (repeated measurements). If None, the circuit is executed with analytic calculations (no shot noise).\n",
    "        \n",
    "        diff_method (str): \n",
    "            Differentiation method used for training the model. Common options are 'best', 'parameter-shift', 'backprop', etc.\n",
    "        \n",
    "        entanglement_pattern (str): \n",
    "            Entanglement pattern used in the circuit, such as 'chain', 'ring', 'all_to_all', etc., as defined by qml.broadcast patterns.\n",
    "        \n",
    "        entanglement_gate (function): \n",
    "            Quantum gate used for entanglement, such as qml.CZ or qml.CNOT. This gate will be applied between qubits according to the specified entanglement pattern.\n",
    "        \n",
    "        input_scaling (bool): \n",
    "            If True, input parameters are scaled by additional learnable parameters (input_params). The input is multiplied by these parameters before being applied to the qubits.\n",
    "                \n",
    "        input_init (function): \n",
    "            Function to initialize the input scaling parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, and function defined by the user.\n",
    "        \n",
    "        weight_init (function): \n",
    "            Function to initialize the weights of the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.normal_, and function defined by the user..\n",
    "                \n",
    "        measure (function): \n",
    "            Measurement function that takes the number of qubits as an argument and returns the measurement result. Common choices are measure_probs, measure_expval_pairs, or any user-defined measurement function.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        JerbiModel: \n",
    "            An instance of the JerbiModel class representing the quantum neural network.\n",
    "        \"\"\"\n",
    "        print(info_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyPostProcessing(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_actions, policy_type = 'raw_contiguous', \n",
    "                 beta_scheduling = False, beta = 1, increase_rate = 0.0005, \n",
    "                 output_scaling = False, output_init = torch.nn.init.ones_):\n",
    "        super(PolicyPostProcessing, self).__init__()\n",
    "\n",
    "        '''\n",
    "        Determines the type of policy used based on the arguments:\n",
    "\n",
    "            n_actions(int) = Number of actions\n",
    "            post_processing(str) = Type of policy ('raw_contiguous', 'raw_parity', 'softmax')\n",
    "            beta_scheduling(bool) = Inverse temperature parameter used in softmax (used if set to True)\n",
    "            beta(float) = Beta parameter or inverse temperature (used only for softmax)\n",
    "            increase_rate(float) = Amount added to beta at the end of each episode (used only for softmax)\n",
    "            output_scaling(bool) = Output parameters are used if True\n",
    "            output_init(function) = How the output parameters are initialized\n",
    "            \n",
    "        '''\n",
    "        self.n_actions = n_actions\n",
    "        self.policy_type = policy_type\n",
    "        self.beta_scheduling = beta_scheduling\n",
    "        self.beta = beta\n",
    "        self.increase_rate = increase_rate\n",
    "        self.output_scaling = output_scaling\n",
    "        self.output_init = output_init\n",
    "\n",
    "        if self.output_scaling == True:\n",
    "            #self.output_params = nn.parameter.Parameter(torch.Tensor(self.n_actions), requires_grad=True)\n",
    "            self.output_params = nn.parameter.Parameter(torch.Tensor(1), requires_grad=True)\n",
    "            self.output_init(self.output_params)\n",
    "        else:\n",
    "            self.register_parameter('w_input', None)\n",
    "\n",
    "    def forward(self,probs):\n",
    "        if self.policy_type == 'raw_contiguous':\n",
    "            policy = self.raw_contiguous(probs)\n",
    "        elif self.policy_type == 'raw_parity':\n",
    "            policy = self.raw_parity(probs)\n",
    "        elif self.policy_type == 'softmax':\n",
    "            policy = self.softmax(probs)\n",
    "        elif self.policy_type == 'softmax_probs':\n",
    "            policy = self.softmax_probs(probs)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid post-processing method specified.\")\n",
    "        return policy\n",
    "\n",
    "    def raw_contiguous(self,probs):\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        chunk_size = len(probs_flatten) // self.n_actions\n",
    "        remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "        policy = []\n",
    "\n",
    "        for i in range(self.n_actions):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size\n",
    "\n",
    "            if i < remainder:\n",
    "                end += 1\n",
    "\n",
    "            # Update the original policy list instead of creating a new one\n",
    "            policy.append(sum(probs_flatten[start:end]))\n",
    "\n",
    "        policy_tensor = torch.stack(policy)\n",
    "        return policy_tensor\n",
    "        \n",
    "    def raw_parity(self,probs):\n",
    "\n",
    "        if self.n_actions % 2 != 0:\n",
    "            raise ValueError('For parity-like policy, n_actions must be an even number')\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        policy = torch.zeros(self.n_actions)\n",
    "        counter = 0\n",
    "        for prob in probs_flatten:\n",
    "            policy[counter] += prob\n",
    "            counter += 1\n",
    "            if counter == self.n_actions:\n",
    "                counter = 0\n",
    "        \n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    def softmax(self, probs):\n",
    "        \n",
    "        if self.output_scaling == True:\n",
    "            probs *= self.output_params\n",
    "\n",
    "        scaled_output = probs * self.beta\n",
    "        softmax_output = F.softmax(scaled_output, dim=0)\n",
    "        return softmax_output\n",
    "    \n",
    "    def softmax_probs(self, probs):\n",
    "\n",
    "        if self.output_scaling == True:\n",
    "            probs *= self.output_params\n",
    "            \n",
    "        probs_flatten = probs.flatten()\n",
    "        chunk_size = len(probs_flatten) // self.n_actions\n",
    "        remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "        policy = []\n",
    "\n",
    "        for i in range(self.n_actions):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size\n",
    "\n",
    "            if i < remainder:\n",
    "                end += 1\n",
    "\n",
    "            policy.append(sum(probs_flatten[start:end]))\n",
    "        policy_tensor = torch.stack(policy)\n",
    "        softmax_output = F.softmax(policy_tensor * self.beta, dim=0)\n",
    "        return softmax_output\n",
    "    \n",
    "    def beta_schedule(self):\n",
    "        if self.beta_scheduling == True:\n",
    "            if self.policy_type == 'softmax' or self.policy_type == 'softmax_probs':\n",
    "                self.beta += self.increase_rate\n",
    "\n",
    "    @classmethod\n",
    "    def info(cls):\n",
    "        '''\n",
    "        Provides a summary of the PolicyType class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        PolicyType Class:\n",
    "\n",
    "        Determines the type of policy used based on the arguments:\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_actions (int): \n",
    "            Number of actions available for the agent to choose from.\n",
    "        \n",
    "        policy_type (str): \n",
    "            Type of policy applied to the probability distribution:\n",
    "            - 'raw_contiguous': Sums up contiguous chunks of probabilities.\n",
    "            - 'raw_parity': Sums up probabilities in a circular manner based on parity.\n",
    "            - 'softmax': Applies a softmax function to the scaled probabilities.\n",
    "            - 'softmax_probs': Sums up contiguous chunks of probabilities and then applies softmax.\n",
    "        \n",
    "        beta_scheduling (bool): \n",
    "            If True, updates the inverse temperature parameter (beta) after each episode. Used only for 'softmax' and 'softmax_probs'.\n",
    "        \n",
    "        beta (float): \n",
    "            Inverse temperature parameter used for scaling probabilities in the softmax policy.\n",
    "        \n",
    "        increase_rate (float): \n",
    "            Amount added to beta at the end of each episode, if beta_scheduling is True.\n",
    "        \n",
    "        output_scaling (bool): \n",
    "            If True, scales the output probabilities by learnable parameters.\n",
    "        \n",
    "        output_init (function): \n",
    "            Initialization function for output parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, etc.\n",
    "        \n",
    "        Methods:\n",
    "        -------\n",
    "        sample(probs):\n",
    "            Selects an action based on the chosen post_processing method.\n",
    "        \n",
    "        raw_contiguous(probs):\n",
    "            Sums up contiguous chunks of probabilities and returns the resulting tensor.\n",
    "        \n",
    "        raw_parity(probs):\n",
    "            Sums up probabilities in a circular manner based on parity and returns the action, log probability, and policy tensor.\n",
    "        \n",
    "        softmax(probs):\n",
    "            Applies a softmax function to the scaled probabilities and returns the action, log probability, and policy tensor.\n",
    "        \n",
    "        softmax_probs(probs):\n",
    "            Sums up contiguous chunks of probabilities, applies softmax, and returns the action, log probability, and policy tensor.\n",
    "        \n",
    "        beta_schedule():\n",
    "            Updates the beta parameter if beta_scheduling is True. Only applicable for 'softmax' and 'softmax_probs' methods.\n",
    "        \"\"\"\n",
    "        print(info_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumPolicy(nn.Module):\n",
    "    \n",
    "    def __init__(self, circuit, post_processing):\n",
    "        super(QuantumPolicy, self).__init__()\n",
    "        self.circuit = circuit\n",
    "        self.post_processing = post_processing\n",
    "\n",
    "    def sample(self, inputs):\n",
    "        '''\n",
    "        Samples an action from the action probability distribution\n",
    "        '''\n",
    "        policy = self.forward(inputs)\n",
    "        dist = torch.distributions.Categorical(policy)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), policy\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Input state is fed to the circuit - its output is then fed to the post processing \n",
    "        '''\n",
    "        probs = self.circuit.forward(inputs)\n",
    "        probs_processed = self.post_processing.forward(probs)\n",
    "        return probs_processed\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        '''\n",
    "        Returns the values of each set of parameters\n",
    "        '''\n",
    "        parameter_values = [param.clone().detach().numpy().flatten() for param in self.circuit.parameters()]\n",
    "        return parameter_values\n",
    "    \n",
    "    def get_gradients(self):\n",
    "        '''\n",
    "        Returns the gradient values of each set of parameters\n",
    "        '''\n",
    "        gradients = []\n",
    "\n",
    "        # Get gradients from circuit parameters\n",
    "        circuit_gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for param in self.circuit.parameters()]\n",
    "        gradients.extend(circuit_gradients)\n",
    "\n",
    "        # Get gradients from policy parameters\n",
    "        policy_gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for param in self.policy.parameters()]\n",
    "        gradients.extend(policy_gradients)\n",
    "\n",
    "        return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update aos parametros deve vir depois da if clause que verifica se foi resolvido - caso verdade, os weights n devem levar update\n",
    "\n",
    "Baselines parecem ter problemas, principalmente a baseline com PQC a aproximar a value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceAgent:\n",
    "\n",
    "    def __init__(self, policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy = policy.to(self.device)\n",
    "        self.policy_optimizer = policy_optimizer\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env_name = env_name\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "        self.solved = False\n",
    "        self.scores_deque = deque(maxlen=self.print_every)\n",
    "\n",
    "    def train(self, run_count='0', datetime = None, path=None, tensorboard=False):\n",
    "\n",
    "        if run_count is not None and path is not None:\n",
    "            base_path = create_directory(path)\n",
    "            env_folder = create_directory(os.path.join(base_path, self.env_name))\n",
    "            experiment_folder_name = f\"{self.policy.post_processing.policy_type}_{self.policy.circuit.n_layers}layer_{datetime}\"\n",
    "            experiment_path = create_directory(os.path.join(env_folder, experiment_folder_name))\n",
    "            run_path = create_directory(os.path.join(experiment_path, f'run_{str(run_count)}'))\n",
    "            #self.save_agent_data(run_path)\n",
    "        \n",
    "        if tensorboard:\n",
    "            writer = SummaryWriter(log_dir=run_path)\n",
    "        \n",
    "        for i in range(1, self.n_episodes + 1):\n",
    "            start_time = time.time()\n",
    "            self.get_trajectory()\n",
    "\n",
    "            if np.mean(self.scores_deque) > self.env.spec.reward_threshold:\n",
    "                self.solved = True\n",
    "\n",
    "            self.update_policy()\n",
    "            end_time = time.time()\n",
    "\n",
    "            self.runtime = end_time - start_time\n",
    "\n",
    "            if tensorboard:\n",
    "                self.writer_function(writer, i)\n",
    "            #if run_count is not None and path is not None:\n",
    "                #self.save_data(run_path)\n",
    "            if not self.solved:\n",
    "                self.policy.post_processing.beta_schedule()\n",
    "        \n",
    "\n",
    "            if i % self.print_every == 0 and self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}s\\t Last {} Episodes average reward: {:.2f}\\t '.format(i, self.scores_deque[-1], self.runtime, self.print_every, np.mean(self.scores_deque)))\n",
    "            elif self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}s\\t '.format(i, self.scores_deque[-1], self.runtime))\n",
    "        \n",
    "    def get_trajectory(self):\n",
    "        '''\n",
    "        Gets a trajectory based on the running policy until it runs out of bounds or achieves maximum reward of an episode\n",
    "        '''\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(self.max_t):\n",
    "            if t == 0:\n",
    "                state_tensor = torch.tensor(state[0]).float().to(self.device)\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float().to(self.device)\n",
    "            action, log_prob, _ = self.policy.sample(state_tensor)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            \n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                self.scores_deque.append(sum(self.rewards))\n",
    "                break\n",
    "\n",
    "    def update_policy(self):\n",
    "        '''\n",
    "        Computes the loss and gradients and updates the policy via gradient methods\n",
    "        '''\n",
    "\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns).to(self.device)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        for log_prob, ret in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * ret)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        self.loss = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        if not self.solved:\n",
    "            self.policy_optimizer.step()\n",
    "        \n",
    "    def save_agent_data(self,main_path):\n",
    "        '''\n",
    "        Stores the model parameters into a json file\n",
    "\n",
    "        '''\n",
    "        print('a')\n",
    "        agent_variables = {\n",
    "            \"Model\": self.policy.circuit.__class__.__name__,\n",
    "            \"Number of Qubits\": self.policy.circuit.n_qubits,\n",
    "            \"Number of Layers\": self.policy.circuit.n_layers,\n",
    "            \"Shots\": self.policy.circuit.shots,\n",
    "            \"Differentiation Method\": self.policy.circuit.diff_method,\n",
    "            \"Entanglement\": self.policy.circuit.entanglement,\n",
    "            \"Entanglement Pattern\": self.policy.circuit.entanglement_pattern,\n",
    "            \"Entanglement Gate\": self.policy.circuit.entanglement_gate,\n",
    "            \"Input Scaling\": self.policy.circuit.input_scaling,\n",
    "            \"Input Initialization\": get_function_representation(self.policy.circuit.input_init),\n",
    "            \"Weight Initialization\": get_function_representation(self.policy.circuit.weight_init),\n",
    "            \"Measure\": get_function_representation(self.policy.circuit.measure),\n",
    "            \"Policy Post Processing\": self.policy.post_processing.policy_type,\n",
    "            \"Optimizers\": str(self.policy_optimizer),\n",
    "            \"Envinronment\": str(self.env_name),\n",
    "            \"Gamma (discounting factor)\": self.gamma,\n",
    "        }\n",
    "        print('a')\n",
    "        if self.policy.post_processing.policy_type in ['softmax', 'softmax_probs']:\n",
    "            agent_variables.update({\n",
    "                \"Softmax scheduling (in case policy is softmax)\": f\"{self.policy.post_processing.beta_scheduling}. Starting beta: {self.policy.post_processing.beta}. Increase rate: {self.policy.post_processing.increase_rate}\",\n",
    "                \"Softmax output scaling (in case policy is softmax)\": f\"{self.policy.post_processing.output_scaling}. Output Initialization: {get_function_representation(self.policy.post_processing.output_init)}\",\n",
    "            })\n",
    "        print('a')\n",
    "        with open(os.path.join(main_path, \"agent_characteristics.json\"), \"w\") as f:\n",
    "            json.dump(agent_variables, f, indent=4)\n",
    "\n",
    "    def save_data(self, run_path):\n",
    "        '''\n",
    "        Saves the data into a .npz file of each episode\n",
    "\n",
    "        '''\n",
    "\n",
    "        if not os.path.exists(run_path):\n",
    "            os.makedirs(run_path)\n",
    "\n",
    "        if os.path.exists(run_path):\n",
    "            data = np.load(run_path, allow_pickle=True)\n",
    "            old_episode_reward = data['episode_reward'].tolist()\n",
    "            old_loss = data['loss'].tolist()\n",
    "            old_runtime = data['runtime'].tolist()\n",
    "            old_params_gradients = data['params_gradients'].tolist()\n",
    "            old_input_params_gradients = data['input_params_gradients'].tolist()\n",
    "        else:\n",
    "            old_episode_reward = []\n",
    "            old_loss = []\n",
    "            old_runtime = []\n",
    "            old_params_gradients = []\n",
    "            old_input_params_gradients = []\n",
    "\n",
    "\n",
    "        old_episode_reward.append(self.scores_deque[-1])\n",
    "        old_loss.append(self.loss.item())\n",
    "        old_runtime.append(self.runtime)\n",
    "        old_params_gradients.append(tensor_to_list(self.policy.get_gradients()[0]))\n",
    "        old_input_params_gradients.append(tensor_to_list(self.policy.get_gradients()[1]))\n",
    "\n",
    "        np.savez_compressed(run_path, episode_reward = np.array(old_episode_reward),\n",
    "                                 loss = np.array(old_loss),\n",
    "                                 runtime = np.array(old_runtime),\n",
    "                                 params_gradients = np.array(old_params_gradients), \n",
    "                                 input_params_gradients = np.array(old_input_params_gradients))\n",
    "        \n",
    "        del old_episode_reward[:]\n",
    "        del old_loss[:]\n",
    "        del old_runtime[:]\n",
    "        del old_params_gradients[:]\n",
    "        del old_input_params_gradients[:]\n",
    "\n",
    "    def writer_function(self, writer, iteration):\n",
    "        '''\n",
    "        Stores data into a tensorboard session\n",
    "\n",
    "        '''\n",
    "        writer.add_scalar(\"Episode Reward\", self.scores_deque[-1], global_step=iteration)\n",
    "        writer.add_scalar(\"Runtime\", self.runtime, global_step=iteration)\n",
    "        writer.add_scalar(\"Loss\", self.loss.item(), global_step=iteration)\n",
    "        if self.policy.post_processing.policy_type in ['softmax', 'softmax_probs']:\n",
    "            writer.add_scalar(\"Beta\", self.policy.post_processing.beta, global_step=iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceBatchAgent:\n",
    "\n",
    "    def __init__(self, policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose, batch_size=10):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy = policy.to(self.device)\n",
    "        self.policy_optimizer = policy_optimizer\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env_name = env_name\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.solved = False\n",
    "        self.scores_deque = deque(maxlen=self.print_every)\n",
    "\n",
    "    def train(self, run_count=None, path=None, tensorboard=False):\n",
    "\n",
    "        if run_count is not None and path is not None:\n",
    "            base_path = create_directory(path)\n",
    "            env_folder = create_directory(os.path.join(base_path, self.env_name))\n",
    "            experiment_folder_name = f\"{self.policy.post_processing.policy_type}_{self.policy.circuit.n_layers}layer_{datetime.now().strftime('%Y-%m-%d_%H.%M')}\"\n",
    "            experiment_path = create_directory(os.path.join(env_folder, experiment_folder_name))\n",
    "            run_path = create_directory(os.path.join(experiment_path, f'run_{str(run_count)}'))\n",
    "            self.save_agent_data(run_path)\n",
    "        \n",
    "        if tensorboard:\n",
    "            writer = SummaryWriter(log_dir=run_path)\n",
    "\n",
    "        batch_log_probs = []\n",
    "        batch_returns = []\n",
    "\n",
    "        for i in range(1, self.n_episodes + 1):\n",
    "            start_time = time.time()\n",
    "            self.get_trajectory()\n",
    "\n",
    "            if np.mean(self.scores_deque) > self.env.spec.reward_threshold:\n",
    "                self.solved = True\n",
    "\n",
    "            self.update_policy()\n",
    "            end_time = time.time()\n",
    "\n",
    "            self.runtime = end_time - start_time\n",
    "\n",
    "            if tensorboard:\n",
    "                self.writer_function(writer, i)\n",
    "            if run_count is not None and path is not None:\n",
    "                self.save_data(run_path)\n",
    "            if not self.solved:\n",
    "                self.policy.post_processing.beta_schedule()\n",
    "\n",
    "            if i % self.print_every == 0 and self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}s\\t Last {} Episodes average reward: {:.2f}\\t '.format(i, self.scores_deque[-1], self.runtime, self.print_every, np.mean(self.scores_deque)))\n",
    "            elif self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}s\\t '.format(i, self.scores_deque[-1], self.runtime))\n",
    "\n",
    "        if batch_log_probs:\n",
    "            self.update_policy(batch_log_probs, batch_returns)\n",
    "\n",
    "    def get_trajectory(self):\n",
    "        '''\n",
    "        Gets a trajectory based on the running policy until it runs out of bounds or achieves maximum reward of an episode\n",
    "        '''\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(self.max_t):\n",
    "            if t == 0:\n",
    "                state_tensor = torch.tensor(state[0]).float().to(self.device)\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float().to(self.device)\n",
    "            action, log_prob, _ = self.policy.sample(state_tensor)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            \n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                self.scores_deque.append(sum(self.rewards))\n",
    "                break\n",
    "\n",
    "    def update_policy(self, batch_log_probs, batch_returns):\n",
    "        '''\n",
    "        Computes the loss and gradients and updates the policy via gradient methods\n",
    "        '''\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for batch in batch_returns:\n",
    "            ep_return = []\n",
    "            for r in batch[::-1]:\n",
    "                R = r + self.gamma * R\n",
    "                ep_return.insert(0, R)\n",
    "            ep_return = torch.tensor(ep_return).to(self.device)\n",
    "            ep_return = (ep_return - ep_return.mean()) / (ep_return.std() + 1e-8)\n",
    "            returns.extend(ep_return)\n",
    "            \n",
    "        for log_prob, ret in zip(batch_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * ret)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        self.loss = torch.cat(policy_unsqueezed).sum() / self.batch_size\n",
    "\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        if not self.solved:\n",
    "            self.policy_optimizer.step()\n",
    "        \n",
    "    def save_agent_data(self,main_path):\n",
    "        '''\n",
    "        Stores the model parameters into a json file\n",
    "\n",
    "        '''\n",
    "        agent_variables = {\n",
    "            \"Model\": self.policy.circuit.__class__.__name__,\n",
    "            \"Number of Qubits\": self.policy.circuit.n_qubits,\n",
    "            \"Number of Layers\": self.policy.circuit.n_layers,\n",
    "            \"Shots\": self.policy.circuit.shots,\n",
    "            \"Differentiation Method\": self.policy.circuit.diff_method,\n",
    "            \"Entanglement\": self.policy.circuit.entanglement,\n",
    "            \"Entanglement Pattern\": self.policy.circuit.entanglement_pattern,\n",
    "            \"Entanglement Gate\": self.policy.circuit.entanglement_gate,\n",
    "            \"Input Scaling\": self.policy.circuit.input_scaling,\n",
    "            \"Input Initialization\": get_function_representation(self.policy.circuit.input_init),\n",
    "            \"Weight Initialization\": get_function_representation(self.policy.circuit.weight_init),\n",
    "            \"Measure\": get_function_representation(self.policy.circuit.measure),\n",
    "            \"Policy Post Processing\": self.policy.post_processing.policy_type,\n",
    "            \"Optimizers\": str(self.policy_optimizer),\n",
    "            \"Envinronment\": str(self.env_name),\n",
    "            \"Gamma (discounting factor)\": self.gamma,\n",
    "        }\n",
    "\n",
    "        if self.policy.post_processing.policy_type in ['softmax', 'softmax_probs']:\n",
    "            agent_variables.update({\n",
    "                \"Softmax scheduling (in case policy is softmax)\": f\"{self.policy.post_processing.beta_scheduling}. Starting beta: {self.policy.post_processing.beta}. Increase rate: {self.policy.post_processing.increase_rate}\",\n",
    "                \"Softmax output scaling (in case policy is softmax)\": f\"{self.policy.post_processing.output_scaling}. Output Initialization: {get_function_representation(self.policy.post_processing.output_init)}\",\n",
    "            })\n",
    "        with open(os.path.join(main_path, \"agent_characteristics.json\"), \"w\") as f:\n",
    "            json.dump(agent_variables, f, indent=4)\n",
    "\n",
    "    def save_data(self, run_path):\n",
    "        '''\n",
    "        Saves the data into a .npz file of each episode\n",
    "\n",
    "        '''\n",
    "\n",
    "        if not os.path.exists(run_path):\n",
    "            os.makedirs(run_path)\n",
    "\n",
    "        if os.path.exists(run_path):\n",
    "            data = np.load(run_path, allow_pickle=True)\n",
    "            old_episode_reward = data['episode_reward'].tolist()\n",
    "            old_loss = data['loss'].tolist()\n",
    "            old_runtime = data['runtime'].tolist()\n",
    "            old_params_gradients = data['params_gradients'].tolist()\n",
    "            old_input_params_gradients = data['input_params_gradients'].tolist()\n",
    "        else:\n",
    "            old_episode_reward = []\n",
    "            old_loss = []\n",
    "            old_runtime = []\n",
    "            old_params_gradients = []\n",
    "            old_input_params_gradients = []\n",
    "\n",
    "\n",
    "        old_episode_reward.append(self.scores_deque[-1])\n",
    "        old_loss.append(self.loss.item())\n",
    "        old_runtime.append(self.runtime)\n",
    "        old_params_gradients.append(tensor_to_list(self.policy.get_gradients()[0]))\n",
    "        old_input_params_gradients.append(tensor_to_list(self.policy.get_gradients()[1]))\n",
    "\n",
    "        np.savez_compressed(run_path, episode_reward = np.array(old_episode_reward),\n",
    "                                 loss = np.array(old_loss),\n",
    "                                 runtime = np.array(old_runtime),\n",
    "                                 params_gradients = np.array(old_params_gradients), \n",
    "                                 input_params_gradients = np.array(old_input_params_gradients))\n",
    "        \n",
    "        del old_episode_reward[:]\n",
    "        del old_loss[:]\n",
    "        del old_runtime[:]\n",
    "        del old_params_gradients[:]\n",
    "        del old_input_params_gradients[:]\n",
    "\n",
    "    def writer_function(self, writer, iteration):\n",
    "        '''\n",
    "        Stores data into a tensorboard session\n",
    "\n",
    "        '''\n",
    "        writer.add_scalar(\"Episode Reward\", self.scores_deque[-1], global_step=iteration)\n",
    "        writer.add_scalar(\"Runtime\", self.runtime, global_step=iteration)\n",
    "        writer.add_scalar(\"Loss\", self.loss.item(), global_step=iteration)\n",
    "        if self.policy.post_processing.policy_type in ['softmax', 'softmax_probs']:\n",
    "            writer.add_scalar(\"Beta\", self.policy.post_processing.beta, global_step=iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceSimpleBaselineAgent:\n",
    "\n",
    "    def __init__(self, policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy = policy.to(self.device)\n",
    "        self.policy_optimizer = policy_optimizer\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env_name = env_name\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "        self.solved = False\n",
    "        self.scores_deque = deque(maxlen=self.print_every)\n",
    "\n",
    "    def train(self, run_count=None, path=None, tensorboard=False):\n",
    "\n",
    "        if run_count is not None and path is not None:\n",
    "            base_path = create_directory(path)\n",
    "            env_folder = create_directory(os.path.join(base_path, self.env_name))\n",
    "            experiment_folder_name = f\"{self.policy.post_processing.policy_type}_{self.policy.circuit.n_layers}layer_{datetime.now().strftime('%Y-%m-%d_%H.%M')}\"\n",
    "            experiment_path = create_directory(os.path.join(env_folder, experiment_folder_name))\n",
    "            run_path = create_directory(os.path.join(experiment_path, f'run_{str(run_count)}'))\n",
    "            self.save_agent_data(run_path)\n",
    "        \n",
    "        if tensorboard:\n",
    "            writer = SummaryWriter(log_dir=run_path)\n",
    "        \n",
    "        for i in range(1, self.n_episodes + 1):\n",
    "            start_time = time.time()\n",
    "            self.get_trajectory()\n",
    "\n",
    "            if np.mean(self.scores_deque) > self.env.spec.reward_threshold:\n",
    "                self.solved = True\n",
    "\n",
    "            self.update_policy()\n",
    "            end_time = time.time()\n",
    "\n",
    "            self.runtime = end_time - start_time\n",
    "\n",
    "            if tensorboard:\n",
    "                self.writer_function(writer, i)\n",
    "            if run_count is not None and path is not None:\n",
    "                self.save_data(run_path)\n",
    "            if not self.solved:\n",
    "                self.policy.post_processing.beta_schedule()\n",
    "\n",
    "            if i % self.print_every == 0 and self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}s\\t Last {} Episodes average reward: {:.2f}\\t '.format(i, self.scores_deque[-1], self.runtime, self.print_every, np.mean(self.scores_deque)))\n",
    "            elif self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}s\\t '.format(i, self.scores_deque[-1], self.runtime))\n",
    "        \n",
    "    def get_trajectory(self):\n",
    "        '''\n",
    "        Gets a trajectory based on the running policy until it runs out of bounds or achieves maximum reward of an episode\n",
    "        '''\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(self.max_t):\n",
    "            if t == 0:\n",
    "                state_tensor = torch.tensor(state[0]).float().to(self.device)\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float().to(self.device)\n",
    "            action, log_prob, _ = self.policy.sample(state_tensor)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            \n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                self.scores_deque.append(sum(self.rewards))\n",
    "                break\n",
    "\n",
    "    def update_policy(self):\n",
    "        '''\n",
    "        Computes the loss and gradients and updates the policy via gradient methods\n",
    "        '''\n",
    "\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns).to(self.device)\n",
    "        #returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        baseline = returns.mean()\n",
    "        for log_prob, ret in zip(self.saved_log_probs, returns):\n",
    "            advantage = ret - baseline\n",
    "            policy_loss.append(-log_prob * advantage)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        self.loss = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        if self.solved is False:\n",
    "            self.policy_optimizer.step()\n",
    "        \n",
    "    def save_agent_data(self,main_path):\n",
    "        '''\n",
    "        Stores the model parameters into a json file\n",
    "\n",
    "        '''\n",
    "        agent_variables = {\n",
    "            \"Model\": self.policy.circuit.__class__.__name__,\n",
    "            \"Number of Qubits\": self.policy.circuit.n_qubits,\n",
    "            \"Number of Layers\": self.policy.circuit.n_layers,\n",
    "            \"Shots\": self.policy.circuit.shots,\n",
    "            \"Differentiation Method\": self.policy.circuit.diff_method,\n",
    "            \"Entanglement\": self.policy.circuit.entanglement,\n",
    "            \"Entanglement Pattern\": self.policy.circuit.entanglement_pattern,\n",
    "            \"Entanglement Gate\": self.policy.circuit.entanglement_gate,\n",
    "            \"Input Scaling\": self.policy.circuit.input_scaling,\n",
    "            \"Input Initialization\": get_function_representation(self.policy.circuit.input_init),\n",
    "            \"Weight Initialization\": get_function_representation(self.policy.circuit.weight_init),\n",
    "            \"Measure\": get_function_representation(self.policy.circuit.measure),\n",
    "            \"Policy Post Processing\": self.policy.post_processing.policy_type,\n",
    "            \"Optimizers\": str(self.policy_optimizer),\n",
    "            \"Envinronment\": str(self.env_name),\n",
    "            \"Gamma (discounting factor)\": self.gamma,\n",
    "        }\n",
    "\n",
    "        if self.policy.post_processing.policy_type in ['softmax', 'softmax_probs']:\n",
    "            agent_variables.update({\n",
    "                \"Softmax scheduling (in case policy is softmax)\": f\"{self.policy.post_processing.beta_scheduling}. Starting beta: {self.policy.post_processing.beta}. Increase rate: {self.policy.post_processing.increase_rate}\",\n",
    "                \"Softmax output scaling (in case policy is softmax)\": f\"{self.policy.post_processing.output_scaling}. Output Initialization: {get_function_representation(self.policy.post_processing.output_init)}\",\n",
    "            })\n",
    "        with open(os.path.join(main_path, \"agent_characteristics.json\"), \"w\") as f:\n",
    "            json.dump(agent_variables, f, indent=4)\n",
    "\n",
    "    def save_data(self, run_path):\n",
    "        '''\n",
    "        Saves the data into a .npz file of each episode\n",
    "\n",
    "        '''\n",
    "\n",
    "        if not os.path.exists(run_path):\n",
    "            os.makedirs(run_path)\n",
    "\n",
    "        if os.path.exists(run_path):\n",
    "            data = np.load(run_path, allow_pickle=True)\n",
    "            old_episode_reward = data['episode_reward'].tolist()\n",
    "            old_loss = data['loss'].tolist()\n",
    "            old_runtime = data['runtime'].tolist()\n",
    "            old_params_gradients = data['params_gradients'].tolist()\n",
    "            old_input_params_gradients = data['input_params_gradients'].tolist()\n",
    "        else:\n",
    "            old_episode_reward = []\n",
    "            old_loss = []\n",
    "            old_runtime = []\n",
    "            old_params_gradients = []\n",
    "            old_input_params_gradients = []\n",
    "\n",
    "\n",
    "        old_episode_reward.append(self.scores_deque[-1])\n",
    "        old_loss.append(self.loss.item())\n",
    "        old_runtime.append(self.runtime)\n",
    "        old_params_gradients.append(tensor_to_list(self.policy.get_gradients()[0]))\n",
    "        old_input_params_gradients.append(tensor_to_list(self.policy.get_gradients()[1]))\n",
    "\n",
    "        np.savez_compressed(run_path, episode_reward = np.array(old_episode_reward),\n",
    "                                 loss = np.array(old_loss),\n",
    "                                 runtime = np.array(old_runtime),\n",
    "                                 params_gradients = np.array(old_params_gradients), \n",
    "                                 input_params_gradients = np.array(old_input_params_gradients))\n",
    "        \n",
    "        del old_episode_reward[:]\n",
    "        del old_loss[:]\n",
    "        del old_runtime[:]\n",
    "        del old_params_gradients[:]\n",
    "        del old_input_params_gradients[:]\n",
    "\n",
    "    def writer_function(self, writer, iteration):\n",
    "        '''\n",
    "        Stores data into a tensorboard session\n",
    "\n",
    "        '''\n",
    "        writer.add_scalar(\"Episode Reward\", self.scores_deque[-1], global_step=iteration)\n",
    "        writer.add_scalar(\"Runtime\", self.runtime, global_step=iteration)\n",
    "        writer.add_scalar(\"Loss\", self.loss.item(), global_step=iteration)\n",
    "        if self.policy.post_processing.policy_type in ['softmax', 'softmax_probs']:\n",
    "            writer.add_scalar(\"Beta\", self.policy.post_processing.beta, global_step=iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceDynamicBaselineAgent:\n",
    "\n",
    "    def __init__(self, policy, policy_optimizer, value_net, value_net_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose):\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy = policy.to(self.device)\n",
    "        self.policy_optimizer = policy_optimizer\n",
    "        self.value_net = value_net.to(self.device)\n",
    "        self.value_net_optimizer = value_net_optimizer\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env_name = env_name\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "        self.solved = False\n",
    "        self.scores_deque = deque(maxlen=self.print_every)\n",
    "\n",
    "\n",
    "    def train(self, run_count=None, path=None, tensorboard=False):\n",
    "        if run_count is not None and path is not None:\n",
    "            base_path = create_directory(path)\n",
    "            env_folder = create_directory(os.path.join(base_path, self.env_name))\n",
    "            experiment_folder_name = f\"{self.policy.post_processing.policy_type}_{self.policy.circuit.n_layers}layer_{datetime.now().strftime('%Y-%m-%d_%H.%M')}\"\n",
    "            experiment_path = create_directory(os.path.join(env_folder, experiment_folder_name))\n",
    "            run_path = create_directory(os.path.join(experiment_path, f'run_{str(run_count)}'))\n",
    "            self.save_agent_data(run_path)\n",
    "\n",
    "        if tensorboard:\n",
    "            writer = SummaryWriter(log_dir=run_path)\n",
    "\n",
    "        for i in range(1, self.n_episodes):\n",
    "            start_time = time.time()\n",
    "            self.get_trajectory()\n",
    "\n",
    "            if np.mean(self.scores_deque) > self.env.spec.reward_threshold:\n",
    "                self.solved = True\n",
    "\n",
    "            self.update_policy()\n",
    "            end_time = time.time()\n",
    "\n",
    "            self.runtime = end_time - start_time\n",
    "\n",
    "            if tensorboard:\n",
    "                self.writer_function(writer, i)\n",
    "            if run_count is not None and path is not None:\n",
    "                self.save_data(run_path)\n",
    "            if not self.solved:\n",
    "                self.policy.post_processing.beta_schedule()\n",
    "\n",
    "            if i % self.print_every == 0 and self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}s\\t Last {} Episodes average reward: {:.2f}\\t '.format(i, self.scores_deque[-1], self.runtime, self.print_every, np.mean(self.scores_deque)))\n",
    "                print(self.scores_deque, np.mean(self.scores_deque))            \n",
    "            elif self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}s\\t '.format(i, self.scores_deque[-1], self.runtime))\n",
    "\n",
    "    def get_trajectory(self):\n",
    "        '''\n",
    "        Gets a trajectory based on the running policy until it runs out of bounds or achieves maximum reward of an episode\n",
    "        '''\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.states = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(self.max_t):\n",
    "            if t == 0:\n",
    "                state_tensor = torch.tensor(state[0]).float().to(self.device)\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float().to(self.device)\n",
    "            self.states.append(state_tensor)\n",
    "            action, log_prob, _ = self.policy.sample(state_tensor)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                self.scores_deque.append(sum(self.rewards))\n",
    "                break\n",
    "\n",
    "    def update_policy(self):\n",
    "        '''\n",
    "        Computes the loss and gradients and updates the policy via gradient methods\n",
    "        '''\n",
    "\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        value_net_loss = []\n",
    "        returns = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns).to(self.device)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        for log_prob, ret, state in zip(self.saved_log_probs, returns, self.states):\n",
    "            value_estimate = self.value_net.forward(state)\n",
    "            advantage = ret - value_estimate.item()\n",
    "            policy_loss.append(-log_prob * advantage)\n",
    "            value_net_loss.append(F.mse_loss(value_estimate, torch.tensor([ret]).to(self.device)))\n",
    "\n",
    "        self.policy_loss = torch.cat([torch.unsqueeze(loss, 0) for loss in policy_loss]).sum()\n",
    "        self.value_loss = torch.cat([torch.unsqueeze(loss, 0) for loss in value_net_loss]).sum()\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        self.policy_loss.backward()\n",
    "        if not self.solved:\n",
    "            self.policy_optimizer.step()\n",
    "\n",
    "        self.value_net_optimizer.zero_grad()\n",
    "        self.value_loss.backward()\n",
    "        if not self.solved:\n",
    "            self.value_net_optimizer.step()\n",
    "        \n",
    "    def save_agent_data(self,main_path):\n",
    "        '''\n",
    "        Stores the model parameters into a json file\n",
    "\n",
    "        '''\n",
    "        agent_variables = {\n",
    "            \"Model\": self.policy.circuit.__class__.__name__,\n",
    "            \"Number of Qubits\": self.policy.circuit.n_qubits,\n",
    "            \"Number of Layers\": self.policy.circuit.n_layers,\n",
    "            \"Shots\": self.policy.circuit.shots,\n",
    "            \"Differentiation Method\": self.policy.circuit.diff_method,\n",
    "            \"Entanglement\": self.policy.circuit.entanglement,\n",
    "            \"Entanglement Pattern\": self.policy.circuit.entanglement_pattern,\n",
    "            \"Entanglement Gate\": self.policy.circuit.entanglement_gate,\n",
    "            \"Input Scaling\": self.policy.circuit.input_scaling,\n",
    "            \"Input Initialization\": get_function_representation(self.policy.circuit.input_init),\n",
    "            \"Weight Initialization\": get_function_representation(self.policy.circuit.weight_init),\n",
    "            \"Measure\": get_function_representation(self.policy.circuit.measure),\n",
    "            \"Policy Post Processing\": self.policy.post_processing.policy_type,\n",
    "            \"Optimizers\": str(self.optimizer),\n",
    "            \"Envinronment\": str(self.env_name),\n",
    "            \"Gamma (discounting factor)\": self.gamma,\n",
    "        }\n",
    "\n",
    "        if self.policy.post_processing.policy_type in ['softmax', 'softmax_probs']:\n",
    "            agent_variables.update({\n",
    "                \"Softmax scheduling (in case policy is softmax)\": f\"{self.policy.post_processing.beta_scheduling}. Starting beta: {self.policy.post_processing.beta}. Increase rate: {self.policy.post_processing.increase_rate}\",\n",
    "                \"Softmax output scaling (in case policy is softmax)\": f\"{self.policy.post_processing.output_scaling}. Output Initialization: {get_function_representation(self.policy.post_processing.output_init)}\",\n",
    "            })\n",
    "        with open(os.path.join(main_path, \"agent_characteristics.json\"), \"w\") as f:\n",
    "            json.dump(agent_variables, f, indent=4)\n",
    "\n",
    "    def save_data(self, run_path):\n",
    "        '''\n",
    "        Saves the data into a .npz file of each episode\n",
    "\n",
    "        '''\n",
    "\n",
    "        if not os.path.exists(run_path):\n",
    "            os.makedirs(run_path)\n",
    "\n",
    "        if os.path.exists(run_path):\n",
    "            data = np.load(run_path, allow_pickle=True)\n",
    "            old_episode_reward = data['episode_reward'].tolist()\n",
    "            old_loss = data['loss'].tolist()\n",
    "            old_runtime = data['runtime'].tolist()\n",
    "            old_params_gradients = data['params_gradients'].tolist()\n",
    "            old_input_params_gradients = data['input_params_gradients'].tolist()\n",
    "        else:\n",
    "            old_episode_reward = []\n",
    "            old_loss = []\n",
    "            old_runtime = []\n",
    "            old_params_gradients = []\n",
    "            old_input_params_gradients = []\n",
    "\n",
    "\n",
    "        old_episode_reward.append(self.scores_deque[-1])\n",
    "        old_loss.append(self.loss.item())\n",
    "        old_runtime.append(self.runtime)\n",
    "        old_params_gradients.append(tensor_to_list(self.policy.get_gradients()[0]))\n",
    "        old_input_params_gradients.append(tensor_to_list(self.policy.get_gradients()[1]))\n",
    "\n",
    "        np.savez_compressed(run_path, episode_reward = np.array(old_episode_reward),\n",
    "                                 loss = np.array(old_loss),\n",
    "                                 runtime = np.array(old_runtime),\n",
    "                                 params_gradients = np.array(old_params_gradients), \n",
    "                                 input_params_gradients = np.array(old_input_params_gradients))\n",
    "        \n",
    "        del old_episode_reward[:]\n",
    "        del old_loss[:]\n",
    "        del old_runtime[:]\n",
    "        del old_params_gradients[:]\n",
    "        del old_input_params_gradients[:]\n",
    "\n",
    "    def writer_function(self, writer, iteration):\n",
    "        '''\n",
    "        Stores data into a tensorboard session\n",
    "\n",
    "        '''\n",
    "        writer.add_scalar(\"Episode Reward\", self.scores_deque[-1], global_step=iteration)\n",
    "        writer.add_scalar(\"Runtime\", self.runtime, global_step=iteration)\n",
    "        writer.add_scalar(\"Loss\", self.loss.item(), global_step=iteration)\n",
    "        if self.policy.post_processing.policy_type in ['softmax', 'softmax_probs']:\n",
    "            writer.add_scalar(\"Beta\", self.policy.post_processing.beta, global_step=iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jerbi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 4\n",
    "shots = None\n",
    "diff_method = 'backprop' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"strong_entangling\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = torch.nn.init.ones_\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval_global\n",
    "policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "policy_circuit.visualize_circuit()\n",
    "#policy_circuit.circuit_spectrum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 4\n",
    "shots = None\n",
    "diff_method = 'best' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = torch.nn.init.ones_\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).normal_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval_global\n",
    "policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "#value_circuit_measure = one_measure_expval_global\n",
    "#value_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "#                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "#                     input_scaling, input_init, weight_init, value_circuit_measure)\n",
    "\n",
    "n_actions = 2\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = True\n",
    "beta = 1\n",
    "increase_rate = 0.0005\n",
    "output_scaling = True\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                         post_processing, \n",
    "                         beta_scheduling, \n",
    "                         beta, increase_rate, \n",
    "                         output_scaling, \n",
    "                         output_init)\n",
    "\n",
    "policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "value_lr_list = [0.01, 0.1]\n",
    "#value_params = list(value_circuit.parameters())\n",
    "#value_optimizer = create_optimizer_with_lr(value_params, value_lr_list)\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "n_episodes = 5000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 10\n",
    "verbose = 1\n",
    "reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "#reinforce_update = ReinforceBatchAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "#reinforce_update = ReinforceSimpleBaselineAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "#reinforce_update = ReinforceDynamicBaselineAgent(policy, policy_optimizer, value_circuit, value_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2300x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_qubits = 3\n",
    "n_layers = 4\n",
    "state_dim = 4\n",
    "shots = None\n",
    "diff_method = 'best' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval_global\n",
    "policy_circuit = UQC(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     weight_init, policy_circuit_measure)\n",
    "\n",
    "policy_circuit.visualize_circuit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tLast reward: 28.00\tRuntime: 0.14s\t \n",
      "Episode 2\tLast reward: 28.00\tRuntime: 0.14s\t \n",
      "Episode 3\tLast reward: 28.00\tRuntime: 0.14s\t \n",
      "Episode 4\tLast reward: 19.00\tRuntime: 0.09s\t \n",
      "Episode 5\tLast reward: 10.00\tRuntime: 0.05s\t \n",
      "Episode 6\tLast reward: 11.00\tRuntime: 0.05s\t \n",
      "Episode 7\tLast reward: 25.00\tRuntime: 0.12s\t \n",
      "Episode 8\tLast reward: 26.00\tRuntime: 0.12s\t \n",
      "Episode 9\tLast reward: 17.00\tRuntime: 0.08s\t \n",
      "Episode 10\tLast reward: 15.00\tRuntime: 0.07s\t Last 10 Episodes average reward: 20.70\t \n",
      "Episode 11\tLast reward: 13.00\tRuntime: 0.06s\t \n",
      "Episode 12\tLast reward: 12.00\tRuntime: 0.06s\t \n",
      "Episode 13\tLast reward: 10.00\tRuntime: 0.05s\t \n",
      "Episode 14\tLast reward: 14.00\tRuntime: 0.07s\t \n",
      "Episode 15\tLast reward: 10.00\tRuntime: 0.05s\t \n",
      "Episode 16\tLast reward: 14.00\tRuntime: 0.07s\t \n",
      "Episode 17\tLast reward: 11.00\tRuntime: 0.05s\t \n",
      "Episode 18\tLast reward: 16.00\tRuntime: 0.08s\t \n",
      "Episode 19\tLast reward: 13.00\tRuntime: 0.06s\t \n",
      "Episode 20\tLast reward: 14.00\tRuntime: 0.07s\t Last 10 Episodes average reward: 12.70\t \n",
      "Episode 21\tLast reward: 12.00\tRuntime: 0.06s\t \n",
      "Episode 22\tLast reward: 9.00\tRuntime: 0.04s\t \n",
      "Episode 23\tLast reward: 16.00\tRuntime: 0.09s\t \n",
      "Episode 24\tLast reward: 14.00\tRuntime: 0.07s\t \n",
      "Episode 25\tLast reward: 13.00\tRuntime: 0.06s\t \n",
      "Episode 26\tLast reward: 16.00\tRuntime: 0.08s\t \n",
      "Episode 27\tLast reward: 15.00\tRuntime: 0.07s\t \n",
      "Episode 28\tLast reward: 14.00\tRuntime: 0.07s\t \n",
      "Episode 29\tLast reward: 23.00\tRuntime: 0.12s\t \n",
      "Episode 30\tLast reward: 18.00\tRuntime: 0.09s\t Last 10 Episodes average reward: 15.00\t \n",
      "Episode 31\tLast reward: 15.00\tRuntime: 0.07s\t \n",
      "Episode 32\tLast reward: 25.00\tRuntime: 0.12s\t \n",
      "Episode 33\tLast reward: 30.00\tRuntime: 0.15s\t \n",
      "Episode 34\tLast reward: 40.00\tRuntime: 0.19s\t \n",
      "Episode 35\tLast reward: 34.00\tRuntime: 0.17s\t \n",
      "Episode 36\tLast reward: 23.00\tRuntime: 0.11s\t \n",
      "Episode 37\tLast reward: 26.00\tRuntime: 0.12s\t \n",
      "Episode 38\tLast reward: 63.00\tRuntime: 0.30s\t \n",
      "Episode 39\tLast reward: 33.00\tRuntime: 0.17s\t \n",
      "Episode 40\tLast reward: 52.00\tRuntime: 0.26s\t Last 10 Episodes average reward: 34.10\t \n",
      "Episode 41\tLast reward: 89.00\tRuntime: 0.44s\t \n",
      "Episode 42\tLast reward: 65.00\tRuntime: 0.32s\t \n",
      "Episode 43\tLast reward: 69.00\tRuntime: 0.34s\t \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 54\u001b[0m\n\u001b[0;32m     50\u001b[0m reinforce_update \u001b[38;5;241m=\u001b[39m ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m#reinforce_update = ReinforceBatchAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m#reinforce_update = ReinforceSimpleBaselineAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m#reinforce_update = ReinforceDynamicBaselineAgent(policy, policy_optimizer, value_circuit, value_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m reinforce_update\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[1;32mIn[31], line 32\u001b[0m, in \u001b[0;36mReinforceAgent.train\u001b[1;34m(self, run_count, datetime, path, tensorboard)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_episodes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     31\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_trajectory()\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscores_deque) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mspec\u001b[38;5;241m.\u001b[39mreward_threshold:\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolved \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[31], line 68\u001b[0m, in \u001b[0;36mReinforceAgent.get_trajectory\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 68\u001b[0m action, log_prob, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39msample(state_tensor)\n\u001b[0;32m     69\u001b[0m state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaved_log_probs\u001b[38;5;241m.\u001b[39mappend(log_prob)\n",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m, in \u001b[0;36mQuantumPolicy.sample\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    Samples an action from the action probability distribution\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(inputs)\n\u001b[0;32m     13\u001b[0m     dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(policy)\n\u001b[0;32m     14\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "Cell \u001b[1;32mIn[10], line 21\u001b[0m, in \u001b[0;36mQuantumPolicy.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    Input state is fed to the circuit - its output is then fed to the post processing \u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcircuit\u001b[38;5;241m.\u001b[39mforward(inputs)\n\u001b[0;32m     22\u001b[0m     probs_processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_processing\u001b[38;5;241m.\u001b[39mforward(probs)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m probs_processed\n",
      "Cell \u001b[1;32mIn[6], line 79\u001b[0m, in \u001b[0;36mUQC.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m''' \u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcircuit(inputs)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\qnn\\torch.py:402\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    399\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(inputs, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    401\u001b[0m \u001b[38;5;66;03m# calculate the forward pass as usual\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate_qnode(inputs)\n\u001b[0;32m    404\u001b[0m \u001b[38;5;66;03m# reshape to the correct number of batch dims\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_batch_dim:\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\qnn\\torch.py:423\u001b[0m, in \u001b[0;36mTorchLayer._evaluate_qnode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluates the QNode for a single input datapoint.\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m    tensor: output datapoint\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    419\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_arg: x},\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{arg: weight\u001b[38;5;241m.\u001b[39mto(x) \u001b[38;5;28;01mfor\u001b[39;00m arg, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnode_weights\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m    422\u001b[0m }\n\u001b[1;32m--> 423\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnode(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mtype(x\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\workflow\\qnode.py:1048\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1043\u001b[0m         full_transform_program\u001b[38;5;241m.\u001b[39m_set_all_argnums(\n\u001b[0;32m   1044\u001b[0m             \u001b[38;5;28mself\u001b[39m, args, kwargs, argnums\n\u001b[0;32m   1045\u001b[0m         )  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;66;03m# pylint: disable=unexpected-keyword-arg\u001b[39;00m\n\u001b[1;32m-> 1048\u001b[0m res \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1049\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape,),\n\u001b[0;32m   1050\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m   1051\u001b[0m     gradient_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_fn,\n\u001b[0;32m   1052\u001b[0m     interface\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterface,\n\u001b[0;32m   1053\u001b[0m     transform_program\u001b[38;5;241m=\u001b[39mfull_transform_program,\n\u001b[0;32m   1054\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m   1055\u001b[0m     gradient_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_kwargs,\n\u001b[0;32m   1056\u001b[0m     override_shots\u001b[38;5;241m=\u001b[39moverride_shots,\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_kwargs,\n\u001b[0;32m   1058\u001b[0m )\n\u001b[0;32m   1060\u001b[0m res \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# convert result to the interface in case the qfunc has no parameters\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\workflow\\execution.py:684\u001b[0m, in \u001b[0;36mexecute\u001b[1;34m(tapes, device, gradient_fn, interface, transform_program, config, grad_on_execution, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform, device_vjp)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;66;03m# Exiting early if we do not need to deal with an interface boundary\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_interface_boundary_required:\n\u001b[1;32m--> 684\u001b[0m     results \u001b[38;5;241m=\u001b[39m inner_execute(tapes)\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m post_processing(results)\n\u001b[0;32m    687\u001b[0m _grad_on_execution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\workflow\\execution.py:283\u001b[0m, in \u001b[0;36m_make_inner_execute.<locals>.inner_execute\u001b[1;34m(tapes, **_)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_only:\n\u001b[0;32m    282\u001b[0m     tapes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(qml\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mconvert_to_numpy_parameters(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tapes)\n\u001b[1;32m--> 283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached_device_execution(tapes)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\workflow\\execution.py:361\u001b[0m, in \u001b[0;36mcache_execute.<locals>.wrapper\u001b[1;34m(tapes, **kwargs)\u001b[0m\n\u001b[0;32m    354\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(cache, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache):\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;66;03m# No caching. Simply execute the execution function\u001b[39;00m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;66;03m# and return the results.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \n\u001b[0;32m    360\u001b[0m     \u001b[38;5;66;03m# must convert to list as new device interface returns tuples\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(fn(tapes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (res, []) \u001b[38;5;28;01mif\u001b[39;00m return_tuple \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m    364\u001b[0m execution_tapes \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\modifiers\\simulator_tracking.py:30\u001b[0m, in \u001b[0;36m_track_execute.<locals>.execute\u001b[1;34m(self, circuits, execution_config)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(untracked_execute)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, circuits, execution_config\u001b[38;5;241m=\u001b[39mDefaultExecutionConfig):\n\u001b[1;32m---> 30\u001b[0m     results \u001b[38;5;241m=\u001b[39m untracked_execute(\u001b[38;5;28mself\u001b[39m, circuits, execution_config)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(circuits, QuantumScript):\n\u001b[0;32m     32\u001b[0m         batch \u001b[38;5;241m=\u001b[39m (circuits,)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\modifiers\\single_tape_support.py:32\u001b[0m, in \u001b[0;36m_make_execute.<locals>.execute\u001b[1;34m(self, circuits, execution_config)\u001b[0m\n\u001b[0;32m     30\u001b[0m     is_single_circuit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     circuits \u001b[38;5;241m=\u001b[39m (circuits,)\n\u001b[1;32m---> 32\u001b[0m results \u001b[38;5;241m=\u001b[39m batch_execute(\u001b[38;5;28mself\u001b[39m, circuits, execution_config)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m is_single_circuit \u001b[38;5;28;01melse\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\default_qubit.py:553\u001b[0m, in \u001b[0;36mDefaultQubit.execute\u001b[1;34m(self, circuits, execution_config)\u001b[0m\n\u001b[0;32m    547\u001b[0m interface \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    548\u001b[0m     execution_config\u001b[38;5;241m.\u001b[39minterface\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m execution_config\u001b[38;5;241m.\u001b[39mgradient_method \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackprop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    551\u001b[0m )\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m    554\u001b[0m         simulate(\n\u001b[0;32m    555\u001b[0m             c,\n\u001b[0;32m    556\u001b[0m             rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng,\n\u001b[0;32m    557\u001b[0m             prng_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prng_key,\n\u001b[0;32m    558\u001b[0m             debugger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debugger,\n\u001b[0;32m    559\u001b[0m             interface\u001b[38;5;241m=\u001b[39minterface,\n\u001b[0;32m    560\u001b[0m             state_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_cache,\n\u001b[0;32m    561\u001b[0m         )\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits\n\u001b[0;32m    563\u001b[0m     )\n\u001b[0;32m    565\u001b[0m vanilla_circuits \u001b[38;5;241m=\u001b[39m [convert_to_numpy_parameters(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits]\n\u001b[0;32m    566\u001b[0m seeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng\u001b[38;5;241m.\u001b[39mintegers(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m31\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vanilla_circuits))\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\default_qubit.py:554\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    547\u001b[0m interface \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    548\u001b[0m     execution_config\u001b[38;5;241m.\u001b[39minterface\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m execution_config\u001b[38;5;241m.\u001b[39mgradient_method \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackprop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    551\u001b[0m )\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m--> 554\u001b[0m         simulate(\n\u001b[0;32m    555\u001b[0m             c,\n\u001b[0;32m    556\u001b[0m             rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng,\n\u001b[0;32m    557\u001b[0m             prng_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prng_key,\n\u001b[0;32m    558\u001b[0m             debugger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debugger,\n\u001b[0;32m    559\u001b[0m             interface\u001b[38;5;241m=\u001b[39minterface,\n\u001b[0;32m    560\u001b[0m             state_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_cache,\n\u001b[0;32m    561\u001b[0m         )\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits\n\u001b[0;32m    563\u001b[0m     )\n\u001b[0;32m    565\u001b[0m vanilla_circuits \u001b[38;5;241m=\u001b[39m [convert_to_numpy_parameters(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits]\n\u001b[0;32m    566\u001b[0m seeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng\u001b[38;5;241m.\u001b[39mintegers(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m31\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vanilla_circuits))\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\qubit\\simulate.py:260\u001b[0m, in \u001b[0;36msimulate\u001b[1;34m(circuit, rng, prng_key, debugger, interface, state_cache)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m circuit\u001b[38;5;241m.\u001b[39mshots \u001b[38;5;129;01mand\u001b[39;00m has_mid_circuit_measurements(circuit):\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m simulate_native_mcm(\n\u001b[0;32m    258\u001b[0m         circuit, rng\u001b[38;5;241m=\u001b[39mrng, prng_key\u001b[38;5;241m=\u001b[39mprng_key, debugger\u001b[38;5;241m=\u001b[39mdebugger, interface\u001b[38;5;241m=\u001b[39minterface\n\u001b[0;32m    259\u001b[0m     )\n\u001b[1;32m--> 260\u001b[0m state, is_state_batched \u001b[38;5;241m=\u001b[39m get_final_state(circuit, debugger\u001b[38;5;241m=\u001b[39mdebugger, interface\u001b[38;5;241m=\u001b[39minterface)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m     state_cache[circuit\u001b[38;5;241m.\u001b[39mhash] \u001b[38;5;241m=\u001b[39m state\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\qubit\\simulate.py:139\u001b[0m, in \u001b[0;36mget_final_state\u001b[1;34m(circuit, debugger, interface, mid_measurements)\u001b[0m\n\u001b[0;32m    137\u001b[0m is_state_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(prep \u001b[38;5;129;01mand\u001b[39;00m prep\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m circuit\u001b[38;5;241m.\u001b[39moperations[\u001b[38;5;28mbool\u001b[39m(prep) :]:\n\u001b[1;32m--> 139\u001b[0m     state \u001b[38;5;241m=\u001b[39m apply_operation(\n\u001b[0;32m    140\u001b[0m         op,\n\u001b[0;32m    141\u001b[0m         state,\n\u001b[0;32m    142\u001b[0m         is_state_batched\u001b[38;5;241m=\u001b[39mis_state_batched,\n\u001b[0;32m    143\u001b[0m         debugger\u001b[38;5;241m=\u001b[39mdebugger,\n\u001b[0;32m    144\u001b[0m         mid_measurements\u001b[38;5;241m=\u001b[39mmid_measurements,\n\u001b[0;32m    145\u001b[0m     )\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;66;03m# Handle postselection on mid-circuit measurements\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op, qml\u001b[38;5;241m.\u001b[39mProjector):\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\functools.py:909\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    907\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dispatch(args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\qubit\\apply_operation.py:204\u001b[0m, in \u001b[0;36mapply_operation\u001b[1;34m(op, state, is_state_batched, debugger, **_)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;129m@singledispatch\u001b[39m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_operation\u001b[39m(\n\u001b[0;32m    154\u001b[0m     op: qml\u001b[38;5;241m.\u001b[39moperation\u001b[38;5;241m.\u001b[39mOperator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_,\n\u001b[0;32m    159\u001b[0m ):\n\u001b[0;32m    160\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply and operator to a given state.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m \n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _apply_operation_default(op, state, is_state_batched, debugger)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\qubit\\apply_operation.py:214\u001b[0m, in \u001b[0;36m_apply_operation_default\u001b[1;34m(op, state, is_state_batched, debugger)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The default behaviour of apply_operation, accessed through the standard dispatch\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03mof apply_operation, as well as conditionally in other dispatches.\"\"\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28mlen\u001b[39m(op\u001b[38;5;241m.\u001b[39mwires) \u001b[38;5;241m<\u001b[39m EINSUM_OP_WIRECOUNT_PERF_THRESHOLD\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m math\u001b[38;5;241m.\u001b[39mndim(state) \u001b[38;5;241m<\u001b[39m EINSUM_STATE_WIRECOUNT_PERF_THRESHOLD\n\u001b[0;32m    213\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m (op\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mand\u001b[39;00m is_state_batched):\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m apply_operation_einsum(op, state, is_state_batched\u001b[38;5;241m=\u001b[39mis_state_batched)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m apply_operation_tensordot(op, state, is_state_batched\u001b[38;5;241m=\u001b[39mis_state_batched)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\devices\\qubit\\apply_operation.py:74\u001b[0m, in \u001b[0;36mapply_operation_einsum\u001b[1;34m(op, state, is_state_batched)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_operation_einsum\u001b[39m(op: qml\u001b[38;5;241m.\u001b[39moperation\u001b[38;5;241m.\u001b[39mOperator, state, is_state_batched: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``Operator`` to ``state`` using ``einsum``. This is more efficent at lower qubit\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    numbers.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m        array[complex]: output_state\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m     mat \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39mmatrix()\n\u001b[0;32m     76\u001b[0m     total_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(state\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m-\u001b[39m is_state_batched\n\u001b[0;32m     77\u001b[0m     num_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(op\u001b[38;5;241m.\u001b[39mwires)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\operation.py:791\u001b[0m, in \u001b[0;36mOperator.matrix\u001b[1;34m(self, wire_order)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatrix\u001b[39m(\u001b[38;5;28mself\u001b[39m, wire_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    772\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Representation of the operator as a matrix in the computational basis.\u001b[39;00m\n\u001b[0;32m    773\u001b[0m \n\u001b[0;32m    774\u001b[0m \u001b[38;5;124;03m    If ``wire_order`` is provided, the numerical representation considers the position of the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;124;03m        tensor_like: matrix representation\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m     canonical_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_matrix(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters)\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    794\u001b[0m         wire_order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwires \u001b[38;5;241m==\u001b[39m Wires(wire_order)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    799\u001b[0m         )\n\u001b[0;32m    800\u001b[0m     ):\n\u001b[0;32m    801\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m canonical_matrix\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\pennylane\\ops\\qubit\\parametric_ops_single_qubit.py:99\u001b[0m, in \u001b[0;36mRX.compute_matrix\u001b[1;34m(theta)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_matrix\u001b[39m(theta):  \u001b[38;5;66;03m# pylint: disable=arguments-differ\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Representation of the operator as a canonical matrix in the computational basis (static method).\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    The canonical matrix is the textbook matrix representation that does not consider wires.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m            [0.0000-0.2474j, 0.9689+0.0000j]])\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     c \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mcos(theta \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    100\u001b[0m     s \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39msin(theta \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mget_interface(theta) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\autoray\\autoray.py:30\u001b[0m, in \u001b[0;36mdo\u001b[1;34m(fn, like, *args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m signature\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict, defaultdict\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo\u001b[39m(fn, \u001b[38;5;241m*\u001b[39margs, like\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Do function named ``fn`` on ``(*args, **kwargs)``, peforming single\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    dispatch to retrieve ``fn`` based on whichever library defines the class of\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m    the ``args[0]``, or the ``like`` keyword argument if specified.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m        <tf.Tensor: id=91, shape=(3, 3), dtype=float32>\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     backend \u001b[38;5;241m=\u001b[39m choose_backend(fn, \u001b[38;5;241m*\u001b[39margs, like\u001b[38;5;241m=\u001b[39mlike, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_qubits = 1\n",
    "n_layers = 4\n",
    "state_dim = 4\n",
    "shots = None\n",
    "diff_method = 'best' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval_global\n",
    "policy_circuit = UQC(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     weight_init, policy_circuit_measure)\n",
    "\n",
    "#value_circuit_measure = one_measure_expval_global\n",
    "#value_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "#                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "#                     input_scaling, input_init, weight_init, value_circuit_measure)\n",
    "\n",
    "n_actions = 2\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = False\n",
    "beta = 1\n",
    "increase_rate = 0.003\n",
    "output_scaling = False\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                         post_processing, \n",
    "                         beta_scheduling, \n",
    "                         beta, increase_rate, \n",
    "                         output_scaling, \n",
    "                         output_init)\n",
    "\n",
    "policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "value_lr_list = [0.01, 0.1]\n",
    "#value_params = list(value_circuit.parameters())\n",
    "#value_optimizer = create_optimizer_with_lr(value_params, value_lr_list)\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "n_episodes = 5000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 10\n",
    "verbose = 1\n",
    "reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "#reinforce_update = ReinforceBatchAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "#reinforce_update = ReinforceSimpleBaselineAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "#reinforce_update = ReinforceDynamicBaselineAgent(policy, policy_optimizer, value_circuit, value_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFQ Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 1\n",
    "shots = None\n",
    "diff_method = 'best' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"ring\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = torch.nn.init.ones_\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval_global\n",
    "policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "policy_circuit.visualize_circuit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 1\n",
    "shots = None\n",
    "diff_method = 'best' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"ring\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = torch.nn.init.ones_\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval_global\n",
    "policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "value_circuit_measure = one_measure_expval_global\n",
    "value_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     input_scaling, input_init, weight_init, value_circuit_measure)\n",
    "\n",
    "n_actions = 2\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = True\n",
    "beta = 1\n",
    "increase_rate = 0.0005\n",
    "output_scaling = False\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                         post_processing, \n",
    "                         beta_scheduling, \n",
    "                         beta, increase_rate, \n",
    "                         output_scaling, \n",
    "                         output_init)\n",
    "\n",
    "policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "value_lr_list = [0.01, 0.1]\n",
    "value_params = list(value_circuit.parameters())\n",
    "value_optimizer = create_optimizer_with_lr(value_params, value_lr_list)\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "n_episodes = 5000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 10\n",
    "verbose = 1\n",
    "reinforce_update = ReinforceAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "#reinforce_update = ReinforceBatchAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "#reinforce_update = ReinforceSimpleBaselineAgent(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "#reinforce_update = ReinforceDynamicBaselineAgent(policy, policy_optimizer, value_circuit, value_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../../data\"\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, data_directory, True)\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 1\n",
    "    n_layers = 1\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval_global\n",
    "    policy_circuit = UQC(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    #value_circuit_measure = one_measure_expval_global\n",
    "    #value_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "    #                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "    #                     input_scaling, input_init, weight_init, value_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = False\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "    value_lr_list = [0.01, 0.1]\n",
    "    #value_params = list(value_circuit.parameters())\n",
    "    #value_optimizer = create_optimizer_with_lr(value_params, value_lr_list)\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 5000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../../data\"\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, data_directory, True)\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 1\n",
    "    n_layers = 2\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval_global\n",
    "    policy_circuit = UQC(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    #value_circuit_measure = one_measure_expval_global\n",
    "    #value_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "    #                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "    #                     input_scaling, input_init, weight_init, value_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = False\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "    value_lr_list = [0.01, 0.1]\n",
    "    #value_params = list(value_circuit.parameters())\n",
    "    #value_optimizer = create_optimizer_with_lr(value_params, value_lr_list)\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 5000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../../data\"\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, data_directory, True)\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 1\n",
    "    n_layers = 1\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = torch.nn.init.ones_\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval_global\n",
    "    policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                                 entanglement, entanglement_pattern, entanglement_gate, input_scaling,\n",
    "                                 input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    #value_circuit_measure = one_measure_expval_global\n",
    "    #value_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "    #                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "    #                     input_scaling, input_init, weight_init, value_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = False\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "    value_lr_list = [0.01, 0.1]\n",
    "    #value_params = list(value_circuit.parameters())\n",
    "    #value_optimizer = create_optimizer_with_lr(value_params, value_lr_list)\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 5000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../../data\"\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, data_directory, True)\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 1\n",
    "    n_layers = 2\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval_global\n",
    "    policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                                 entanglement, entanglement_pattern, entanglement_gate, input_scaling,\n",
    "                                 input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    #value_circuit_measure = one_measure_expval_global\n",
    "    #value_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "    #                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "    #                     input_scaling, input_init, weight_init, value_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = False\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "    value_lr_list = [0.01, 0.1]\n",
    "    #value_params = list(value_circuit.parameters())\n",
    "    #value_optimizer = create_optimizer_with_lr(value_params, value_lr_list)\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 5000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../../data\"\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, data_directory, True)\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 1\n",
    "    n_layers = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = torch.nn.init.ones_\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval_global\n",
    "    policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                                 entanglement, entanglement_pattern, entanglement_gate, input_scaling,\n",
    "                                 input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    #value_circuit_measure = one_measure_expval_global\n",
    "    #value_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "    #                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "    #                     input_scaling, input_init, weight_init, value_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = False\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "    value_lr_list = [0.01, 0.1]\n",
    "    #value_params = list(value_circuit.parameters())\n",
    "    #value_optimizer = create_optimizer_with_lr(value_params, value_lr_list)\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 5000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyPostProcessing(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_actions, policy_type = 'raw_contiguous', \n",
    "                 beta_scheduling = False, beta = 1, increase_rate = 0.0005, \n",
    "                 output_scaling = False, output_init = torch.nn.init.ones_):\n",
    "        super(PolicyPostProcessing, self).__init__()\n",
    "\n",
    "        '''\n",
    "        Determines the type of policy used based on the arguments:\n",
    "\n",
    "            n_actions(int) = Number of actions\n",
    "            post_processing(str) = Type of policy ('raw_contiguous', 'raw_parity', 'softmax')\n",
    "            beta_scheduling(bool) = Inverse temperature parameter used in softmax (used if set to True)\n",
    "            beta(float) = Beta parameter or inverse temperature (used only for softmax)\n",
    "            increase_rate(float) = Amount added to beta at the end of each episode (used only for softmax)\n",
    "            output_scaling(bool) = Output parameters are used if True\n",
    "            output_init(function) = How the output parameters are initialized\n",
    "            \n",
    "        '''\n",
    "        self.n_actions = n_actions\n",
    "        self.policy_type = policy_type\n",
    "        self.beta_scheduling = beta_scheduling\n",
    "        self.beta = beta\n",
    "        self.increase_rate = increase_rate\n",
    "        self.output_scaling = output_scaling\n",
    "        self.output_init = output_init\n",
    "\n",
    "        if self.output_scaling == True:\n",
    "            self.output_params = nn.parameter.Parameter(torch.Tensor(self.n_actions), requires_grad=True)\n",
    "            #self.output_params = nn.parameter.Parameter(torch.Tensor(1), requires_grad=True)\n",
    "            self.output_init(self.output_params)\n",
    "        else:\n",
    "            self.register_parameter('w_input', None)\n",
    "\n",
    "    def forward(self,probs):\n",
    "        if self.policy_type == 'raw_contiguous':\n",
    "            policy = self.raw_contiguous(probs)\n",
    "        elif self.policy_type == 'raw_parity':\n",
    "            policy = self.raw_parity(probs)\n",
    "        elif self.policy_type == 'softmax':\n",
    "            policy = self.softmax(probs)\n",
    "        elif self.policy_type == 'softmax_probs':\n",
    "            policy = self.softmax_probs(probs)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid post-processing method specified.\")\n",
    "        return policy\n",
    "\n",
    "    def raw_contiguous(self,probs):\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        chunk_size = len(probs_flatten) // self.n_actions\n",
    "        remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "        policy = []\n",
    "\n",
    "        for i in range(self.n_actions):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size\n",
    "\n",
    "            if i < remainder:\n",
    "                end += 1\n",
    "\n",
    "            # Update the original policy list instead of creating a new one\n",
    "            policy.append(sum(probs_flatten[start:end]))\n",
    "\n",
    "        policy_tensor = torch.stack(policy)\n",
    "        return policy_tensor\n",
    "        \n",
    "    def raw_parity(self,probs):\n",
    "\n",
    "        if self.n_actions % 2 != 0:\n",
    "            raise ValueError('For parity-like policy, n_actions must be an even number')\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        policy = torch.zeros(self.n_actions)\n",
    "        counter = 0\n",
    "        for prob in probs_flatten:\n",
    "            policy[counter] += prob\n",
    "            counter += 1\n",
    "            if counter == self.n_actions:\n",
    "                counter = 0\n",
    "        \n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    def softmax(self, probs):\n",
    "        \n",
    "        if self.output_scaling == True:\n",
    "            probs *= self.output_params\n",
    "\n",
    "        scaled_output = probs * self.beta\n",
    "        softmax_output = F.softmax(scaled_output, dim=0)\n",
    "        return softmax_output\n",
    "    \n",
    "    def softmax_probs(self, probs):\n",
    "\n",
    "        if self.output_scaling == True:\n",
    "            probs *= self.output_params\n",
    "            \n",
    "        probs_flatten = probs.flatten()\n",
    "        chunk_size = len(probs_flatten) // self.n_actions\n",
    "        remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "        policy = []\n",
    "\n",
    "        for i in range(self.n_actions):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size\n",
    "\n",
    "            if i < remainder:\n",
    "                end += 1\n",
    "\n",
    "            policy.append(sum(probs_flatten[start:end]))\n",
    "        policy_tensor = torch.stack(policy)\n",
    "        softmax_output = F.softmax(policy_tensor * self.beta, dim=0)\n",
    "        return softmax_output\n",
    "    \n",
    "    def beta_schedule(self):\n",
    "        if self.beta_scheduling == True:\n",
    "            if self.policy_type == 'softmax' or self.policy_type == 'softmax_probs':\n",
    "                self.beta += self.increase_rate\n",
    "\n",
    "    @classmethod\n",
    "    def info(cls):\n",
    "        '''\n",
    "        Provides a summary of the PolicyType class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        PolicyType Class:\n",
    "\n",
    "        Determines the type of policy used based on the arguments:\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_actions (int): \n",
    "            Number of actions available for the agent to choose from.\n",
    "        \n",
    "        policy_type (str): \n",
    "            Type of policy applied to the probability distribution:\n",
    "            - 'raw_contiguous': Sums up contiguous chunks of probabilities.\n",
    "            - 'raw_parity': Sums up probabilities in a circular manner based on parity.\n",
    "            - 'softmax': Applies a softmax function to the scaled probabilities.\n",
    "            - 'softmax_probs': Sums up contiguous chunks of probabilities and then applies softmax.\n",
    "        \n",
    "        beta_scheduling (bool): \n",
    "            If True, updates the inverse temperature parameter (beta) after each episode. Used only for 'softmax' and 'softmax_probs'.\n",
    "        \n",
    "        beta (float): \n",
    "            Inverse temperature parameter used for scaling probabilities in the softmax policy.\n",
    "        \n",
    "        increase_rate (float): \n",
    "            Amount added to beta at the end of each episode, if beta_scheduling is True.\n",
    "        \n",
    "        output_scaling (bool): \n",
    "            If True, scales the output probabilities by learnable parameters.\n",
    "        \n",
    "        output_init (function): \n",
    "            Initialization function for output parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, etc.\n",
    "        \n",
    "        Methods:\n",
    "        -------\n",
    "        sample(probs):\n",
    "            Selects an action based on the chosen post_processing method.\n",
    "        \n",
    "        raw_contiguous(probs):\n",
    "            Sums up contiguous chunks of probabilities and returns the resulting tensor.\n",
    "        \n",
    "        raw_parity(probs):\n",
    "            Sums up probabilities in a circular manner based on parity and returns the action, log probability, and policy tensor.\n",
    "        \n",
    "        softmax(probs):\n",
    "            Applies a softmax function to the scaled probabilities and returns the action, log probability, and policy tensor.\n",
    "        \n",
    "        softmax_probs(probs):\n",
    "            Sums up contiguous chunks of probabilities, applies softmax, and returns the action, log probability, and policy tensor.\n",
    "        \n",
    "        beta_schedule():\n",
    "            Updates the beta parameter if beta_scheduling is True. Only applicable for 'softmax' and 'softmax_probs' methods.\n",
    "        \"\"\"\n",
    "        print(info_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m rundate \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH.\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM.\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)      \n\u001b[0;32m     62\u001b[0m num_agents \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m---> 64\u001b[0m results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mnum_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_agents))\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\Users\\Bernardo\\anaconda3\\Lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_directory = \"../../data\"\n",
    "\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, data_directory, True)\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = torch.nn.init.ones_\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval_global\n",
    "    policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                                 entanglement, entanglement_pattern, entanglement_gate, input_scaling,\n",
    "                                 input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    #value_circuit_measure = one_measure_expval_global\n",
    "    #value_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "    #                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "    #                     input_scaling, input_init, weight_init, value_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = False\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "    value_lr_list = [0.01, 0.1]\n",
    "    #value_params = list(value_circuit.parameters())\n",
    "    #value_optimizer = create_optimizer_with_lr(value_params, value_lr_list)\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 5000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, print_every, verbose, i, rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "import optuna\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(policy, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(policy, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "def objective_function(results):\n",
    "\n",
    "    results_mean = np.mean(results, axis=0)\n",
    "    area = np.abs(np.trapz(results_mean))\n",
    "    maximum_performance_area = float(len(results[0]) * 200)\n",
    "\n",
    "    # Create a metric called performance area and normalize it between 0 and 1\n",
    "    performance_area = area / maximum_performance_area\n",
    "    return performance_area\n",
    "\n",
    "'''\n",
    "def sum_and_average(results):\n",
    "\n",
    "    averages = np.mean(results, axis=1)\n",
    "    return np.mean(averages)\n",
    "'''\n",
    "\n",
    "def objective(trial):    \n",
    "\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling, beta, increase_rate = True, 1, 0.005\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr1 = trial.suggest_float(\"lr1\", 1e-5, 1e-1, log=True)\n",
    "    #lr2 = trial.suggest_float(\"lr2\", 1e-5, 1e-1, log=True)\n",
    "    #lr3 = trial.suggest_float(\"lr3\", 1e-5, 1e-1, log=True)\n",
    "    lr2, lr3 = 0.1, 0.1\n",
    "    lr_list= [lr1, lr2, lr3]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 10\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 1\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 5\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, trial.number) for i in range(num_agents))\n",
    "    performance_metric = objective_function(results)\n",
    "    #performance_metric = sum_and_average(results)\n",
    "    return performance_metric\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    print(f\"Best trial: {study.best_trial.value}\")\n",
    "    print(f\"Best parameters: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_output = F.softmax(torch.Tensor([0.1,0.9]) * 3, dim=0)\n",
    "print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
