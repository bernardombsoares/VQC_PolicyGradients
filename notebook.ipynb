{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pennylane as qml \n",
    "import torch\n",
    "\n",
    "def create_circuit(n_qubits,n_layers=None,circ = \"simplified_two_design\",fim=False, shots=None):\n",
    "\n",
    "    dev = qml.device(\"default.qubit.torch\", wires=n_qubits, shots=shots)\n",
    "\n",
    "    def RZRY(params):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        #qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        #qml.AngleEmbedding(params,wires=range(n_qubits))\n",
    "        for q in range(n_qubits):\n",
    "            qml.Hadamard(wires=q)\n",
    "\n",
    "        for w in range(n_layers): \n",
    "            for q in range(n_qubits):\n",
    "                index = w * (2*n_qubits) + q * 2\n",
    "                qml.RZ(params[index],wires=q)\n",
    "                qml.RY(params[index + 1],wires=q)\n",
    "        \n",
    "        qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        return qml.probs(wires=range(n_qubits))\n",
    "\n",
    "    def S2D(init_params,params,measurement_qubits=0,prod_approx=False):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        \n",
    "        #qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        if not prod_approx:\n",
    "            return qml.probs(wires=list(range(measurement_qubits)))\n",
    "        else:\n",
    "            return [qml.probs(i) for i in range(measurement_qubits)]\n",
    "\n",
    "    def SU(params):\n",
    "        qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        \n",
    "        ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "        for i in range(2,n_qubits):\n",
    "            ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "        return qml.expval(ZZ)\n",
    "    \n",
    "    def simmpleRZRY(params,cnots=True):\n",
    "        qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RZ, wires=range(n_qubits), pattern=\"single\", parameters=params[0])\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params[1])\n",
    "        if cnots:\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "            return qml.expval(qml.PauliZ(n_qubits-1))\n",
    "        else:\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "            return qml.expval(ZZ)\n",
    "        \n",
    "    def RY(params,y=True,probs=False,prod=False, entanglement=None):\n",
    "        #qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params)\n",
    "        #qml.broadcast(qml.CZ, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "\n",
    "        if entanglement==\"all_to_all\":\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        if y==True:\n",
    "            #YY = qml.operation.Tensor(qml.PauliY(0), qml.PauliY(1))\n",
    "            YY = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                #YY = qml.operation.Tensor(YY, qml.PauliY(i))\n",
    "                YY.append(qml.PauliZ(i))\n",
    "            \n",
    "            #return [qml.expval(i) for i in YY]\n",
    "            return qml.expval(YY)\n",
    "\n",
    "        elif probs==False:\n",
    "\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            #ZZ = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))        \n",
    "                #ZZ.append(qml.PauliZ(i))        \n",
    "\n",
    "            #return [qml.expval(i) for i in ZZ]\n",
    "            return qml.expval(ZZ)\n",
    "\n",
    "        else:\n",
    "            if prod:\n",
    "                return [qml.probs(i) for i in range(n_qubits)]\n",
    "            else:\n",
    "                return qml.probs(wires=range(n_qubits))\n",
    "            \n",
    "        \n",
    "        \n",
    "    def GHZ(params,measurement_qubits=0):\n",
    "        qml.RY(params,wires=0)\n",
    "        qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "\n",
    "    def random_product_state(params,gate_sequence=None):\n",
    "                \n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(np.pi / 4, wires=i)\n",
    "\n",
    "        for ll in range(len(params)):\n",
    "\n",
    "            for i in range(n_qubits):\n",
    "                gate_sequence[\"{}{}\".format(ll,i)](params[ll][i], wires=i)\n",
    "\n",
    "            #for i in range(n_qubits - 1):\n",
    "                #qml.CZ(wires=[i, i + 1])\n",
    "    def SEL(params, measurement_qubits=0):\n",
    "        qml.StronglyEntanglingLayers(params, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    def RL(params, measurement_qubits=0):\n",
    "        qml.RandomLayers(params, ratio_imprim=0.8 ,imprimitive=qml.CZ, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    if circ == \"rzry\":\n",
    "        qcircuit = RZRY\n",
    "    elif circ == \"simplified_two_design\":\n",
    "        qcircuit = S2D\n",
    "    elif circ == \"special_unitary\":\n",
    "        qcircuit = SU\n",
    "    elif circ == \"simpleRZRY\":\n",
    "        qcircuit = simmpleRZRY\n",
    "    elif circ == \"RY\":\n",
    "        qcircuit = RY\n",
    "    elif circ == \"ghz\":\n",
    "        qcircuit = GHZ\n",
    "    elif circ == \"random_product_state\":\n",
    "        qcircuit = random_product_state\n",
    "    elif circ == \"SEL\":\n",
    "        qcircuit = SEL\n",
    "    elif circ == \"RL\":\n",
    "        qcircuit = RL\n",
    "    if not fim:\n",
    "        circuit = qml.QNode(qcircuit, dev,interface=\"torch\", diff_method=\"backprop\")\n",
    "    else:\n",
    "        circuit = qml.QNode(qcircuit, dev)\n",
    "\n",
    "    return circuit\n",
    "\n",
    "def compute_gradient(log_prob, w):\n",
    "    \"\"\"Compute gradient of the log probability with respect to weights.\n",
    "    \n",
    "    Args:\n",
    "    - log_prob (torch.Tensor): The log probability tensor.\n",
    "    - w (torch.Tensor): The weights tensor, with requires_grad=True.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The gradient of log_prob with respect to w, flattened.\n",
    "    \"\"\"\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()\n",
    "    log_prob.backward(retain_graph=True)\n",
    "    \n",
    "    if w.grad is None:\n",
    "        raise ValueError(\"The gradient for the given log_prob with respect to w is None.\")\n",
    "    \n",
    "    return w.grad.view(-1).detach().numpy()\n",
    "\n",
    "def policy(probs, policy_type=\"contiguous-like\", n_actions=2, n_qubits=1):\n",
    "\n",
    "    if policy_type == \"contiguous-like\":\n",
    "        return probs\n",
    "    elif policy_type == \"parity-like\":\n",
    "        policy = torch.zeros(n_actions)\n",
    "        for i in range(len(probs)):\n",
    "            a=[]\n",
    "            for m in range(int(np.log2(n_actions))):\n",
    "                if m==0:    \n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)\n",
    "                else:\n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)[:-m]\n",
    "                \n",
    "                a.append(bitstring.count(\"1\") % 2)\n",
    "            policy[int(\"\".join(str(x) for x in a),2)] += probs[i]\n",
    "\n",
    "        return policy    \n",
    "    \n",
    "def compute_policy_and_gradient(args):\n",
    "    n_qubits, shapes, type , n_actions, policy_type, clamp = args\n",
    "\n",
    "    if policy_type == \"parity-like\":\n",
    "        measure_qubits = n_qubits\n",
    "    else:\n",
    "        measure_qubits = int(np.log2(n_actions))\n",
    "\n",
    "    qc = create_circuit(n_qubits, circ=type, fim=False, shots=None)\n",
    "\n",
    "    if type == \"simplified_two_design\":\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_init = torch.tensor(weights[0], requires_grad=False)\n",
    "        weights_tensor_params = torch.tensor(weights[1], requires_grad=True)\n",
    "        \n",
    "        probs = qc(weights_tensor_init,weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    else:\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_params = torch.tensor(weights, requires_grad=True)\n",
    "\n",
    "        probs = qc(weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    pi = policy(probs, policy_type=policy_type, n_actions=n_actions, n_qubits=n_qubits)\n",
    "    if clamp is not None:\n",
    "        pi = torch.clamp(pi, clamp, 1)\n",
    "\n",
    "    dist = torch.distributions.Categorical(probs=pi)\n",
    "    \n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    gradient_no_clamp = np.linalg.norm(compute_gradient(log_prob, weights_tensor_params), 2)\n",
    "    return gradient_no_clamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, env, n_episodes=1000, max_t=1000, gamma=1.0, print_every=5):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    average_scores = []\n",
    "    runtime_sum = 0\n",
    "    for e in range(1, n_episodes):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Sample the action from current policy\n",
    "            if t==0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = policy.sample(state_tensor)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # Total expected reward\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "    #standardized returns\n",
    "        R=0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0,R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + np.finfo(np.float32).eps)\n",
    "\n",
    "        for log_prob, R in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        policy_sum = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "    # Backpropagation\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        policy_sum.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        runtime = end_time-start_time\n",
    "        \n",
    "        runtime_sum += runtime\n",
    "        if e % print_every == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tLast {}\\tEpisodes average reward: {:.2f}\\tRuntime: {:.2f}'.format(e, scores_deque[-1], print_every, np.mean(scores_deque), runtime_sum))\n",
    "            runtime_sum = 0\n",
    "        if np.mean(scores_deque) == 500:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores, policy.gradient_list, average_scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def save_training_data(self):\n",
    "        ''' \n",
    "        Saves training data into json files\n",
    "        '''\n",
    "        current_directory = os.path.dirname(__file__)\n",
    "        folder_name = f\"{str(self.env_name)}_{self.policy.policy.post_processing}_{self.policy.circuit.n_layers}\"\n",
    "        folder_path = os.path.join(current_directory, folder_name)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        episode_data = [self.scores_deque, \n",
    "                        self.runtime, \n",
    "                        self.loss.item(), \n",
    "                        tensor_to_list(self.policy.get_gradients()[0]), \n",
    "                        tensor_to_list(self.policy.get_gradients()[1])]\n",
    "\n",
    "        if folder_path is not None:\n",
    "            file_path = os.path.join(self.folder_path, f\"{self.file_name}.json\")\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, 'r') as f:\n",
    "                    existing_data = json.load(f)\n",
    "                existing_data.append(episode_data)\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump(existing_data, f, indent=4)\n",
    "            else:\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump([episode_data], f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "def create_directory(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def tensor_to_list(tensor):\n",
    "    \"\"\"\n",
    "    Convert a tensor or numpy array to a nested list.\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, list):\n",
    "        return [tensor_to_list(t) for t in tensor]\n",
    "    elif isinstance(tensor, dict):\n",
    "        return {key: tensor_to_list(value) for key, value in tensor.items()}\n",
    "    elif isinstance(tensor, np.ndarray):\n",
    "        return tensor.tolist()\n",
    "    elif isinstance(tensor, torch.Tensor):\n",
    "        return tensor.tolist()\n",
    "    else:\n",
    "        return tensor\n",
    "    \n",
    "def create_optimizer_with_lr(params, lr_list, use_amsgrad=False):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': p, 'lr': lr} for p, lr in zip(params, lr_list)\n",
    "    ], amsgrad=use_amsgrad)\n",
    "    return optimizer\n",
    "\n",
    "def get_function_representation(func):\n",
    "    if callable(func):\n",
    "        # Check if the function is a lambda\n",
    "        if func.__name__ == \"<lambda>\":\n",
    "            # Optionally, check if the function has a custom description attribute\n",
    "            return f\"{func.__module__}.<lambda>\" + (getattr(func, 'description', ''))\n",
    "        else:\n",
    "            return f\"{func.__module__}.{func.__name__}\"\n",
    "    return \"Unknown Function Type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measures\n",
    "\n",
    "def measure_probs(qubits):\n",
    "    return qml.probs(wires=range(qubits)) \n",
    "\n",
    "def one_measure_expval(qubits):\n",
    "\n",
    "    pauli_string = qml.PauliZ(0)\n",
    "    for i in range(1, qubits):\n",
    "        pauli_string = pauli_string @ qml.PauliZ(i)\n",
    "    \n",
    "    expvals = []\n",
    "    expvals.append(qml.expval(pauli_string))\n",
    "\n",
    "    return expvals\n",
    "\n",
    "def two_measure_expval(qubits):\n",
    "\n",
    "    pauli_string = qml.PauliZ(0)\n",
    "    for i in range(1, qubits):\n",
    "        pauli_string = pauli_string @ qml.PauliZ(i)\n",
    "    \n",
    "    expvals = []\n",
    "    expvals.append(qml.expval(pauli_string))\n",
    "    expvals.append(qml.expval(-pauli_string))\n",
    "\n",
    "    return expvals\n",
    "\n",
    "def three_measure_expval(qubits):\n",
    "\n",
    "    z0 = qml.PauliZ(0)\n",
    "    z1 = qml.PauliZ(1)\n",
    "    z0z1 = z0 @ z1\n",
    "\n",
    "    expvals = []\n",
    "    expvals.append(qml.expval(z0))   \n",
    "    expvals.append(qml.expval(z0z1))  \n",
    "    expvals.append(qml.expval(z1))    \n",
    "\n",
    "    return expvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vqc operations\n",
    "\n",
    "def strong_entangling(n_qubits, layer, entanglement_gate):\n",
    "    for qubit in range(n_qubits):\n",
    "        target = (qubit + layer + 1) % n_qubits\n",
    "        if target != qubit:\n",
    "            qml.CZ(wires=[qubit, target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JerbiModel(nn.Module):\n",
    "    '''\n",
    "    Creates a parametrized quantum circuit based on the work in https://doi.org/10.48550/arXiv.2103.05577.\n",
    "\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers, shots = None, diff_method = 'best', entanglement = True,\n",
    "                entanglement_pattern = \"all_to_all\", entanglement_gate = qml.CNOT, input_scaling = True, \n",
    "                input_init = torch.nn.init.ones_, weight_init = torch.nn.init.normal_, measure = two_measure_expval):\n",
    "        super(JerbiModel, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_scaling = input_scaling\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        if self.n_layers < 1:\n",
    "            raise ValueError(\"Number of layers can't take values below 1\")\n",
    "        \n",
    "        self.weight_shapes = {\n",
    "            \"params\": (self.n_layers + 1, self.n_qubits, 2),\n",
    "            \"input_params\": (self.n_layers, self.n_qubits, 2),\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"params\": self.weight_init,\n",
    "            \"input_params\": self.input_init,\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, params, input_params):\n",
    "            # In case the number of qubits is greater than the input size AND it is multiple of the input length, \n",
    "            # then tile input tensor 'multiplier' times\n",
    "            if self.n_qubits > len(inputs) and self.n_qubits % len(inputs) == 0:\n",
    "                multiplier = self.n_qubits // len(inputs)\n",
    "                inputs = torch.cat([inputs] * multiplier)\n",
    "\n",
    "            # If the number of qubits is not equal to the input length and not a multiple of the input length, raise an error\n",
    "            elif self.n_qubits != len(inputs) and self.n_qubits % len(inputs) != 0:\n",
    "                raise ValueError('Number of qubits cannot be divided by input lenght')\n",
    "\n",
    "            # Apply Hadamard gates to every qubits\n",
    "            qml.broadcast(qml.Hadamard, wires=range(self.n_qubits), pattern=\"single\")\n",
    "            \n",
    "            # Iterate through layers\n",
    "            for layer in range(self.n_layers):\n",
    "                # Iterate through qubits\n",
    "                for wire in range(self.n_qubits):\n",
    "                    # Parameterized block\n",
    "                    qml.RZ(params[layer][wire][0], wires=wire)\n",
    "                    qml.RY(params[layer][wire][1], wires=wire)\n",
    "\n",
    "                # Entanglement between qubits\n",
    "                if self.entanglement:\n",
    "                    if self.entanglement_pattern == 'strong_entangling':\n",
    "                        strong_entangling(self.n_qubits, layer, self.entanglement_gate)\n",
    "                    else:\n",
    "                        qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "                # If input scaling set to True, multiply inputs by weights\n",
    "                if self.input_scaling:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RY(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                        qml.RZ(input_params[layer][wire][1] * inputs[wire], wires=wire)\n",
    "                # If input scaling set to False, use only the inputs\n",
    "                else:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RY(inputs[wire], wires=wire)\n",
    "                        qml.RZ(inputs[wire], wires=wire)\n",
    "                \n",
    "            # Last parameterized layer\n",
    "            for wire in range(self.n_qubits):\n",
    "                qml.RZ(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)  \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        \n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        circuit = qml.draw_mpl(self.qnode)(inputs, \n",
    "                                        initialized_params[\"params\"], \n",
    "                                        initialized_params[\"input_params\"])\n",
    "        \n",
    "    def circuit_spectrum(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        res = qml.fourier.circuit_spectrum(self.qnode)(inputs, initialized_params[\"params\"], initialized_params[\"input_params\"])\n",
    "        for inp, freqs in res.items():\n",
    "            print(f\"{inp}: {freqs}\")       \n",
    "\n",
    "        \n",
    "    @classmethod\n",
    "    def info(cls):        \n",
    "        '''\n",
    "        Provides a summary of the JerbiModel class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        Creates the Parameterized Quantum Circuit based on the design in https://doi.org/10.48550/arXiv.2103.05577.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_qubits (int): \n",
    "            Number of qubits used in the quantum circuit.\n",
    "        \n",
    "        n_layers (int): \n",
    "            Number of layers in the quantum circuit. Each layer typically consists of parameterized rotations followed by entanglement gates.\n",
    "        \n",
    "        shots (int, optional): \n",
    "            Number of times the circuit gets executed (repeated measurements). If None, the circuit is executed with analytic calculations (no shot noise).\n",
    "        \n",
    "        diff_method (str): \n",
    "            Differentiation method used for training the model. Common options are 'best', 'parameter-shift', 'backprop', etc.\n",
    "        \n",
    "        entanglement_pattern (str): \n",
    "            Entanglement pattern used in the circuit, such as 'chain', 'ring', 'all_to_all', etc., as defined by qml.broadcast patterns.\n",
    "        \n",
    "        entanglement_gate (function): \n",
    "            Quantum gate used for entanglement, such as qml.CZ or qml.CNOT. This gate will be applied between qubits according to the specified entanglement pattern.\n",
    "        \n",
    "        input_scaling (bool): \n",
    "            If True, input parameters are scaled by additional learnable parameters (input_params). The input is multiplied by these parameters before being applied to the qubits.\n",
    "                \n",
    "        input_init (function): \n",
    "            Function to initialize the input scaling parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, and function defined by the user.\n",
    "        \n",
    "        weight_init (function): \n",
    "            Function to initialize the weights of the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.normal_, and function defined by the user.\n",
    "                \n",
    "        measure (function): \n",
    "            Measurement function that takes the number of qubits as an argument and returns the measurement result. Common choices are measure_probs, measure_expval_pairs, or any user-defined measurement function.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        JerbiModel: \n",
    "            An instance of the JerbiModel class representing the quantum neural network.\n",
    "        \"\"\"\n",
    "        print(info_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UQC_FullEnc(nn.Module):\n",
    "    '''\n",
    "    Creates a parameterized quantum circuit (Universal Quantum Classifier) based in https://doi.org/10.22331/q-2020-02-06-226.\n",
    "\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers, state_dim, shots = None, diff_method = 'best', entanglement = True,\n",
    "                entanglement_pattern = \"all_to_all\", entanglement_gate = qml.CNOT, \n",
    "                weight_init = torch.nn.init.normal_, measure = two_measure_expval):\n",
    "        super(UQC_FullEnc, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.state_dim = state_dim\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "        self.input_scaling = None\n",
    "        self.input_init = None\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        self.weight_shapes = {\n",
    "            \"weights\": (self.n_layers, self.n_qubits, self.state_dim),\n",
    "            \"params\": (self.n_layers, self.n_qubits, 1),\n",
    "            \"bias\": (self.n_layers, self.n_qubits)\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"weights\": partial(torch.nn.init.normal_, mean=0.0, std=0.01),\n",
    "            \"params\": self.weight_init,\n",
    "            \"bias\": torch.nn.init.zeros_\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, weights, params, bias):\n",
    "\n",
    "            # Iterate through layers\n",
    "            for layer in range(self.n_layers):\n",
    "                # Iterate through qubits\n",
    "                for wire in range(self.n_qubits):\n",
    "                    # Compute the Hadamard product, sum with bias, and use it as the angle for R_X\n",
    "                    hadamard_product = torch.dot(inputs.clone().detach(), weights[layer][wire])\n",
    "                    angle = hadamard_product + bias[layer][wire]\n",
    "\n",
    "                    # Apply the Rx gate with computed angle\n",
    "\n",
    "                    qml.RZ(angle, wires=wire)\n",
    "\n",
    "                    qml.RY(params[layer][wire][0], wires=wire)\n",
    "                # Entanglement between qubits\n",
    "                if self.entanglement:\n",
    "                    if self.entanglement_pattern == 'strong_entangling':\n",
    "                        strong_entangling(self.n_qubits, layer)\n",
    "                    else:\n",
    "                        qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(self.qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        \n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.state_dim)], dtype=torch.float32)\n",
    "        \n",
    "        # Initialize all parameters using the provided initialization methods\n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        circuit = qml.draw_mpl(self.qnode)(inputs, \n",
    "                                        initialized_params[\"weights\"], \n",
    "                                        initialized_params[\"params\"], \n",
    "                                        initialized_params[\"bias\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UQC_PartialEnc(nn.Module):\n",
    "    '''\n",
    "    Creates a parameterized quantum circuit (Universal Quantum Classifier) based in https://doi.org/10.22331/q-2020-02-06-226.\n",
    "\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers, state_dim, shots = None, diff_method = 'best', entanglement = True,\n",
    "                entanglement_pattern = \"all_to_all\", entanglement_gate = qml.CNOT, \n",
    "                weight_init = torch.nn.init.normal_, measure = two_measure_expval):\n",
    "        super(UQC_PartialEnc, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.state_dim = state_dim\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "        self.input_scaling = None\n",
    "        self.input_init = None\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        self.weight_shapes = {\n",
    "            \"weights\": (self.n_layers, self.n_qubits, int(self.state_dim/self.n_qubits)),\n",
    "            \"params\": (self.n_layers, self.n_qubits, 1),\n",
    "            \"bias\": (self.n_layers, self.n_qubits)\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"weights\": partial(torch.nn.init.normal_, mean=0.0, std=0.01),\n",
    "            \"params\": self.weight_init,\n",
    "            \"bias\": torch.nn.init.zeros_\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, weights, params, bias):\n",
    "\n",
    "            separate_inputs = np.array_split(inputs,self.n_qubits)\n",
    "            # Iterate through layers\n",
    "            for layer in range(self.n_layers):\n",
    "                # Iterate through qubits\n",
    "                for wire in range(self.n_qubits):\n",
    "                    # Compute the Hadamard product, sum with bias, and use it as the angle for R_X\n",
    "                    hadamard_product = torch.dot(separate_inputs[wire], weights[layer][wire])\n",
    "                    angle = hadamard_product + bias[layer][wire]\n",
    "\n",
    "                    # Apply the Rx gate with computed angle\n",
    "                    qml.RZ(angle, wires=wire)\n",
    "\n",
    "                    qml.RY(params[layer][wire][0], wires=wire)\n",
    "                # Entanglement between qubits\n",
    "                if self.entanglement:\n",
    "                    if self.entanglement_pattern == 'strong_entangling':\n",
    "                        strong_entangling(self.n_qubits, layer)\n",
    "                    else:\n",
    "                        qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(self.qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        \n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.state_dim)], dtype=torch.float32)\n",
    "        \n",
    "        # Initialize all parameters using the provided initialization methods\n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        circuit = qml.draw_mpl(self.qnode)(inputs, \n",
    "                                        initialized_params[\"weights\"], \n",
    "                                        initialized_params[\"params\"], \n",
    "                                        initialized_params[\"bias\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfqTutorial(nn.Module):\n",
    "    '''\n",
    "    Creates a parameterized quantum circuit based on the TensorFlow Quantum tutorial in https://www.tensorflow.org/quantum/tutorials/quantum_reinforcement_learning\n",
    "\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers, shots = None, diff_method = 'best', entanglement = True,\n",
    "                entanglement_pattern = \"ring\", entanglement_gate = qml.CNOT, input_scaling = True, \n",
    "                input_init = torch.nn.init.ones_, weight_init = torch.nn.init.normal_, measure = two_measure_expval):\n",
    "        super(TfqTutorial, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_scaling = input_scaling\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        if self.n_layers < 1:\n",
    "            raise ValueError(\"Number of layers can't take values below 1\")\n",
    "        \n",
    "        self.weight_shapes = {\n",
    "            \"params\": (self.n_layers + 1, self.n_qubits, 3),\n",
    "            \"input_params\": (self.n_layers, self.n_qubits, 1),\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"params\": self.weight_init,\n",
    "            \"input_params\": self.input_init,\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, params, input_params):\n",
    "            # In case the number of qubits is greater than the input size AND it is multiple of the input length, \n",
    "            # then tile input tensor 'multiplier' times\n",
    "            if self.n_qubits > len(inputs) and self.n_qubits % len(inputs) == 0:\n",
    "                multiplier = self.n_qubits // len(inputs)\n",
    "                inputs = torch.cat([inputs] * multiplier)\n",
    "\n",
    "            # If the number of qubits is not equal to the input length and not a multiple of the input length, raise an error\n",
    "            elif self.n_qubits != len(inputs) and self.n_qubits % len(inputs) != 0:\n",
    "                raise ValueError('Number of qubits cannot be divided by input lenght')\n",
    "            \n",
    "            # Iterate through layers\n",
    "            for layer in range(self.n_layers):\n",
    "                # Iterate through qubits\n",
    "                for wire in range(self.n_qubits):\n",
    "                    # Parameterized block\n",
    "                    qml.RX(params[layer][wire][0], wires=wire)\n",
    "                    qml.RY(params[layer][wire][1], wires=wire)\n",
    "                    qml.RZ(params[layer][wire][2], wires=wire)\n",
    "\n",
    "                # Entanglement between qubits\n",
    "                if self.entanglement:\n",
    "                    if self.entanglement_pattern == 'strong_entangling':\n",
    "                        strong_entangling(self.n_qubits, layer)\n",
    "                    else:\n",
    "                        qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "                # If input scaling set to True, multiply inputs by weights\n",
    "                if self.input_scaling:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RX(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                # If input scaling set to False, use only the inputs\n",
    "                else:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RX(inputs[wire], wires=wire)\n",
    "                \n",
    "            # Last parameterized layer\n",
    "            for wire in range(self.n_qubits):\n",
    "                qml.RX(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "                qml.RZ(params[-1][wire][2], wires=wire)\n",
    "\n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)  \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        \n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        circuit = qml.draw_mpl(self.qnode)(inputs, \n",
    "                                        initialized_params[\"params\"], \n",
    "                                        initialized_params[\"input_params\"])\n",
    "        \n",
    "\n",
    "    def circuit_spectrum(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        res = qml.fourier.circuit_spectrum(self.qnode)(inputs, initialized_params[\"params\"], initialized_params[\"input_params\"])\n",
    "        for inp, freqs in res.items():\n",
    "            print(f\"{inp}: {freqs}\")       \n",
    "\n",
    "        \n",
    "    @classmethod\n",
    "    def info(cls):        \n",
    "        '''\n",
    "        Provides a summary of the JerbiModel class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        Creates the Jerbi policy (Parameterized Quantum Circuit) based on the design in https://doi.org/10.48550/arXiv.2103.05577.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_qubits (int): \n",
    "            Number of qubits used in the quantum circuit.\n",
    "        \n",
    "        n_layers (int): \n",
    "            Number of layers in the quantum circuit. Each layer typically consists of parameterized rotations followed by entanglement gates.\n",
    "        \n",
    "        shots (int, optional): \n",
    "            Number of times the circuit gets executed (repeated measurements). If None, the circuit is executed with analytic calculations (no shot noise).\n",
    "        \n",
    "        diff_method (str): \n",
    "            Differentiation method used for training the model. Common options are 'best', 'parameter-shift', 'backprop', etc.\n",
    "        \n",
    "        entanglement_pattern (str): \n",
    "            Entanglement pattern used in the circuit, such as 'chain', 'ring', 'all_to_all', etc., as defined by qml.broadcast patterns.\n",
    "        \n",
    "        entanglement_gate (function): \n",
    "            Quantum gate used for entanglement, such as qml.CZ or qml.CNOT. This gate will be applied between qubits according to the specified entanglement pattern.\n",
    "        \n",
    "        input_scaling (bool): \n",
    "            If True, input parameters are scaled by additional learnable parameters (input_params). The input is multiplied by these parameters before being applied to the qubits.\n",
    "                \n",
    "        input_init (function): \n",
    "            Function to initialize the input scaling parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, and function defined by the user.\n",
    "        \n",
    "        weight_init (function): \n",
    "            Function to initialize the weights of the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.normal_, and function defined by the user..\n",
    "                \n",
    "        measure (function): \n",
    "            Measurement function that takes the number of qubits as an argument and returns the measurement result. Common choices are measure_probs, measure_expval_pairs, or any user-defined measurement function.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        JerbiModel: \n",
    "            An instance of the JerbiModel class representing the quantum neural network.\n",
    "        \"\"\"\n",
    "        print(info_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RZRYRZ(nn.Module):\n",
    "    '''\n",
    "    Creates a parametrized quantum circuit based on the work in https://doi.org/10.48550/arXiv.2103.05577.\n",
    "\n",
    "    For detailed information about the parameters, call the info() method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers, shots = None, diff_method = 'best', entanglement = True,\n",
    "                entanglement_pattern = \"all_to_all\", entanglement_gate = qml.CNOT, input_scaling = True, \n",
    "                input_init = torch.nn.init.ones_, weight_init = torch.nn.init.normal_, measure = two_measure_expval):\n",
    "        super(RZRYRZ, self).__init__()\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.shots = shots\n",
    "        self.diff_method = diff_method\n",
    "        self.entanglement = entanglement\n",
    "        self.entanglement_pattern = entanglement_pattern\n",
    "        self.entanglement_gate = entanglement_gate\n",
    "        self.input_scaling = input_scaling\n",
    "        self.input_init = input_init\n",
    "        self.weight_init = weight_init\n",
    "        self.measure = measure\n",
    "\n",
    "        self.circuit = self.generate_circuit()\n",
    "    \n",
    "    def generate_circuit(self):\n",
    "        if self.shots is None:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        else:\n",
    "            dev = qml.device(\"default.qubit\", wires=self.n_qubits, shots=self.shots)\n",
    "        \n",
    "        if self.n_layers < 1:\n",
    "            raise ValueError(\"Number of layers can't take values below 1\")\n",
    "        \n",
    "        self.weight_shapes = {\n",
    "            \"params\": (self.n_layers + 1, self.n_qubits, 3),\n",
    "            \"input_params\": (self.n_layers, self.n_qubits, 3),\n",
    "        }\n",
    "        \n",
    "        self.init_method = {\n",
    "            \"params\": self.weight_init,\n",
    "            \"input_params\": self.input_init,\n",
    "        }\n",
    "        \n",
    "        @qml.qnode(dev, interface='torch', diff_method=self.diff_method)\n",
    "        def qnode(inputs, params, input_params):\n",
    "            # In case the number of qubits is greater than the input size AND it is multiple of the input length, \n",
    "            # then tile input tensor 'multiplier' times\n",
    "            if self.n_qubits > len(inputs) and self.n_qubits % len(inputs) == 0:\n",
    "                multiplier = self.n_qubits // len(inputs)\n",
    "                inputs = torch.cat([inputs] * multiplier)\n",
    "\n",
    "            # If the number of qubits is not equal to the input length and not a multiple of the input length, raise an error\n",
    "            elif self.n_qubits != len(inputs) and self.n_qubits % len(inputs) != 0:\n",
    "                raise ValueError('Number of qubits cannot be divided by input lenght')\n",
    "\n",
    "            # Apply Hadamard gates to every qubits\n",
    "            qml.broadcast(qml.Hadamard, wires=range(self.n_qubits), pattern=\"single\")\n",
    "            \n",
    "            # Iterate through layers\n",
    "            for layer in range(self.n_layers):\n",
    "                # Iterate through qubits\n",
    "                for wire in range(self.n_qubits):\n",
    "                    # Parameterized block\n",
    "                    qml.RZ(params[layer][wire][0], wires=wire)\n",
    "                    qml.RY(params[layer][wire][1], wires=wire)\n",
    "                    qml.RZ(params[layer][wire][2], wires=wire)\n",
    "\n",
    "                # Entanglement between qubits\n",
    "                if self.entanglement:\n",
    "                    if self.entanglement_pattern == 'strong_entangling':\n",
    "                        strong_entangling(self.n_qubits, layer)\n",
    "                    else:\n",
    "                        qml.broadcast(self.entanglement_gate, wires=range(self.n_qubits), pattern=self.entanglement_pattern)\n",
    "\n",
    "                # If input scaling set to True, multiply inputs by weights\n",
    "                if self.input_scaling:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RZ(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                        qml.RY(input_params[layer][wire][1] * inputs[wire], wires=wire)\n",
    "                        qml.RZ(input_params[layer][wire][2] * inputs[wire], wires=wire)\n",
    "\n",
    "                # If input scaling set to False, use only the inputs\n",
    "                else:\n",
    "                    for wire in range(self.n_qubits):\n",
    "                        qml.RZ(inputs[wire], wires=wire)\n",
    "                        qml.RY(inputs[wire], wires=wire)\n",
    "                        qml.RZ(inputs[wire], wires=wire)\n",
    "\n",
    "                \n",
    "            # Last parameterized layer\n",
    "            for wire in range(self.n_qubits):\n",
    "                qml.RZ(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "                qml.RZ(params[-1][wire][2], wires=wire)\n",
    "                \n",
    "            return self.measure(self.n_qubits)\n",
    "\n",
    "        self.qnode = qnode\n",
    "\n",
    "        model = qml.qnn.TorchLayer(qnode, weight_shapes=self.weight_shapes, init_method=self.init_method)  \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ''' \n",
    "        \n",
    "        '''\n",
    "        return self.circuit(inputs)\n",
    "    \n",
    "    def visualize_circuit(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        circuit = qml.draw_mpl(self.qnode)(inputs, \n",
    "                                        initialized_params[\"params\"], \n",
    "                                        initialized_params[\"input_params\"])\n",
    "        \n",
    "\n",
    "    def circuit_spectrum(self):\n",
    "        inputs = torch.tensor([0.1 * i for i in range(self.n_qubits)], dtype=torch.float32)\n",
    "        \n",
    "        initialized_params = {}\n",
    "        for key, shape in self.weight_shapes.items():\n",
    "            initialized_params[key] = self.init_method[key](torch.empty(shape))\n",
    "\n",
    "        # Draw the circuit\n",
    "        res = qml.fourier.circuit_spectrum(self.qnode)(inputs, initialized_params[\"params\"], initialized_params[\"input_params\"])\n",
    "        for inp, freqs in res.items():\n",
    "            print(f\"{inp}: {freqs}\")       \n",
    "\n",
    "        \n",
    "    @classmethod\n",
    "    def info(cls):        \n",
    "        '''\n",
    "        Provides a summary of the JerbiModel class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        Creates the Jerbi policy (Parameterized Quantum Circuit) based on the design in https://doi.org/10.48550/arXiv.2103.05577.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_qubits (int): \n",
    "            Number of qubits used in the quantum circuit.\n",
    "        \n",
    "        n_layers (int): \n",
    "            Number of layers in the quantum circuit. Each layer typically consists of parameterized rotations followed by entanglement gates.\n",
    "        \n",
    "        shots (int, optional): \n",
    "            Number of times the circuit gets executed (repeated measurements). If None, the circuit is executed with analytic calculations (no shot noise).\n",
    "        \n",
    "        diff_method (str): \n",
    "            Differentiation method used for training the model. Common options are 'best', 'parameter-shift', 'backprop', etc.\n",
    "        \n",
    "        entanglement_pattern (str): \n",
    "            Entanglement pattern used in the circuit, such as 'chain', 'ring', 'all_to_all', etc., as defined by qml.broadcast patterns.\n",
    "        \n",
    "        entanglement_gate (function): \n",
    "            Quantum gate used for entanglement, such as qml.CZ or qml.CNOT. This gate will be applied between qubits according to the specified entanglement pattern.\n",
    "        \n",
    "        input_scaling (bool): \n",
    "            If True, input parameters are scaled by additional learnable parameters (input_params). The input is multiplied by these parameters before being applied to the qubits.\n",
    "                \n",
    "        input_init (function): \n",
    "            Function to initialize the input scaling parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, and function defined by the user.\n",
    "        \n",
    "        weight_init (function): \n",
    "            Function to initialize the weights of the quantum circuit, such as torch.nn.init.uniform_, torch.nn.init.normal_, and function defined by the user..\n",
    "                \n",
    "        measure (function): \n",
    "            Measurement function that takes the number of qubits as an argument and returns the measurement result. Common choices are measure_probs, measure_expval_pairs, or any user-defined measurement function.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        JerbiModel: \n",
    "            An instance of the JerbiModel class representing the quantum neural network.\n",
    "        \"\"\"\n",
    "        print(info_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyPostProcessing(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_actions, policy_type = 'raw_contiguous', \n",
    "                 beta_scheduling = False, beta = 1, increase_rate = 0.0005, \n",
    "                 output_scaling = False, output_init = torch.nn.init.ones_):\n",
    "        super(PolicyPostProcessing, self).__init__()\n",
    "\n",
    "        '''\n",
    "        Determines the type of policy used based on the arguments:\n",
    "\n",
    "            n_actions(int) = Number of actions\n",
    "            post_processing(str) = Type of policy ('raw_contiguous', 'raw_parity', 'softmax')\n",
    "            beta_scheduling(bool) = Inverse temperature parameter used in softmax (used if set to True)\n",
    "            beta(float) = Beta parameter or inverse temperature (used only for softmax)\n",
    "            increase_rate(float) = Amount added to beta at the end of each episode (used only for softmax)\n",
    "            output_scaling(bool) = Output parameters are used if True\n",
    "            output_init(function) = How the output parameters are initialized\n",
    "            \n",
    "        '''\n",
    "        self.n_actions = n_actions\n",
    "        self.policy_type = policy_type\n",
    "        self.beta_scheduling = beta_scheduling\n",
    "        self.beta = beta\n",
    "        self.increase_rate = increase_rate\n",
    "        self.output_scaling = output_scaling\n",
    "        self.output_init = output_init\n",
    "\n",
    "        if self.output_scaling == True:\n",
    "            self.output_params = nn.parameter.Parameter(torch.Tensor(self.n_actions), requires_grad=True)\n",
    "            #self.output_params = nn.parameter.Parameter(torch.Tensor(1), requires_grad=True)\n",
    "            self.output_init(self.output_params)\n",
    "        else:\n",
    "            self.register_parameter('w_input', None)\n",
    "\n",
    "    def forward(self,probs):\n",
    "        if self.policy_type == 'raw_contiguous':\n",
    "            policy = self.raw_contiguous(probs)\n",
    "        elif self.policy_type == 'raw_parity':\n",
    "            policy = self.raw_parity(probs)\n",
    "        elif self.policy_type == 'softmax':\n",
    "            policy = self.softmax(probs)\n",
    "        elif self.policy_type == 'softmax_probs':\n",
    "            policy = self.softmax_probs(probs)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid post-processing method specified.\")\n",
    "        return policy\n",
    "\n",
    "    def raw_contiguous(self,probs):\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        chunk_size = len(probs_flatten) // self.n_actions\n",
    "        remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "        policy = []\n",
    "\n",
    "        for i in range(self.n_actions):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size\n",
    "\n",
    "            if i < remainder:\n",
    "                end += 1\n",
    "\n",
    "            # Update the original policy list instead of creating a new one\n",
    "            policy.append(sum(probs_flatten[start:end]))\n",
    "\n",
    "        policy_tensor = torch.stack(policy)\n",
    "        return policy_tensor\n",
    "        \n",
    "    def raw_parity(self,probs):\n",
    "\n",
    "        if self.n_actions % 2 != 0:\n",
    "            raise ValueError('For parity-like policy, n_actions must be an even number')\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        policy = torch.zeros(self.n_actions)\n",
    "        counter = 0\n",
    "        for prob in probs_flatten:\n",
    "            policy[counter] += prob\n",
    "            counter += 1\n",
    "            if counter == self.n_actions:\n",
    "                counter = 0\n",
    "        \n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    def softmax(self, probs):\n",
    "        \n",
    "        if self.output_scaling == True:\n",
    "            probs *= self.output_params\n",
    "\n",
    "        scaled_output = probs * self.beta\n",
    "        softmax_output = F.softmax(scaled_output, dim=0)\n",
    "        return softmax_output\n",
    "    \n",
    "    def softmax_probs(self, probs):\n",
    "\n",
    "        if self.output_scaling == True:\n",
    "            probs *= self.output_params\n",
    "            \n",
    "        probs_flatten = probs.flatten()\n",
    "        chunk_size = len(probs_flatten) // self.n_actions\n",
    "        remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "        policy = []\n",
    "\n",
    "        for i in range(self.n_actions):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size\n",
    "\n",
    "            if i < remainder:\n",
    "                end += 1\n",
    "\n",
    "            policy.append(sum(probs_flatten[start:end]))\n",
    "        policy_tensor = torch.stack(policy)\n",
    "        softmax_output = F.softmax(policy_tensor * self.beta, dim=0)\n",
    "        return softmax_output\n",
    "    \n",
    "    def beta_schedule(self):\n",
    "        if self.beta_scheduling == True:\n",
    "            if self.policy_type == 'softmax' or self.policy_type == 'softmax_probs':\n",
    "                self.beta += self.increase_rate\n",
    "\n",
    "    @classmethod\n",
    "    def info(cls):\n",
    "        '''\n",
    "        Provides a summary of the PolicyType class and its parameters/methods.\n",
    "        '''\n",
    "        info_text = \"\"\"\n",
    "        PolicyType Class:\n",
    "\n",
    "        Determines the type of policy used based on the arguments:\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_actions (int): \n",
    "            Number of actions available for the agent to choose from.\n",
    "        \n",
    "        policy_type (str): \n",
    "            Type of policy applied to the probability distribution:\n",
    "            - 'raw_contiguous': Sums up contiguous chunks of probabilities.\n",
    "            - 'raw_parity': Sums up probabilities in a circular manner based on parity.\n",
    "            - 'softmax': Applies a softmax function to the scaled probabilities.\n",
    "            - 'softmax_probs': Sums up contiguous chunks of probabilities and then applies softmax.\n",
    "        \n",
    "        beta_scheduling (bool): \n",
    "            If True, updates the inverse temperature parameter (beta) after each episode. Used only for 'softmax' and 'softmax_probs'.\n",
    "        \n",
    "        beta (float): \n",
    "            Inverse temperature parameter used for scaling probabilities in the softmax policy.\n",
    "        \n",
    "        increase_rate (float): \n",
    "            Amount added to beta at the end of each episode, if beta_scheduling is True.\n",
    "        \n",
    "        output_scaling (bool): \n",
    "            If True, scales the output probabilities by learnable parameters.\n",
    "        \n",
    "        output_init (function): \n",
    "            Initialization function for output parameters, such as torch.nn.init.uniform_, torch.nn.init.ones_, etc.\n",
    "        \n",
    "        Methods:\n",
    "        -------\n",
    "        sample(probs):\n",
    "            Selects an action based on the chosen post_processing method.\n",
    "        \n",
    "        raw_contiguous(probs):\n",
    "            Sums up contiguous chunks of probabilities and returns the resulting tensor.\n",
    "        \n",
    "        raw_parity(probs):\n",
    "            Sums up probabilities in a circular manner based on parity and returns the action, log probability, and policy tensor.\n",
    "        \n",
    "        softmax(probs):\n",
    "            Applies a softmax function to the scaled probabilities and returns the action, log probability, and policy tensor.\n",
    "        \n",
    "        softmax_probs(probs):\n",
    "            Sums up contiguous chunks of probabilities, applies softmax, and returns the action, log probability, and policy tensor.\n",
    "        \n",
    "        beta_schedule():\n",
    "            Updates the beta parameter if beta_scheduling is True. Only applicable for 'softmax' and 'softmax_probs' methods.\n",
    "        \"\"\"\n",
    "        print(info_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumPolicy(nn.Module):\n",
    "    \n",
    "    def __init__(self, circuit, post_processing):\n",
    "        super(QuantumPolicy, self).__init__()\n",
    "        self.circuit = circuit\n",
    "        self.post_processing = post_processing\n",
    "\n",
    "    def sample(self, inputs):\n",
    "        '''\n",
    "        Samples an action from the action probability distribution\n",
    "        '''\n",
    "        policy = self.forward(inputs)\n",
    "        dist = torch.distributions.Categorical(policy)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), policy\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Input state is fed to the circuit - its output is then fed to the post processing \n",
    "        '''\n",
    "        probs = self.circuit.forward(inputs)\n",
    "        probs_processed = self.post_processing.forward(probs)\n",
    "        return probs_processed\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        '''\n",
    "        Returns the values of each set of parameters\n",
    "        '''\n",
    "        parameters = []\n",
    "\n",
    "        circuit_parameters = [param.clone().detach().numpy().flatten() for param in self.circuit.parameters()]\n",
    "        parameters.extend(circuit_parameters)\n",
    "\n",
    "        policy_parameters = [param.clone().detach().numpy().flatten() for param in self.post_processing.parameters()]\n",
    "        parameters.extend(policy_parameters)\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "    def get_gradients(self):\n",
    "        '''\n",
    "        Returns the gradient values of each set of parameters\n",
    "        '''\n",
    "        gradients = []\n",
    "\n",
    "        # Get gradients from circuit parameters\n",
    "        circuit_gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for param in self.circuit.parameters()]\n",
    "        gradients.extend(circuit_gradients)\n",
    "\n",
    "        # Get gradients from policy parameters\n",
    "        policy_gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for param in self.post_processing.parameters()]\n",
    "        gradients.extend(policy_gradients)\n",
    "\n",
    "        return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baselines parecem ter problemas, principalmente a baseline com PQC a aproximar a value function\n",
    "Save agent carateristicas precisa de alterações de forma a suportar circuitos que possam ser escritos por outros usarios, que possam ter variaveis com diferentes nomes ( criar ciclo for e gravar todas as variaveis associadas à class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceAgent:\n",
    "\n",
    "    def __init__(self, policy, policy_optimizer, env_name, n_episodes, max_t, gamma = 0.99, baseline = False, batch_size = 10, print_every = 100, verbose = 1):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy = policy.to(self.device)\n",
    "        self.policy_optimizer = policy_optimizer\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env_name = env_name\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "\n",
    "\n",
    "        self.solved = False\n",
    "        self.scores_deque = deque(maxlen=self.print_every)\n",
    "        self.baseline = baseline\n",
    "        self.batch_size = batch_size\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.batch_log_probs = []\n",
    "        self.batch_rewards = []\n",
    "        self.loss = torch.tensor(0.0)\n",
    "\n",
    "    def train(self, run_count=None, rundate = None, path=None, tensorboard=False):\n",
    "\n",
    "        if run_count is not None and path is not None:\n",
    "            data_path = create_directory(os.path.join(path, 'data'))\n",
    "            env_folder = create_directory(os.path.join(data_path, self.env_name))\n",
    "            experiment_folder_name = f\"{self.policy.circuit.__class__.__name__}_{self.policy.circuit.n_qubits}qubits_{self.policy.circuit.n_layers}layer_{rundate}\"\n",
    "            experiment_path = create_directory(os.path.join(env_folder, experiment_folder_name))\n",
    "            run_path = create_directory(os.path.join(experiment_path, f'run_{str(run_count)}'))\n",
    "            self.save_agent_data(experiment_path)\n",
    "        \n",
    "        if tensorboard:\n",
    "            writer = SummaryWriter(log_dir=run_path)\n",
    "\n",
    "        for i in range(1, self.n_episodes + 1):\n",
    "            start_time = time.time()\n",
    "            self.get_trajectory()\n",
    "\n",
    "            if np.mean(self.scores_deque) > self.env.spec.reward_threshold:\n",
    "                self.solved = True\n",
    "\n",
    "            if i % self.batch_size == 0:\n",
    "                self.update_policy()\n",
    "\n",
    "            end_time = time.time()\n",
    "            self.runtime = end_time - start_time\n",
    "\n",
    "            if tensorboard:\n",
    "                self.writer_function(writer, i)\n",
    "            if run_count is not None and path is not None:\n",
    "                self.save_data(run_path)\n",
    "            if not self.solved:\n",
    "                self.policy.post_processing.beta_schedule()\n",
    "\n",
    "            if i % self.print_every == 0 and self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}s\\t Last {} Episodes average reward: {:.2f}\\t '.format(i, self.scores_deque[-1], self.runtime, self.print_every, np.mean(self.scores_deque)))\n",
    "            elif self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tRuntime: {:.2f}s\\t '.format(i, self.scores_deque[-1], self.runtime))\n",
    "\n",
    "        if tensorboard:\n",
    "            writer.close()\n",
    "\n",
    "    def get_trajectory(self):\n",
    "        '''\n",
    "        Gets a trajectory based on the running policy until it runs out of bounds or achieves maximum reward of an episode\n",
    "        '''\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(self.max_t):\n",
    "            if t == 0:\n",
    "                state_tensor = torch.tensor(state[0]).float().to(self.device)\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float().to(self.device)\n",
    "            action, log_prob, _ = self.policy.sample(state_tensor)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            \n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                self.scores_deque.append(sum(self.rewards))\n",
    "                self.batch_log_probs.append(self.saved_log_probs)\n",
    "                self.batch_rewards.append(self.rewards)\n",
    "                break\n",
    "        \n",
    "        self.scores_deque.append(sum(self.rewards))\n",
    "        self.batch_log_probs.append(self.saved_log_probs)\n",
    "        self.batch_rewards.append(self.rewards)\n",
    "\n",
    "    def update_policy(self):\n",
    "        '''\n",
    "        Computes the loss and gradients and updates the policy via gradient methods\n",
    "        '''\n",
    "        all_returns = []\n",
    "        for batch in self.batch_rewards:\n",
    "            R = 0\n",
    "            ep_return = []\n",
    "            for r in batch[::-1]:\n",
    "                R = r + self.gamma * R\n",
    "                ep_return.insert(0, R)\n",
    "            ep_return = torch.tensor(ep_return).to(self.device)\n",
    "            ep_return = (ep_return - ep_return.mean()) / (ep_return.std() + 1e-8)  # Normalize returns\n",
    "            all_returns.append(ep_return)\n",
    "\n",
    "        policy_loss = []\n",
    "        if self.baseline:\n",
    "            for log_probs, ep_returns in zip(self.batch_log_probs, all_returns):\n",
    "                baseline = ep_returns.mean()  # Compute the baseline for this trajectory\n",
    "                for log_prob, ret in zip(log_probs, ep_returns):\n",
    "                    advantage = ret - baseline  # Compute the advantage\n",
    "                    policy_loss.append(-log_prob * advantage)\n",
    "        else:\n",
    "            for log_probs, ep_returns in zip(self.batch_log_probs, all_returns):\n",
    "                for log_prob, ret in zip(log_probs, ep_returns):\n",
    "                    policy_loss.append(-log_prob * ret)\n",
    "\n",
    "        if policy_loss:\n",
    "            policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "            self.loss = torch.cat(policy_unsqueezed).sum() / self.batch_size\n",
    "\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        if not self.solved:\n",
    "            self.policy_optimizer.step()\n",
    "\n",
    "        self.batch_log_probs = []\n",
    "        self.batch_rewards = []\n",
    "        \n",
    "    def save_agent_data(self,main_path):\n",
    "        '''\n",
    "        Stores the model parameters into a json file\n",
    "\n",
    "        '''\n",
    "        agent_variables = {\n",
    "            \"Model\": self.policy.circuit.__class__.__name__,\n",
    "            \"Number of Qubits\": self.policy.circuit.n_qubits,\n",
    "            \"Number of Layers\": self.policy.circuit.n_layers,\n",
    "            \"Shots\": self.policy.circuit.shots,\n",
    "            \"Differentiation Method\": self.policy.circuit.diff_method,\n",
    "            \"Entanglement\": self.policy.circuit.entanglement,\n",
    "            \"Entanglement Pattern\": self.policy.circuit.entanglement_pattern,\n",
    "            \"Entanglement Gate\": get_function_representation(self.policy.circuit.entanglement_gate),\n",
    "            \"Input Scaling\": self.policy.circuit.input_scaling,\n",
    "            \"Input Initialization\": get_function_representation(self.policy.circuit.input_init),\n",
    "            \"Weight Initialization\": get_function_representation(self.policy.circuit.weight_init),\n",
    "            \"Measure\": get_function_representation(self.policy.circuit.measure),\n",
    "            \"Policy Post Processing\": self.policy.post_processing.policy_type,\n",
    "            \"Optimizers\": str(self.policy_optimizer),\n",
    "            \"Envinronment\": str(self.env_name),\n",
    "            \"Gamma (discounting factor)\": self.gamma,\n",
    "        }\n",
    "        if self.policy.post_processing.policy_type in ['softmax', 'softmax_probs']:\n",
    "            agent_variables.update({\n",
    "                \"Softmax scheduling (in case policy is softmax)\": f\"{self.policy.post_processing.beta_scheduling}. Starting beta: {self.policy.post_processing.beta}. Increase rate: {self.policy.post_processing.increase_rate}\",\n",
    "                \"Softmax output scaling (in case policy is softmax)\": f\"{self.policy.post_processing.output_scaling}. Output Initialization: {get_function_representation(self.policy.post_processing.output_init)}\",\n",
    "            })\n",
    "        with open(os.path.join(main_path, \"agent_characteristics.json\"), \"w\") as f:\n",
    "            json.dump(agent_variables, f, indent=4)\n",
    "\n",
    "    def save_data(self, run_path):\n",
    "        '''\n",
    "        Saves the data into a .npz file of each episode\n",
    "\n",
    "        '''\n",
    "        data_file = os.path.join(run_path, \"run_data.npz\")\n",
    "        if os.path.exists(data_file):\n",
    "            data = np.load(data_file, allow_pickle=True)\n",
    "            old_episode_reward = data['episode_reward'].tolist()\n",
    "            old_loss = data['loss'].tolist()\n",
    "            old_runtime = data['runtime'].tolist()\n",
    "            old_params_gradients = data['params_gradients'].tolist()\n",
    "            old_input_params_gradients = data['input_params_gradients'].tolist()\n",
    "        else:\n",
    "            old_episode_reward = []\n",
    "            old_loss = []\n",
    "            old_runtime = []\n",
    "            old_params_gradients = []\n",
    "            old_input_params_gradients = []\n",
    "\n",
    "\n",
    "        old_episode_reward.append(self.scores_deque[-1])\n",
    "        old_loss.append(self.loss.item())\n",
    "        old_runtime.append(self.runtime)\n",
    "        old_params_gradients.append(tensor_to_list(self.policy.get_gradients()[0]))\n",
    "        old_input_params_gradients.append(tensor_to_list(self.policy.get_gradients()[1]))\n",
    "\n",
    "        np.savez_compressed(data_file, episode_reward = np.array(old_episode_reward),\n",
    "                                 loss = np.array(old_loss),\n",
    "                                 runtime = np.array(old_runtime),\n",
    "                                 params_gradients = np.array(old_params_gradients), \n",
    "                                 input_params_gradients = np.array(old_input_params_gradients))\n",
    "        \n",
    "        del old_episode_reward[:]\n",
    "        del old_loss[:]\n",
    "        del old_runtime[:]\n",
    "        del old_params_gradients[:]\n",
    "        del old_input_params_gradients[:]\n",
    "\n",
    "    def writer_function(self, writer, iteration):\n",
    "        '''\n",
    "        Stores data into a tensorboard session\n",
    "\n",
    "        '''\n",
    "        writer.add_scalar(\"Episode Reward\", self.scores_deque[-1], global_step=iteration)\n",
    "        writer.add_scalar(\"Runtime\", self.runtime, global_step=iteration)\n",
    "        writer.add_scalar(\"Loss\", self.loss.item(), global_step=iteration)\n",
    "        if self.policy.post_processing.policy_type in ['softmax', 'softmax_probs']:\n",
    "            writer.add_scalar(\"Beta\", self.policy.post_processing.beta, global_step=iteration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jerbi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 6\n",
    "n_layers = 4\n",
    "shots = None\n",
    "diff_method = 'best' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = torch.nn.init.ones_\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).normal_(-np.pi, np.pi)\n",
    "policy_circuit_measure = three_measure_expval\n",
    "policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "policy_circuit.visualize_circuit()\n",
    "#policy_circuit.circuit_spectrum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 4\n",
    "shots = None\n",
    "diff_method = 'best' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "input_scaling = True\n",
    "input_init = torch.nn.init.ones_\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval\n",
    "policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "\n",
    "n_actions = 2\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = False\n",
    "beta = 1\n",
    "increase_rate = 0.0005\n",
    "output_scaling = True\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                         post_processing, \n",
    "                         beta_scheduling, \n",
    "                         beta, increase_rate, \n",
    "                         output_scaling, \n",
    "                         output_init)\n",
    "\n",
    "policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "env_name = 'CartPole-v0'\n",
    "n_episodes = 1000\n",
    "max_t = 200\n",
    "gamma = 0.98\n",
    "print_every = 100\n",
    "verbose = 1\n",
    "baseline = False\n",
    "batch_size = 5\n",
    "reinforce_update = ReinforceAgent(policy, \n",
    "                                  policy_optimizer, \n",
    "                                  env_name, \n",
    "                                  n_episodes, \n",
    "                                  max_t, \n",
    "                                  gamma, \n",
    "                                  baseline, \n",
    "                                  batch_size, \n",
    "                                  print_every, \n",
    "                                  verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 3\n",
    "n_layers = 15\n",
    "state_dim = 4\n",
    "shots = None\n",
    "diff_method = 'best' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "policy_circuit_measure = two_measure_expval\n",
    "policy_circuit = UQC_FullEnc(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                    entanglement, entanglement_pattern, entanglement_gate, \n",
    "                    weight_init, policy_circuit_measure)\n",
    "\n",
    "policy_circuit.visualize_circuit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 5\n",
    "state_dim = 6\n",
    "shots = None\n",
    "diff_method = 'best' \n",
    "entanglement = True\n",
    "entanglement_pattern = \"all_to_all\"\n",
    "entanglement_gate = qml.CZ\n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "policy_circuit_measure = three_measure_expval\n",
    "policy_circuit = UQC_FullEnc(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                     entanglement, entanglement_pattern, entanglement_gate, \n",
    "                     weight_init, policy_circuit_measure)\n",
    "\n",
    "n_actions = 3\n",
    "post_processing = 'softmax'\n",
    "beta_scheduling = False\n",
    "beta = 1\n",
    "increase_rate = 0.0005\n",
    "output_scaling = True\n",
    "output_init = torch.nn.init.ones_\n",
    "policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                         post_processing, \n",
    "                         beta_scheduling, \n",
    "                         beta, increase_rate, \n",
    "                         output_scaling, \n",
    "                         output_init)\n",
    "\n",
    "policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "policy_lr_list= [0.1, 0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "env_name = 'Acrobot-v1'\n",
    "n_episodes = 1000\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 100\n",
    "verbose = 1\n",
    "baseline = False\n",
    "batch_size = 1\n",
    "reinforce_update = ReinforceAgent(policy, \n",
    "                                  policy_optimizer, \n",
    "                                  env_name, \n",
    "                                  n_episodes, \n",
    "                                  max_t, \n",
    "                                  gamma, \n",
    "                                  baseline, \n",
    "                                  batch_size, \n",
    "                                  print_every, \n",
    "                                  verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UQC Full Encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UQC 1Q 5L (o ultimo deu resultados insanamente bons, portanto melhor repetir (?)) (mudei as gates para RZ RY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 1\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = UQC_FullEnc(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.01, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = False\n",
    "    batch_size = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UQC 1Q 5L no entangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 1\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = False\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = UQC_FullEnc(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.01, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = False\n",
    "    batch_size = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UQC 2Q 5L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 2\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = UQC_FullEnc(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.01, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = False\n",
    "    batch_size = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UQC 2Q 5L no entangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 2\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = False\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = UQC_FullEnc(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.01, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = False\n",
    "    batch_size = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UQC 4Q 5L "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = UQC_FullEnc(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.01, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = False\n",
    "    batch_size = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UQC 4Q 5L no entangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = False\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = UQC_FullEnc(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.01, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = False\n",
    "    batch_size = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UQC Partial Encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UQC Partial 1Q 5L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 1\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = UQC_PartialEnc(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.01, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = False\n",
    "    batch_size = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UQC Partial 1Q 5L no entangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 1\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = False\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = UQC_PartialEnc(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.01, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = False\n",
    "    batch_size = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UQC Partial 2Q 5L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 2\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = UQC_PartialEnc(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.01, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = False\n",
    "    batch_size = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UQC Partial 2Q 5L no entangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 2\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = False\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = UQC_PartialEnc(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.01, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = False\n",
    "    batch_size = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UQC Partial 4Q 5L "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = UQC_PartialEnc(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.01, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = False\n",
    "    batch_size = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UQC Partial 4Q 5L no entangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = False\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = UQC_PartialEnc(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.01, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = False\n",
    "    batch_size = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acrobot tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acrobot UQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 5\n",
    "    state_dim = 6\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = three_measure_expval\n",
    "    policy_circuit = UQC_FullEnc(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 3\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.01, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'Acrobot-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = False\n",
    "    batch_size = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acrobot jerbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = torch.nn.init.ones_\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = three_measure_expval\n",
    "    policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 3\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'Acrobot-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = False\n",
    "    batch_size = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acrobot TFQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"ring\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = torch.nn.init.ones_\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = three_measure_expval\n",
    "    policy_circuit = TfqTutorial(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 3\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'Acrobot-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = False\n",
    "    batch_size = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforce Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforce batched = 10 UQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 1\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = UQC_FullEnc(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.01, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = False\n",
    "    batch_size = 10\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforce batched = 10 jerbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = torch.nn.init.ones_\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = False\n",
    "    batch_size = 10\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforce Baseline True UQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 1\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = UQC_FullEnc(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.01, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforce Baseline True jerbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = torch.nn.init.ones_\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforce Baseline batched = 10 UQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 1\n",
    "    n_layers = 5\n",
    "    state_dim = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = UQC_FullEnc(n_qubits, n_layers, state_dim, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.1, 0.01, 0.01, 0.1]  # [weights, params, bias, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforce Baseline batched = 10 Jerbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "two_levels_up = os.path.abspath(os.path.join(current_dir, \"../../\"))\n",
    "def train_agents(pqc, optimizers,env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceAgent(pqc, optimizers, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose)\n",
    "    reinforce_update.train(file_name, rundate, two_levels_up, True)\n",
    "\n",
    "    return ('Agent ' + str(file_name) + ': ' + str(reinforce_update.solved))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4\n",
    "    shots = None\n",
    "    diff_method = 'best' \n",
    "    entanglement = True\n",
    "    entanglement_pattern = \"all_to_all\"\n",
    "    entanglement_gate = qml.CZ\n",
    "    input_scaling = True\n",
    "    input_init = torch.nn.init.ones_\n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    policy_circuit_measure = two_measure_expval\n",
    "    policy_circuit = JerbiModel(n_qubits, n_layers, shots, diff_method, \n",
    "                        entanglement, entanglement_pattern, entanglement_gate, \n",
    "                        input_scaling, input_init, weight_init, policy_circuit_measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling = False\n",
    "    beta = 1\n",
    "    increase_rate = 0.003\n",
    "    output_scaling = True\n",
    "    output_init = torch.nn.init.ones_\n",
    "    policy_post_process = PolicyPostProcessing(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicy(policy_circuit,policy_post_process)\n",
    "\n",
    "    policy_lr_list= [0.01, 0.1, 0.1]  # [weights, input_weights, output_weights]\n",
    "    policy_params = list(policy_circuit.parameters()) + list(policy_post_process.parameters())\n",
    "    policy_optimizer= create_optimizer_with_lr(policy_params, policy_lr_list, use_amsgrad=True)\n",
    "\n",
    "\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 1000\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 100\n",
    "    verbose = 1\n",
    "    baseline = True\n",
    "    batch_size = 10\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 8\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, policy_optimizer, env_name, n_episodes, max_t, gamma, baseline, batch_size, print_every, verbose, str(i), rundate) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn Off PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(100)\n",
    "\n",
    "import os\n",
    "os.system(\"shutdown /s /t 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jerbi test entanglements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel runs\n",
    "import optuna\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(policy, optimizers,env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(policy, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name, rundate)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return reinforce_update.scores\n",
    "\n",
    "def objective_function(results):\n",
    "\n",
    "    results_mean = np.mean(results, axis=0)\n",
    "    area = np.abs(np.trapz(results_mean))\n",
    "    maximum_performance_area = float(len(results[0]) * 200)\n",
    "\n",
    "    # Create a metric called performance area and normalize it between 0 and 1\n",
    "    performance_area = area / maximum_performance_area\n",
    "    return performance_area\n",
    "\n",
    "'''\n",
    "def sum_and_average(results):\n",
    "\n",
    "    averages = np.mean(results, axis=1)\n",
    "    return np.mean(averages)\n",
    "'''\n",
    "\n",
    "def objective(trial):    \n",
    "\n",
    "    n_qubits = 4\n",
    "    n_layers = 4      #set to 1 if data_reuploading is off\n",
    "    entanglement = \"all_to_all\"\n",
    "    shots = None\n",
    "    input_scaling = True\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'backprop' \n",
    "    weight_init = torch.nn.init.normal_\n",
    "    #weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "    input_init = torch.nn.init.ones_\n",
    "    measure = measure_expval_pairs\n",
    "    circuit = CircuitGenerator(n_qubits, \n",
    "                            n_layers,\n",
    "                            entanglement,\n",
    "                            shots,\n",
    "                            input_scaling,\n",
    "                            design,\n",
    "                            diff_method,\n",
    "                            weight_init,\n",
    "                            input_init,\n",
    "                            measure)\n",
    "\n",
    "    n_actions = 2\n",
    "    post_processing = 'softmax'\n",
    "    beta_scheduling, beta, increase_rate = True, 1, 0.005\n",
    "    output_scaling, output_init = False, torch.nn.init.ones_\n",
    "    policy_type = PolicyType(n_actions, \n",
    "                            post_processing, \n",
    "                            beta_scheduling, \n",
    "                            beta, \n",
    "                            increase_rate, \n",
    "                            output_scaling, \n",
    "                            output_init)\n",
    "\n",
    "    policy = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "    lr1 = trial.suggest_float(\"lr1\", 1e-5, 1e-1, log=True)\n",
    "    #lr2 = trial.suggest_float(\"lr2\", 1e-5, 1e-1, log=True)\n",
    "    #lr3 = trial.suggest_float(\"lr3\", 1e-5, 1e-1, log=True)\n",
    "    lr2, lr3 = 0.1, 0.1\n",
    "    lr_list= [lr1, lr2, lr3]  # [weights, input_weights, output_weights]\n",
    "    circuit_params = list(circuit.parameters())\n",
    "    policy_params = list(policy_type.parameters())\n",
    "    params = circuit_params + policy_params\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    n_episodes = 10\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 1\n",
    "    verbose = 1\n",
    "    rundate = datetime.now().strftime('%Y-%m-%d_%H.%M.%S')      \n",
    "    num_agents = 5\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(policy, optimizers, env_name, n_episodes, max_t, gamma, print_every, verbose, i, trial.number) for i in range(num_agents))\n",
    "    performance_metric = objective_function(results)\n",
    "    #performance_metric = sum_and_average(results)\n",
    "    return performance_metric\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    print(f\"Best trial: {study.best_trial.value}\")\n",
    "    print(f\"Best parameters: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_output = F.softmax(torch.Tensor([0.1,0.9]) * 3, dim=0)\n",
    "print(softmax_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
