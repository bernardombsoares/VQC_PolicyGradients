{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pennylane as qml \n",
    "import torch\n",
    "\n",
    "def create_circuit(n_qubits,n_layers=None,circ = \"simplified_two_design\",fim=False, shots=None):\n",
    "\n",
    "    dev = qml.device(\"default.qubit.torch\", wires=n_qubits, shots=shots)\n",
    "\n",
    "    def RZRY(params):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        #qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        #qml.AngleEmbedding(params,wires=range(n_qubits))\n",
    "        for q in range(n_qubits):\n",
    "            qml.Hadamard(wires=q)\n",
    "\n",
    "        for w in range(n_layers): \n",
    "            for q in range(n_qubits):\n",
    "                index = w * (2*n_qubits) + q * 2\n",
    "                qml.RZ(params[index],wires=q)\n",
    "                qml.RY(params[index + 1],wires=q)\n",
    "        \n",
    "        qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        return qml.probs(wires=range(n_qubits))\n",
    "\n",
    "    def S2D(init_params,params,measurement_qubits=0,prod_approx=False):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        \n",
    "        #qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        if not prod_approx:\n",
    "            return qml.probs(wires=list(range(measurement_qubits)))\n",
    "        else:\n",
    "            return [qml.probs(i) for i in range(measurement_qubits)]\n",
    "\n",
    "    def SU(params):\n",
    "        qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        \n",
    "        ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "        for i in range(2,n_qubits):\n",
    "            ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "        return qml.expval(ZZ)\n",
    "    \n",
    "    def simmpleRZRY(params,cnots=True):\n",
    "        qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RZ, wires=range(n_qubits), pattern=\"single\", parameters=params[0])\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params[1])\n",
    "        if cnots:\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "            return qml.expval(qml.PauliZ(n_qubits-1))\n",
    "        else:\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "            return qml.expval(ZZ)\n",
    "        \n",
    "    def RY(params,y=True,probs=False,prod=False, entanglement=None):\n",
    "        #qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params)\n",
    "        #qml.broadcast(qml.CZ, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "\n",
    "        if entanglement==\"all_to_all\":\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        if y==True:\n",
    "            #YY = qml.operation.Tensor(qml.PauliY(0), qml.PauliY(1))\n",
    "            YY = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                #YY = qml.operation.Tensor(YY, qml.PauliY(i))\n",
    "                YY.append(qml.PauliZ(i))\n",
    "            \n",
    "            #return [qml.expval(i) for i in YY]\n",
    "            return qml.expval(YY)\n",
    "\n",
    "        elif probs==False:\n",
    "\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            #ZZ = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))        \n",
    "                #ZZ.append(qml.PauliZ(i))        \n",
    "\n",
    "            #return [qml.expval(i) for i in ZZ]\n",
    "            return qml.expval(ZZ)\n",
    "\n",
    "        else:\n",
    "            if prod:\n",
    "                return [qml.probs(i) for i in range(n_qubits)]\n",
    "            else:\n",
    "                return qml.probs(wires=range(n_qubits))\n",
    "            \n",
    "        \n",
    "        \n",
    "    def GHZ(params,measurement_qubits=0):\n",
    "        qml.RY(params,wires=0)\n",
    "        qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "\n",
    "    def random_product_state(params,gate_sequence=None):\n",
    "                \n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(np.pi / 4, wires=i)\n",
    "\n",
    "        for ll in range(len(params)):\n",
    "\n",
    "            for i in range(n_qubits):\n",
    "                gate_sequence[\"{}{}\".format(ll,i)](params[ll][i], wires=i)\n",
    "\n",
    "            #for i in range(n_qubits - 1):\n",
    "                #qml.CZ(wires=[i, i + 1])\n",
    "    def SEL(params, measurement_qubits=0):\n",
    "        qml.StronglyEntanglingLayers(params, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    def RL(params, measurement_qubits=0):\n",
    "        qml.RandomLayers(params, ratio_imprim=0.8 ,imprimitive=qml.CZ, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    if circ == \"rzry\":\n",
    "        qcircuit = RZRY\n",
    "    elif circ == \"simplified_two_design\":\n",
    "        qcircuit = S2D\n",
    "    elif circ == \"special_unitary\":\n",
    "        qcircuit = SU\n",
    "    elif circ == \"simpleRZRY\":\n",
    "        qcircuit = simmpleRZRY\n",
    "    elif circ == \"RY\":\n",
    "        qcircuit = RY\n",
    "    elif circ == \"ghz\":\n",
    "        qcircuit = GHZ\n",
    "    elif circ == \"random_product_state\":\n",
    "        qcircuit = random_product_state\n",
    "    elif circ == \"SEL\":\n",
    "        qcircuit = SEL\n",
    "    elif circ == \"RL\":\n",
    "        qcircuit = RL\n",
    "    if not fim:\n",
    "        circuit = qml.QNode(qcircuit, dev,interface=\"torch\", diff_method=\"backprop\")\n",
    "    else:\n",
    "        circuit = qml.QNode(qcircuit, dev)\n",
    "\n",
    "    return circuit\n",
    "\n",
    "def compute_gradient(log_prob, w):\n",
    "    \"\"\"Compute gradient of the log probability with respect to weights.\n",
    "    \n",
    "    Args:\n",
    "    - log_prob (torch.Tensor): The log probability tensor.\n",
    "    - w (torch.Tensor): The weights tensor, with requires_grad=True.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The gradient of log_prob with respect to w, flattened.\n",
    "    \"\"\"\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()\n",
    "    log_prob.backward(retain_graph=True)\n",
    "    \n",
    "    if w.grad is None:\n",
    "        raise ValueError(\"The gradient for the given log_prob with respect to w is None.\")\n",
    "    \n",
    "    return w.grad.view(-1).detach().numpy()\n",
    "\n",
    "def policy(probs, policy_type=\"contiguous-like\", n_actions=2, n_qubits=1):\n",
    "\n",
    "    if policy_type == \"contiguous-like\":\n",
    "        return probs\n",
    "    elif policy_type == \"parity-like\":\n",
    "        policy = torch.zeros(n_actions)\n",
    "        for i in range(len(probs)):\n",
    "            a=[]\n",
    "            for m in range(int(np.log2(n_actions))):\n",
    "                if m==0:    \n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)\n",
    "                else:\n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)[:-m]\n",
    "                \n",
    "                a.append(bitstring.count(\"1\") % 2)\n",
    "            policy[int(\"\".join(str(x) for x in a),2)] += probs[i]\n",
    "\n",
    "        return policy    \n",
    "    \n",
    "def compute_policy_and_gradient(args):\n",
    "    n_qubits, shapes, type , n_actions, policy_type, clamp = args\n",
    "\n",
    "    if policy_type == \"parity-like\":\n",
    "        measure_qubits = n_qubits\n",
    "    else:\n",
    "        measure_qubits = int(np.log2(n_actions))\n",
    "\n",
    "    qc = create_circuit(n_qubits, circ=type, fim=False, shots=None)\n",
    "\n",
    "    if type == \"simplified_two_design\":\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_init = torch.tensor(weights[0], requires_grad=False)\n",
    "        weights_tensor_params = torch.tensor(weights[1], requires_grad=True)\n",
    "        \n",
    "        probs = qc(weights_tensor_init,weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    else:\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_params = torch.tensor(weights, requires_grad=True)\n",
    "\n",
    "        probs = qc(weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    pi = policy(probs, policy_type=policy_type, n_actions=n_actions, n_qubits=n_qubits)\n",
    "    if clamp is not None:\n",
    "        pi = torch.clamp(pi, clamp, 1)\n",
    "\n",
    "    dist = torch.distributions.Categorical(probs=pi)\n",
    "    \n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    gradient_no_clamp = np.linalg.norm(compute_gradient(log_prob, weights_tensor_params), 2)\n",
    "    return gradient_no_clamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, env, n_episodes=1000, max_t=1000, gamma=1.0, print_every=5):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    average_scores = []\n",
    "    runtime_sum = 0\n",
    "    for e in range(1, n_episodes):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Sample the action from current policy\n",
    "            if t==0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = policy.sample(state_tensor)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # Total expected reward\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "    #standardized returns\n",
    "        R=0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0,R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + np.finfo(np.float32).eps)\n",
    "\n",
    "        for log_prob, R in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        policy_sum = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "    # Backpropagation\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        policy_sum.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        runtime = end_time-start_time\n",
    "        \n",
    "        runtime_sum += runtime\n",
    "        if e % print_every == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tLast {}\\tEpisodes average reward: {:.2f}\\tRuntime: {:.2f}'.format(e, scores_deque[-1], print_every, np.mean(scores_deque), runtime_sum))\n",
    "            runtime_sum = 0\n",
    "        if np.mean(scores_deque) == 500:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores, policy.gradient_list, average_scores  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "\n",
    "def tensor_to_list(tensor):\n",
    "    \"\"\"\n",
    "    Convert a tensor or numpy array to a nested list.\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, list):\n",
    "        return [tensor_to_list(t) for t in tensor]\n",
    "    elif isinstance(tensor, dict):\n",
    "        return {key: tensor_to_list(value) for key, value in tensor.items()}\n",
    "    elif isinstance(tensor, np.ndarray):\n",
    "        return tensor.tolist()\n",
    "    elif isinstance(tensor, torch.Tensor):\n",
    "        return tensor.tolist()\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "def measure_probs(qubits):\n",
    "    return qml.probs(wires=range(qubits)) \n",
    "\n",
    "def measure_expval_pairs(qubits):\n",
    "    expvals = []\n",
    "    for i in range(qubits // 2):\n",
    "        expvals.append(qml.expval(qml.PauliZ(2*i) @ qml.PauliZ(2*i + 1)))\n",
    "    return expvals\n",
    "    \n",
    "def create_optimizer_with_lr(params, lr_list):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': params, 'lr': lr} for params, lr in zip(params, lr_list)\n",
    "    ])\n",
    "    return optimizer\n",
    "\n",
    "def jerbi_circuit(n_qubits, n_layers, shots, input_scaling, diff_method, weight_init, input_init, measure, measure_qubits):\n",
    "\n",
    "    if shots is None:\n",
    "        dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "    else:\n",
    "        dev = qml.device(\"default.qubit\", wires=n_qubits, shots=shots)\n",
    "    \n",
    "    if n_layers < 1:\n",
    "        raise ValueError(\"Number of layers can't take values below 1\")\n",
    "    \n",
    "    weight_shapes = {\"params\": (n_layers + 1, n_qubits, 2),\n",
    "                    \"input_params\": (n_layers, n_qubits, 2)}\n",
    "    init_method   = {\"params\": weight_init,\n",
    "                    \"input_params\": input_init}\n",
    "    \n",
    "    @qml.qnode(dev, interface='torch', diff_method=diff_method)\n",
    "    def qnode(inputs, params, input_params):\n",
    "    #in case n_qubits != input length\n",
    "        if n_qubits > len(inputs) and n_qubits % len(inputs) == 0:\n",
    "            multiplier = n_qubits // len(inputs)\n",
    "            inputs = torch.cat([inputs] * multiplier)\n",
    "        elif n_qubits != len(inputs) and n_qubits % len(inputs) != 0:\n",
    "            raise ValueError('Number of qubits cannot be divided by input lenght')\n",
    "            \n",
    "    #hadamard\n",
    "        qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        \n",
    "        for layer in range(n_layers):\n",
    "            for wire in range(n_qubits):\n",
    "                qml.RZ(params[layer][wire][0], wires=wire)\n",
    "                qml.RY(params[layer][wire][1], wires=wire)\n",
    "\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "            if input_scaling:\n",
    "                for wire in range(n_qubits):\n",
    "                    qml.RY(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                    qml.RZ(input_params[layer][wire][1] * inputs[wire], wires=wire)\n",
    "            else:\n",
    "                for wire in range(n_qubits):\n",
    "                    qml.RY(inputs[wire], wires=wire)\n",
    "                    qml.RZ(inputs[wire], wires=wire)\n",
    "\n",
    "        for wire in range(n_qubits):\n",
    "            qml.RZ(params[-1][wire][0], wires=wire)\n",
    "            qml.RY(params[-1][wire][1], wires=wire)\n",
    "            \n",
    "        return measure(measure_qubits)\n",
    "\n",
    "    model = qml.qnn.TorchLayer(qnode, weight_shapes=weight_shapes, init_method=init_method)  \n",
    "    \n",
    "    return model\n",
    "    \n",
    "def S2D(n_qubits, n_layers, shots, input_scaling, diff_method, weight_init, input_init, measure_type, observables, measure_qubits):\n",
    "\n",
    "    dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "    observables = observables if observables is not None else None\n",
    "    \n",
    "    shapes = qml.SimplifiedTwoDesign.shape(n_layers=n_layers, n_wires=n_qubits)\n",
    "\n",
    "    weight_shapes = {\"params\": shapes[1],\n",
    "                     \"input_params\": shapes[0]}\n",
    "    \n",
    "    init_method   = {\"params\": weight_init,\n",
    "                     \"input_params\": input_init}\n",
    "\n",
    "    @qml.qnode(dev, interface='torch', diff_method='parameter-shift')\n",
    "    def qnode(inputs, params, input_params):\n",
    "\n",
    "        return measure_selection(measure_type, observables, measure_qubits)\n",
    "\n",
    "    model = qml.qnn.TorchLayer(qnode, weight_shapes=weight_shapes, init_method=init_method)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers,  shots= None, input_scaling=True, design='jerbi_circuit', diff_method = 'backprop', weight_init=torch.nn.init.uniform_, input_init = torch.nn.init.ones_, measure = None, measure_qubits = None):\n",
    "        super(CircuitGenerator, self).__init__()\n",
    "        '''\n",
    "\n",
    "        Creates a parameterized quantum circuit based on the arguments:\n",
    "\n",
    "            n_qubits(int) = Number of qubits\n",
    "            n_layers(int) = Number of layers (0 if no data re-uploading)\n",
    "            shots(int) = Number of times the circuit gets executed\n",
    "            input_scaling(bool) = Input parameters are used if True (input*input_params)\n",
    "            design(str) = The PQC ansatz design ('jerbi_circuit')\n",
    "            diff_method(str) = Differentiation method ('best', 'backprop', 'parameter-shift', ...)\n",
    "            weight_init (torch.nn.init) = How PQC weights are initialized (.uniform_, .ones_, ...)\n",
    "            input_init (torch.nn.init) = How input weights are initialized (.uniform_, .ones_, ...)\n",
    "            measure (function) = Measure function (measure_probs, measure_expval_pairs)\n",
    "            measure_qubits (int) = Number of qubits to be measured (in some cases might be equal to the number of qubits)\n",
    "            \n",
    "        '''\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.shots = shots\n",
    "        self.input_scaling = input_scaling\n",
    "        self.design = design\n",
    "        self.diff_method = diff_method\n",
    "        self.weight_init = weight_init\n",
    "        self.input_init = input_init\n",
    "        if measure is None:\n",
    "            self.measure = measure_probs\n",
    "        else:\n",
    "            self.measure = measure\n",
    "\n",
    "        if measure_qubits is None:\n",
    "            self.measure_qubits = n_qubits\n",
    "        else:\n",
    "            self.measure_qubits = measure_qubits\n",
    "\n",
    "        if self.design == 'jerbi_circuit':\n",
    "            self.circuit = jerbi_circuit(n_qubits = self.n_qubits,\n",
    "                                        n_layers = self.n_layers,\n",
    "                                        shots = self.shots,\n",
    "                                        input_scaling = self.input_scaling,\n",
    "                                        diff_method = self.diff_method,\n",
    "                                        weight_init = self.weight_init,\n",
    "                                        input_init = self.input_init,\n",
    "                                        measure = self.measure,\n",
    "                                        measure_qubits = self.measure_qubits)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported circuit type\")\n",
    "\n",
    "    def input(self,inputs):\n",
    "\n",
    "        outputs = self.circuit(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyType(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_qubits, n_actions, post_processing = 'raw_contiguous'):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_qubits = n_qubits\n",
    "        self.post_processing = post_processing\n",
    "        self.T = 1\n",
    "\n",
    "    def input(self,probs):\n",
    "        if self.post_processing == 'raw_contiguous':\n",
    "            policy = self.raw_contiguous(probs)\n",
    "        elif self.post_processing == 'raw_parity':\n",
    "            policy = self.raw_parity(probs)\n",
    "        elif self.post_processing == 'softmax':\n",
    "            policy = self.softmax(probs)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid post-processing method specified.\")\n",
    "        return policy\n",
    "\n",
    "    def raw_contiguous(self,probs):\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        chunk_size = len(probs_flatten) // self.n_actions\n",
    "        remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "        policy = []\n",
    "\n",
    "        for i in range(self.n_actions):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size\n",
    "\n",
    "            if i < remainder:\n",
    "                end += 1\n",
    "\n",
    "            # Update the original policy list instead of creating a new one\n",
    "            policy.append(sum(probs_flatten[start:end]))\n",
    "\n",
    "        policy_tensor = torch.stack(policy)\n",
    "        return policy_tensor\n",
    "        \n",
    "    def raw_parity(self,probs):\n",
    "\n",
    "        if self.n_actions % 2 != 0:\n",
    "            raise ValueError('For parity-like policy, n_actions must be an even number')\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        policy = torch.zeros(self.n_actions)\n",
    "        counter = 0\n",
    "        for prob in probs_flatten:\n",
    "            policy[counter] += prob\n",
    "            counter += 1\n",
    "            if counter == self.n_actions:\n",
    "                counter = 0\n",
    "        \n",
    "        policy_tensor = torch.stack(policy)\n",
    "\n",
    "        return policy_tensor\n",
    "    \n",
    "    def softmax(self, probs):\n",
    "        if len(probs) == self.n_actions:\n",
    "            scaled_output = probs * self.T\n",
    "            softmax_output = F.softmax(scaled_output, dim=0)\n",
    "            return softmax_output\n",
    "        else:\n",
    "            probs_flatten = probs.flatten()\n",
    "            chunk_size = len(probs_flatten) // self.n_actions\n",
    "            remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "            policy = []\n",
    "\n",
    "            for i in range(self.n_actions):\n",
    "                start = i * chunk_size\n",
    "                end = (i + 1) * chunk_size\n",
    "\n",
    "                if i < remainder:\n",
    "                    end += 1\n",
    "\n",
    "                # Update the original policy list instead of creating a new one\n",
    "                policy.append(sum(probs_flatten[start:end]))\n",
    "            policy_tensor = torch.stack(policy)\n",
    "            softmax_output = F.softmax(policy_tensor/self.T, dim=0)\n",
    "            return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumPolicyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, circuit, policy):\n",
    "        super(QuantumPolicyModel, self).__init__()\n",
    "        self.circuit = circuit\n",
    "        self.policy = policy\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Input state is fed to the circuit - its output is then fed to the post processing \n",
    "        '''\n",
    "        probs = self.circuit.input(inputs)\n",
    "        probs_processed = self.policy.input(probs)\n",
    "        return probs_processed\n",
    "    \n",
    "    def sample(self, inputs):\n",
    "        '''\n",
    "        Samples an action from the action probability distribution aka policy\n",
    "        '''\n",
    "        policy = self.forward(inputs)\n",
    "        dist = torch.distributions.Categorical(policy)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), policy\n",
    "    \n",
    "    def T_schedule(self,current_step,total_steps):\n",
    "\n",
    "        max_T = 1.0  # Initial temperature\n",
    "        min_T = 0.1  # Final temperature\n",
    "        decay_rate = 0.0025  # Decay rate\n",
    "\n",
    "        self.policy.T = max_T * (1 - decay_rate * current_step / total_steps)\n",
    "        self.policy.T = max(self.policy.T, min_T)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        '''\n",
    "        Returns the values of each set of parameters\n",
    "        '''\n",
    "        parameter_values = [param.clone().detach().numpy().flatten() for param in self.circuit.parameters()]\n",
    "        return parameter_values\n",
    "    \n",
    "    def get_gradients(self):\n",
    "        '''\n",
    "        Returns the gradient values of each set of parameters\n",
    "        '''\n",
    "        gradients = [torch.flatten(param.grad.clone().detach()) if param.grad is not None else torch.flatten(torch.zeros_like(param)) for name, param in self.circuit.named_parameters()]\n",
    "        return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceUpdate():\n",
    "\n",
    "    def __init__(self, pqc, optimizer, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name = None):\n",
    "        \n",
    "        self.pqc = pqc\n",
    "        self.optimizer = optimizer\n",
    "        self.env = env\n",
    "        self.env_name = env_name\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.scores_deque = deque(maxlen=print_every)\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "        self.file_name = file_name\n",
    "        self.running_reward = 10\n",
    "\n",
    "    def get_trajectory(self):\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(self.max_t):\n",
    "            if t == 0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = self.pqc.sample(state_tensor)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            \n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                self.scores_deque.append(sum(self.rewards))\n",
    "                break\n",
    "\n",
    "    def update_policy(self):\n",
    "\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        for log_prob, ret in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * ret)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        self.loss = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()    \n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def save_episode_data(self,path):\n",
    "        '''\n",
    "        current_directory = os.path.dirname(__file__)\n",
    "        folder_name = f\"{str(self.env_name)}_{self.pqc.policy.post_processing}_{self.pqc.circuit.n_layers}\"\n",
    "        folder_path = os.path.join(current_directory, folder_name)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        '''\n",
    "\n",
    "        agent_variables = {\n",
    "            \"Number of Qubits\": self.pqc.circuit.n_qubits,\n",
    "            \"Number of Layers\": self.pqc.circuit.n_layers,\n",
    "            \"Shots\": self.pqc.circuit.shots,\n",
    "            \"Input Scaling\": self.pqc.circuit.input_scaling,\n",
    "            \"Design\": self.pqc.circuit.design,\n",
    "            \"Differentiation Method\": self.pqc.circuit.diff_method,\n",
    "            \"Weight Initiation\": str(self.pqc.circuit.weight_init),\n",
    "            \"Input_init\": str(self.pqc.circuit.input_init),\n",
    "            \"Measure\": str(self.pqc.circuit.measure),\n",
    "            \"Measure Qubits\": self.pqc.circuit.measure_qubits,\n",
    "            \"Policy Type\": self.pqc.policy.post_processing,\n",
    "            \"Optimizers\": str(self.optimizer),\n",
    "            \"Envinronment Name\": str(self.env_name),\n",
    "            \"Gamma (discounting factor)\": self.gamma,\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(path, \"agent_characteristics.json\"), \"w\") as f:\n",
    "            json.dump(agent_variables, f, indent=4)\n",
    "        '''\n",
    "        episode_data = {\n",
    "                \"episode_reward\": self.scores_deque[-1],\n",
    "                \"episode_length\": len(self.rewards),\n",
    "                \"runtime\": self.runtime,\n",
    "                \"loss\": self.loss.item(),\n",
    "                \"gradient\": tensor_to_list(self.pqc.get_gradients()),\n",
    "                \"parameters\": tensor_to_list(self.pqc.get_parameters())\n",
    "        }\n",
    "\n",
    "        if folder_path is not None:\n",
    "            file_path = os.path.join(self.folder_path, f\"{self.file_name}.json\")\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, 'r') as f:\n",
    "                    existing_data = json.load(f)\n",
    "                existing_data.append(episode_data)\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump(existing_data, f, indent=4)\n",
    "            else:\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump([episode_data], f, indent=4)\n",
    "        '''\n",
    "        \n",
    "    def writer_function(self, writer, iteration):\n",
    "\n",
    "        writer.add_scalar(\"Episode Reward\", np.mean(self.scores_deque), global_step=iteration)\n",
    "        writer.add_scalar(\"Runtime\", self.runtime, global_step=iteration)\n",
    "        writer.add_scalar(\"Loss\", self.loss.item(), global_step=iteration)\n",
    "        writer.add_scalar(\"Gradients1\", np.mean(tensor_to_list(self.pqc.get_gradients()[0])), global_step=iteration)\n",
    "        writer.add_scalar(\"Gradients2\", np.mean(tensor_to_list(self.pqc.get_gradients()[1])), global_step=iteration)\n",
    "                    \n",
    "    def train(self):\n",
    "        \n",
    "        logs_dir = \"data\"\n",
    "        os.makedirs(logs_dir, exist_ok=True)\n",
    "        experiment_folder = f\"{self.pqc.policy.post_processing}_{self.pqc.circuit.n_layers}\"\n",
    "        experiment_path = os.path.join(logs_dir, experiment_folder)\n",
    "        os.makedirs(experiment_path, exist_ok=True)\n",
    "        run = os.path.join(experiment_path,str(self.file_name))\n",
    "        os.makedirs(run, exist_ok=True)\n",
    "        writer = SummaryWriter(log_dir=run)\n",
    "        self.save_episode_data(experiment_path)\n",
    "        \n",
    "        for i in range(1, self.n_episodes):\n",
    "            start_time = time.time()\n",
    "            self.get_trajectory()\n",
    "            self.pqc.T_schedule(i,self.n_episodes)\n",
    "            self.update_policy()\n",
    "            end_time = time.time()\n",
    "            self.runtime = end_time - start_time\n",
    "            self.writer_function(writer,i)\n",
    "            self.running_reward = (self.running_reward * 0.99) + (len(self.rewards) * 0.01)\n",
    "            \n",
    "            if self.running_reward > self.env.spec.reward_threshold:\n",
    "                print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i, np.mean(self.scores_deque)))\n",
    "                break\n",
    "            elif i % self.print_every == 0 and self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tLast {} Episodes average reward: {:.2f}\\tRuntime: {:.2f}\\t {:.2f}\\t'.format(i, self.scores_deque[-1], self.print_every, np.mean(self.scores_deque), self.runtime, self.running_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (828706188.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class Analysis():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bernardo\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5\tLast reward: 19.00\tLast 5 Episodes average reward: 16.60\tRuntime: 0.22\t 10.32\t\n",
      "Episode 10\tLast reward: 26.00\tLast 5 Episodes average reward: 18.00\tRuntime: 0.32\t 10.70\t\n",
      "Episode 15\tLast reward: 16.00\tLast 5 Episodes average reward: 18.60\tRuntime: 0.18\t 11.09\t\n",
      "Episode 20\tLast reward: 13.00\tLast 5 Episodes average reward: 16.00\tRuntime: 0.15\t 11.33\t\n",
      "Episode 25\tLast reward: 72.00\tLast 5 Episodes average reward: 26.80\tRuntime: 0.87\t 12.10\t\n",
      "Episode 30\tLast reward: 25.00\tLast 5 Episodes average reward: 20.20\tRuntime: 0.29\t 12.50\t\n",
      "Episode 35\tLast reward: 40.00\tLast 5 Episodes average reward: 26.20\tRuntime: 0.47\t 13.17\t\n",
      "Episode 40\tLast reward: 60.00\tLast 5 Episodes average reward: 34.60\tRuntime: 0.71\t 14.23\t\n",
      "Episode 45\tLast reward: 13.00\tLast 5 Episodes average reward: 26.20\tRuntime: 0.15\t 14.82\t\n",
      "Episode 50\tLast reward: 22.00\tLast 5 Episodes average reward: 22.40\tRuntime: 0.25\t 15.19\t\n",
      "Episode 55\tLast reward: 11.00\tLast 5 Episodes average reward: 17.00\tRuntime: 0.13\t 15.27\t\n",
      "Episode 60\tLast reward: 10.00\tLast 5 Episodes average reward: 13.00\tRuntime: 0.12\t 15.16\t\n",
      "Episode 65\tLast reward: 18.00\tLast 5 Episodes average reward: 17.20\tRuntime: 0.21\t 15.26\t\n",
      "Episode 70\tLast reward: 11.00\tLast 5 Episodes average reward: 18.60\tRuntime: 0.13\t 15.42\t\n",
      "Episode 75\tLast reward: 12.00\tLast 5 Episodes average reward: 20.20\tRuntime: 0.14\t 15.66\t\n",
      "Episode 80\tLast reward: 20.00\tLast 5 Episodes average reward: 23.80\tRuntime: 0.25\t 16.06\t\n",
      "Episode 85\tLast reward: 12.00\tLast 5 Episodes average reward: 16.40\tRuntime: 0.14\t 16.07\t\n",
      "Episode 90\tLast reward: 20.00\tLast 5 Episodes average reward: 26.60\tRuntime: 0.24\t 16.59\t\n",
      "Episode 95\tLast reward: 80.00\tLast 5 Episodes average reward: 27.60\tRuntime: 0.96\t 17.15\t\n",
      "Episode 100\tLast reward: 38.00\tLast 5 Episodes average reward: 21.60\tRuntime: 0.44\t 17.37\t\n",
      "Episode 105\tLast reward: 36.00\tLast 5 Episodes average reward: 21.00\tRuntime: 0.42\t 17.55\t\n",
      "Episode 110\tLast reward: 45.00\tLast 5 Episodes average reward: 25.80\tRuntime: 0.54\t 17.96\t\n",
      "Episode 115\tLast reward: 11.00\tLast 5 Episodes average reward: 32.00\tRuntime: 0.13\t 18.64\t\n",
      "Episode 120\tLast reward: 19.00\tLast 5 Episodes average reward: 22.20\tRuntime: 0.23\t 18.82\t\n",
      "Episode 125\tLast reward: 10.00\tLast 5 Episodes average reward: 27.80\tRuntime: 0.12\t 19.25\t\n",
      "Episode 130\tLast reward: 9.00\tLast 5 Episodes average reward: 27.40\tRuntime: 0.11\t 19.63\t\n",
      "Episode 135\tLast reward: 15.00\tLast 5 Episodes average reward: 16.00\tRuntime: 0.18\t 19.46\t\n",
      "Episode 140\tLast reward: 16.00\tLast 5 Episodes average reward: 18.00\tRuntime: 0.20\t 19.38\t\n",
      "Episode 145\tLast reward: 18.00\tLast 5 Episodes average reward: 18.60\tRuntime: 0.22\t 19.35\t\n",
      "Episode 150\tLast reward: 25.00\tLast 5 Episodes average reward: 27.80\tRuntime: 0.30\t 19.76\t\n",
      "Episode 155\tLast reward: 30.00\tLast 5 Episodes average reward: 20.40\tRuntime: 0.38\t 19.80\t\n",
      "Episode 160\tLast reward: 23.00\tLast 5 Episodes average reward: 22.80\tRuntime: 0.28\t 19.94\t\n",
      "Episode 165\tLast reward: 11.00\tLast 5 Episodes average reward: 22.60\tRuntime: 0.14\t 20.07\t\n",
      "Episode 170\tLast reward: 27.00\tLast 5 Episodes average reward: 34.40\tRuntime: 0.34\t 20.78\t\n",
      "Episode 175\tLast reward: 15.00\tLast 5 Episodes average reward: 22.20\tRuntime: 0.20\t 20.85\t\n",
      "Episode 180\tLast reward: 17.00\tLast 5 Episodes average reward: 27.20\tRuntime: 0.20\t 21.15\t\n",
      "Episode 185\tLast reward: 55.00\tLast 5 Episodes average reward: 30.20\tRuntime: 0.66\t 21.60\t\n",
      "Episode 190\tLast reward: 25.00\tLast 5 Episodes average reward: 30.20\tRuntime: 0.30\t 22.02\t\n",
      "Episode 195\tLast reward: 21.00\tLast 5 Episodes average reward: 20.60\tRuntime: 0.25\t 21.95\t\n",
      "Episode 200\tLast reward: 56.00\tLast 5 Episodes average reward: 37.80\tRuntime: 0.68\t 22.74\t\n",
      "Episode 205\tLast reward: 18.00\tLast 5 Episodes average reward: 27.00\tRuntime: 0.22\t 22.95\t\n",
      "Episode 210\tLast reward: 41.00\tLast 5 Episodes average reward: 33.40\tRuntime: 0.49\t 23.47\t\n",
      "Episode 215\tLast reward: 19.00\tLast 5 Episodes average reward: 34.20\tRuntime: 0.23\t 23.99\t\n",
      "Episode 220\tLast reward: 14.00\tLast 5 Episodes average reward: 21.20\tRuntime: 0.17\t 23.85\t\n",
      "Episode 225\tLast reward: 17.00\tLast 5 Episodes average reward: 26.60\tRuntime: 0.21\t 23.98\t\n",
      "Episode 230\tLast reward: 14.00\tLast 5 Episodes average reward: 29.40\tRuntime: 0.17\t 24.25\t\n",
      "Episode 235\tLast reward: 15.00\tLast 5 Episodes average reward: 25.00\tRuntime: 0.18\t 24.28\t\n",
      "Episode 240\tLast reward: 77.00\tLast 5 Episodes average reward: 47.60\tRuntime: 0.94\t 25.43\t\n",
      "Episode 245\tLast reward: 14.00\tLast 5 Episodes average reward: 25.40\tRuntime: 0.17\t 25.42\t\n",
      "Episode 250\tLast reward: 52.00\tLast 5 Episodes average reward: 33.60\tRuntime: 0.65\t 25.83\t\n",
      "Episode 255\tLast reward: 24.00\tLast 5 Episodes average reward: 23.20\tRuntime: 0.30\t 25.70\t\n",
      "Episode 260\tLast reward: 11.00\tLast 5 Episodes average reward: 22.20\tRuntime: 0.14\t 25.52\t\n",
      "Episode 265\tLast reward: 12.00\tLast 5 Episodes average reward: 26.00\tRuntime: 0.15\t 25.54\t\n",
      "Episode 270\tLast reward: 53.00\tLast 5 Episodes average reward: 32.60\tRuntime: 0.64\t 25.90\t\n",
      "Episode 275\tLast reward: 41.00\tLast 5 Episodes average reward: 24.20\tRuntime: 0.49\t 25.82\t\n",
      "Episode 280\tLast reward: 21.00\tLast 5 Episodes average reward: 22.80\tRuntime: 0.26\t 25.67\t\n",
      "Episode 285\tLast reward: 53.00\tLast 5 Episodes average reward: 31.00\tRuntime: 0.63\t 25.93\t\n",
      "Episode 290\tLast reward: 28.00\tLast 5 Episodes average reward: 21.60\tRuntime: 0.34\t 25.72\t\n",
      "Episode 295\tLast reward: 54.00\tLast 5 Episodes average reward: 27.60\tRuntime: 0.65\t 25.82\t\n",
      "Episode 300\tLast reward: 12.00\tLast 5 Episodes average reward: 27.60\tRuntime: 0.15\t 25.91\t\n",
      "Episode 305\tLast reward: 22.00\tLast 5 Episodes average reward: 27.40\tRuntime: 0.27\t 25.98\t\n",
      "Episode 310\tLast reward: 14.00\tLast 5 Episodes average reward: 20.60\tRuntime: 0.17\t 25.71\t\n",
      "Episode 315\tLast reward: 84.00\tLast 5 Episodes average reward: 28.00\tRuntime: 1.01\t 25.84\t\n",
      "Episode 320\tLast reward: 39.00\tLast 5 Episodes average reward: 30.40\tRuntime: 0.47\t 26.06\t\n",
      "Episode 325\tLast reward: 15.00\tLast 5 Episodes average reward: 29.80\tRuntime: 0.19\t 26.24\t\n",
      "Episode 330\tLast reward: 55.00\tLast 5 Episodes average reward: 25.40\tRuntime: 0.66\t 26.21\t\n",
      "Episode 335\tLast reward: 12.00\tLast 5 Episodes average reward: 21.40\tRuntime: 0.15\t 25.97\t\n",
      "Episode 340\tLast reward: 24.00\tLast 5 Episodes average reward: 14.80\tRuntime: 0.30\t 25.43\t\n",
      "Episode 345\tLast reward: 9.00\tLast 5 Episodes average reward: 24.60\tRuntime: 0.12\t 25.38\t\n",
      "Episode 350\tLast reward: 25.00\tLast 5 Episodes average reward: 25.00\tRuntime: 0.30\t 25.36\t\n",
      "Episode 355\tLast reward: 24.00\tLast 5 Episodes average reward: 14.60\tRuntime: 0.29\t 24.84\t\n",
      "Episode 360\tLast reward: 11.00\tLast 5 Episodes average reward: 27.00\tRuntime: 0.14\t 24.94\t\n",
      "Episode 365\tLast reward: 30.00\tLast 5 Episodes average reward: 25.80\tRuntime: 0.37\t 24.98\t\n",
      "Episode 370\tLast reward: 12.00\tLast 5 Episodes average reward: 16.20\tRuntime: 0.14\t 24.55\t\n",
      "Episode 375\tLast reward: 26.00\tLast 5 Episodes average reward: 28.80\tRuntime: 0.32\t 24.75\t\n",
      "Episode 380\tLast reward: 23.00\tLast 5 Episodes average reward: 26.40\tRuntime: 0.29\t 24.83\t\n",
      "Episode 385\tLast reward: 41.00\tLast 5 Episodes average reward: 35.60\tRuntime: 0.50\t 25.37\t\n",
      "Episode 390\tLast reward: 18.00\tLast 5 Episodes average reward: 24.20\tRuntime: 0.22\t 25.30\t\n",
      "Episode 395\tLast reward: 49.00\tLast 5 Episodes average reward: 41.20\tRuntime: 0.59\t 26.08\t\n",
      "Episode 400\tLast reward: 54.00\tLast 5 Episodes average reward: 36.80\tRuntime: 0.65\t 26.61\t\n",
      "Episode 405\tLast reward: 39.00\tLast 5 Episodes average reward: 29.20\tRuntime: 0.47\t 26.73\t\n",
      "Episode 410\tLast reward: 14.00\tLast 5 Episodes average reward: 22.60\tRuntime: 0.17\t 26.53\t\n",
      "Episode 415\tLast reward: 36.00\tLast 5 Episodes average reward: 34.20\tRuntime: 0.44\t 26.91\t\n",
      "Episode 420\tLast reward: 36.00\tLast 5 Episodes average reward: 38.60\tRuntime: 0.44\t 27.48\t\n",
      "Episode 425\tLast reward: 13.00\tLast 5 Episodes average reward: 18.40\tRuntime: 0.16\t 27.03\t\n",
      "Episode 430\tLast reward: 35.00\tLast 5 Episodes average reward: 28.00\tRuntime: 0.43\t 27.08\t\n",
      "Episode 435\tLast reward: 10.00\tLast 5 Episodes average reward: 26.00\tRuntime: 0.13\t 27.03\t\n",
      "Episode 440\tLast reward: 12.00\tLast 5 Episodes average reward: 28.20\tRuntime: 0.15\t 27.08\t\n",
      "Episode 445\tLast reward: 24.00\tLast 5 Episodes average reward: 25.20\tRuntime: 0.29\t 26.99\t\n",
      "Episode 450\tLast reward: 9.00\tLast 5 Episodes average reward: 21.60\tRuntime: 0.11\t 26.72\t\n",
      "Episode 455\tLast reward: 26.00\tLast 5 Episodes average reward: 30.40\tRuntime: 0.31\t 26.90\t\n",
      "Episode 460\tLast reward: 14.00\tLast 5 Episodes average reward: 15.20\tRuntime: 0.18\t 26.33\t\n",
      "Episode 465\tLast reward: 32.00\tLast 5 Episodes average reward: 21.60\tRuntime: 0.38\t 26.10\t\n",
      "Episode 470\tLast reward: 36.00\tLast 5 Episodes average reward: 25.60\tRuntime: 0.43\t 26.08\t\n",
      "Episode 475\tLast reward: 19.00\tLast 5 Episodes average reward: 23.60\tRuntime: 0.23\t 25.95\t\n",
      "Episode 480\tLast reward: 63.00\tLast 5 Episodes average reward: 37.80\tRuntime: 0.77\t 26.54\t\n",
      "Episode 485\tLast reward: 12.00\tLast 5 Episodes average reward: 48.60\tRuntime: 0.16\t 27.61\t\n",
      "Episode 490\tLast reward: 32.00\tLast 5 Episodes average reward: 38.00\tRuntime: 0.39\t 28.11\t\n",
      "Episode 495\tLast reward: 22.00\tLast 5 Episodes average reward: 23.20\tRuntime: 0.26\t 27.87\t\n"
     ]
    }
   ],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 1      #set to 1 if data_reuploading is off\n",
    "n_actions = 2\n",
    "shots = None\n",
    "input_scaling = False\n",
    "design = 'jerbi_circuit' \n",
    "diff_method = 'adjoint' \n",
    "weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(-np.pi, np.pi)\n",
    "input_init = torch.nn.init.uniform_\n",
    "measure = measure_probs\n",
    "measure_qubits = None\n",
    "circuit = CircuitGenerator(n_qubits, n_layers, weight_init=weight_init)\n",
    "\n",
    "post_processing = 'softmax'\n",
    "policy_type = PolicyType(n_qubits, n_actions)\n",
    "\n",
    "\n",
    "pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "\n",
    "pqc = pqc\n",
    "lr_list= [0.01,0.08]\n",
    "params= circuit.parameters()\n",
    "optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)\n",
    "n_episodes = 500\n",
    "max_t = 500\n",
    "gamma = 0.98\n",
    "print_every = 5\n",
    "verbose = 1\n",
    "\n",
    "reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Agent 0: Training completed', 'Agent 1: Training completed', 'Agent 2: Training completed', 'Agent 3: Training completed', 'Agent 4: Training completed', 'Agent 5: Training completed', 'Agent 6: Training completed', 'Agent 7: Training completed', 'Agent 8: Training completed', 'Agent 9: Training completed']\n"
     ]
    }
   ],
   "source": [
    "# Parallel runs\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agents(agent_id, pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name):\n",
    "\n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, file_name)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {agent_id}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 1      #set to 1 if data_reuploading is off\n",
    "    n_actions = 2\n",
    "    shots = None\n",
    "    input_scaling = False\n",
    "    design = 'jerbi_circuit' \n",
    "    diff_method = 'adjoint' \n",
    "    weight_init = lambda shape, dtype=torch.float: torch.FloatTensor(shape).uniform_(0, np.pi)\n",
    "    input_init = torch.nn.init.uniform_\n",
    "    measure = measure_probs\n",
    "    measure_qubits = None\n",
    "    circuit = CircuitGenerator(n_qubits, n_layers, weight_init=weight_init)\n",
    "\n",
    "\n",
    "    post_processing = 'softmax'\n",
    "    policy_type = PolicyType(n_qubits, n_actions)\n",
    "\n",
    "\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "    \n",
    "\n",
    "    pqc = pqc\n",
    "    lr_list= [0.01,0.08]\n",
    "    params= circuit.parameters()\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 500\n",
    "    max_t = 500\n",
    "    gamma = 0.98\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "        \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agents)(i, pqc, optimizers, env, env_name, n_episodes, max_t, gamma, print_every, verbose, i) for i in range(num_agents))\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the folder containing JSON files\n",
    "path = os.getcwd()\n",
    "folder_name = 'CartPole-v1_raw_contiguous_8'\n",
    "folder_path = os.path.join(path,folder_name)\n",
    "\n",
    "json_files = [file for file in os.listdir(folder_path) if file.endswith('.json')]\n",
    "\n",
    "# Initialize an empty list to store episode rewards from all agents\n",
    "episode_rewards = []\n",
    "\n",
    "# Iterate through each JSON file\n",
    "for json_file in json_files:\n",
    "    with open(os.path.join(folder_path, json_file), 'r') as f:\n",
    "        # Load JSON data\n",
    "        data = json.load(f)\n",
    "        # Check if data is a list\n",
    "        if isinstance(data, list):\n",
    "            # Extract episode rewards for each agent\n",
    "            agent_rewards = [agent[\"episode_reward\"] for agent in data]\n",
    "            # Add episode rewards to the list\n",
    "            episode_rewards.append(agent_rewards)\n",
    "        else:\n",
    "            print(f\"Invalid JSON data in file {json_file}\")\n",
    "\n",
    "# Calculate the average episode reward for each episode across agents\n",
    "average_rewards = np.mean(np.array(episode_rewards), axis=0)\n",
    "# Plot the average rewards\n",
    "plt.plot(range(1, len(average_rewards) + 1), average_rewards, linestyle='-')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Episode Reward')\n",
    "plt.title('Average Episode Rewards Across Agents')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs[1].fill_between(np.arange(len(skolik_datareup_acrobot_mean_return)), np.clip(skolik_datareup_acrobot_mean_return - skolik_datareup_acrobot_std_return,a_min=-500,a_max=0), np.clip(skolik_datareup_acrobot_mean_return + skolik_datareup_acrobot_std_return,a_min=-500,a_max=0), alpha=0.2, color=\"red\")\n",
    "axs[1].fill_between(np.arange(len(skolik_datareup_acrobot_mean_return)), np.clip(skolik_datareup_no_in_acrobot_mean_return - skolik_datareup_no_in_acrobot_std_return,a_min=-500,a_max=0), np.clip(skolik_datareup_no_in_acrobot_mean_return + skolik_datareup_no_in_acrobot_std_return,a_min=-500,a_max=0), alpha=0.2, color=\"green\")\n",
    "axs[1].fill_between(np.arange(len(skolik_datareup_acrobot_mean_return)), np.clip(skolik_datareup_no_out_acrobot_mean_return - skolik_datareup_no_out_acrobot_std_return,a_min=-500,a_max=0), np.clip(skolik_datareup_no_out_acrobot_mean_return + skolik_datareup_no_out_acrobot_std_return,a_min=-500,a_max=0), alpha=0.2, color=\"blue\")\n",
    "axs[1].fill_between(np.arange(len(skolik_datareup_acrobot_mean_return)), np.clip(skolik_datareup_no_in_out_acrobot_mean_return - skolik_datareup_no_in_out_acrobot_std_return,a_min=-500,a_max=0), np.clip(skolik_datareup_no_in_out_acrobot_mean_return + skolik_datareup_no_in_out_acrobot_std_return,a_min=-500,a_max=0), alpha=0.2, color=\"purple\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.title('Score per episode')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(average_scores)+1), average_scores)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Average score of the previous 10 episodes')\n",
    "plt.xlabel('Average of previous 10 #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1_gradients = gradient_list[::2]\n",
    "param2_gradients = gradient_list[1::2]\n",
    "\n",
    "# Plotting gradients for parameters 1\n",
    "plt.plot(param1_gradients, label='Parameters')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Gradient Mean')\n",
    "plt.title('Gradients of Parameters')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting gradients for parameters 2\n",
    "plt.plot(param2_gradients, label='Parameters')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Gradient Mean')\n",
    "plt.title('Gradients of Input Parameters')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_runtimes = np.cumsum([runtime / 60 for runtime in runtimes])\n",
    "\n",
    "plt.plot(cumulative_runtimes)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Cumulative Runtime (Minutes)')\n",
    "plt.title('Cumulative Runtimes')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import os\n",
    "import inspect\n",
    "import gym\n",
    "import glob\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "from IPython.display import HTML, display\n",
    "import torch\n",
    "\n",
    "def show_video_of_model(policy, env_name):\n",
    "    current_dir = os.path.abspath('')\n",
    "    video_dir = os.path.join(current_dir, 'video')\n",
    "    os.makedirs(video_dir, exist_ok=True)  # Ensure video directory exists\n",
    "\n",
    "    # Create the Gym environment with render mode\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    video_path = os.path.join(video_dir, '{}.mp4'.format(env_name))\n",
    "    vid = video_recorder.VideoRecorder(env, path=video_path)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    for t in range(1000):\n",
    "        # Convert state to tensor if needed\n",
    "        state_tensor = torch.tensor(state[0]).float() if t == 0 else torch.tensor(state).float()\n",
    "        \n",
    "        # Capture frame\n",
    "        vid.capture_frame()\n",
    "        \n",
    "        # Sample action from policy\n",
    "        action, log_prob, _, = policy.sample(state_tensor)\n",
    "        \n",
    "        # Take action in the environment\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Close video recorder and environment\n",
    "    vid.close()\n",
    "    env.close()\n",
    "    \n",
    "    print(\"Video saved at:\", video_path)\n",
    "\n",
    "def show_video(env_name):\n",
    "    current_dir = os.path.abspath('')\n",
    "    video_dir = os.path.join(current_dir, 'video')\n",
    "    mp4list = glob.glob(os.path.join(video_dir, '*.mp4'))\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = os.path.join(video_dir, '{}.mp4'.format(env_name))\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "\n",
    "# Example usage\n",
    "# show_video_of_model(pqc, 'CartPole-v1')\n",
    "# show_video('CartPole-v1')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
