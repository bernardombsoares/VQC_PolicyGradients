{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pennylane as qml \n",
    "import torch\n",
    "\n",
    "def create_circuit(n_qubits,n_layers=None,circ = \"simplified_two_design\",fim=False, shots=None):\n",
    "\n",
    "    dev = qml.device(\"default.qubit.torch\", wires=n_qubits, shots=shots)\n",
    "\n",
    "    def RZRY(params):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        #qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        #qml.AngleEmbedding(params,wires=range(n_qubits))\n",
    "        for q in range(n_qubits):\n",
    "            qml.Hadamard(wires=q)\n",
    "\n",
    "        for w in range(n_layers): \n",
    "            for q in range(n_qubits):\n",
    "                index = w * (2*n_qubits) + q * 2\n",
    "                qml.RZ(params[index],wires=q)\n",
    "                qml.RY(params[index + 1],wires=q)\n",
    "        \n",
    "        qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        return qml.probs(wires=range(n_qubits))\n",
    "\n",
    "    def S2D(init_params,params,measurement_qubits=0,prod_approx=False):\n",
    "        #qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        qml.SimplifiedTwoDesign(initial_layer_weights=init_params, weights=params, wires=range(n_qubits))\n",
    "        \n",
    "        #qml.broadcast(qml.CNOT , wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        if not prod_approx:\n",
    "            return qml.probs(wires=list(range(measurement_qubits)))\n",
    "        else:\n",
    "            return [qml.probs(i) for i in range(measurement_qubits)]\n",
    "\n",
    "    def SU(params):\n",
    "        qml.SpecialUnitary(params, wires=range(n_qubits))\n",
    "        \n",
    "        ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "        for i in range(2,n_qubits):\n",
    "            ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "        return qml.expval(ZZ)\n",
    "    \n",
    "    def simmpleRZRY(params,cnots=True):\n",
    "        qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RZ, wires=range(n_qubits), pattern=\"single\", parameters=params[0])\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params[1])\n",
    "        if cnots:\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "            return qml.expval(qml.PauliZ(n_qubits-1))\n",
    "        else:\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "\n",
    "            return qml.expval(ZZ)\n",
    "        \n",
    "    def RY(params,y=True,probs=False,prod=False, entanglement=None):\n",
    "        #qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        qml.broadcast(qml.RY, wires=range(n_qubits), pattern=\"single\", parameters=params)\n",
    "        #qml.broadcast(qml.CZ, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "\n",
    "        if entanglement==\"all_to_all\":\n",
    "            qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"all_to_all\")\n",
    "        \n",
    "        if y==True:\n",
    "            #YY = qml.operation.Tensor(qml.PauliY(0), qml.PauliY(1))\n",
    "            YY = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                #YY = qml.operation.Tensor(YY, qml.PauliY(i))\n",
    "                YY.append(qml.PauliZ(i))\n",
    "            \n",
    "            #return [qml.expval(i) for i in YY]\n",
    "            return qml.expval(YY)\n",
    "\n",
    "        elif probs==False:\n",
    "\n",
    "            ZZ = qml.operation.Tensor(qml.PauliZ(0), qml.PauliZ(1))\n",
    "            #ZZ = [qml.PauliZ(0), qml.PauliZ(1)]\n",
    "            for i in range(2,n_qubits):\n",
    "                ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))        \n",
    "                #ZZ.append(qml.PauliZ(i))        \n",
    "\n",
    "            #return [qml.expval(i) for i in ZZ]\n",
    "            return qml.expval(ZZ)\n",
    "\n",
    "        else:\n",
    "            if prod:\n",
    "                return [qml.probs(i) for i in range(n_qubits)]\n",
    "            else:\n",
    "                return qml.probs(wires=range(n_qubits))\n",
    "            \n",
    "        \n",
    "        \n",
    "    def GHZ(params,measurement_qubits=0):\n",
    "        qml.RY(params,wires=0)\n",
    "        qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "\n",
    "    def random_product_state(params,gate_sequence=None):\n",
    "                \n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(np.pi / 4, wires=i)\n",
    "\n",
    "        for ll in range(len(params)):\n",
    "\n",
    "            for i in range(n_qubits):\n",
    "                gate_sequence[\"{}{}\".format(ll,i)](params[ll][i], wires=i)\n",
    "\n",
    "            #for i in range(n_qubits - 1):\n",
    "                #qml.CZ(wires=[i, i + 1])\n",
    "    def SEL(params, measurement_qubits=0):\n",
    "        qml.StronglyEntanglingLayers(params, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    def RL(params, measurement_qubits=0):\n",
    "        qml.RandomLayers(params, ratio_imprim=0.8 ,imprimitive=qml.CZ, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(measurement_qubits))\n",
    "    \n",
    "    if circ == \"rzry\":\n",
    "        qcircuit = RZRY\n",
    "    elif circ == \"simplified_two_design\":\n",
    "        qcircuit = S2D\n",
    "    elif circ == \"special_unitary\":\n",
    "        qcircuit = SU\n",
    "    elif circ == \"simpleRZRY\":\n",
    "        qcircuit = simmpleRZRY\n",
    "    elif circ == \"RY\":\n",
    "        qcircuit = RY\n",
    "    elif circ == \"ghz\":\n",
    "        qcircuit = GHZ\n",
    "    elif circ == \"random_product_state\":\n",
    "        qcircuit = random_product_state\n",
    "    elif circ == \"SEL\":\n",
    "        qcircuit = SEL\n",
    "    elif circ == \"RL\":\n",
    "        qcircuit = RL\n",
    "    if not fim:\n",
    "        circuit = qml.QNode(qcircuit, dev,interface=\"torch\", diff_method=\"backprop\")\n",
    "    else:\n",
    "        circuit = qml.QNode(qcircuit, dev)\n",
    "\n",
    "    return circuit\n",
    "\n",
    "def compute_gradient(log_prob, w):\n",
    "    \"\"\"Compute gradient of the log probability with respect to weights.\n",
    "    \n",
    "    Args:\n",
    "    - log_prob (torch.Tensor): The log probability tensor.\n",
    "    - w (torch.Tensor): The weights tensor, with requires_grad=True.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The gradient of log_prob with respect to w, flattened.\n",
    "    \"\"\"\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()\n",
    "    log_prob.backward(retain_graph=True)\n",
    "    \n",
    "    if w.grad is None:\n",
    "        raise ValueError(\"The gradient for the given log_prob with respect to w is None.\")\n",
    "    \n",
    "    return w.grad.view(-1).detach().numpy()\n",
    "\n",
    "def policy(probs, policy_type=\"contiguous-like\", n_actions=2, n_qubits=1):\n",
    "\n",
    "    if policy_type == \"contiguous-like\":\n",
    "        return probs\n",
    "    elif policy_type == \"parity-like\":\n",
    "        policy = torch.zeros(n_actions)\n",
    "        for i in range(len(probs)):\n",
    "            a=[]\n",
    "            for m in range(int(np.log2(n_actions))):\n",
    "                if m==0:    \n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)\n",
    "                else:\n",
    "                    bitstring = np.binary_repr(i,width=n_qubits)[:-m]\n",
    "                \n",
    "                a.append(bitstring.count(\"1\") % 2)\n",
    "            policy[int(\"\".join(str(x) for x in a),2)] += probs[i]\n",
    "\n",
    "        return policy    \n",
    "    \n",
    "def compute_policy_and_gradient(args):\n",
    "    n_qubits, shapes, type , n_actions, policy_type, clamp = args\n",
    "\n",
    "    if policy_type == \"parity-like\":\n",
    "        measure_qubits = n_qubits\n",
    "    else:\n",
    "        measure_qubits = int(np.log2(n_actions))\n",
    "\n",
    "    qc = create_circuit(n_qubits, circ=type, fim=False, shots=None)\n",
    "\n",
    "    if type == \"simplified_two_design\":\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_init = torch.tensor(weights[0], requires_grad=False)\n",
    "        weights_tensor_params = torch.tensor(weights[1], requires_grad=True)\n",
    "        \n",
    "        probs = qc(weights_tensor_init,weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    else:\n",
    "        weights = [np.random.uniform(-np.pi,np.pi,size=shape) for shape in shapes]    \n",
    "        weights_tensor_params = torch.tensor(weights, requires_grad=True)\n",
    "\n",
    "        probs = qc(weights_tensor_params, measurement_qubits=measure_qubits)\n",
    "\n",
    "    pi = policy(probs, policy_type=policy_type, n_actions=n_actions, n_qubits=n_qubits)\n",
    "    if clamp is not None:\n",
    "        pi = torch.clamp(pi, clamp, 1)\n",
    "\n",
    "    dist = torch.distributions.Categorical(probs=pi)\n",
    "    \n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    gradient_no_clamp = np.linalg.norm(compute_gradient(log_prob, weights_tensor_params), 2)\n",
    "    return gradient_no_clamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceUpdate():\n",
    "\n",
    "    def __init__(self, pqc, optimizer, env, n_episodes, max_t, gamma=1.0, print_every=1, verbose=1):\n",
    "        \n",
    "        self.pqc = pqc\n",
    "        self.optimizer = optimizer\n",
    "        self.env = env\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.scores_deque = deque(maxlen=10)\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.scores_list = []\n",
    "        self.runtime_list = []\n",
    "        self.gradient_list = []\n",
    "        self.loss_list = []\n",
    "        self.parameters_list = []\n",
    "        self.steps_per_ep_list = []\n",
    "        \n",
    "    def get_trajectory(self):\n",
    "        \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(self.max_t):\n",
    "            # Sample the action from current policy\n",
    "            if t==0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = self.pqc.sample(state_tensor)\n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            self.rewards.append(reward)\n",
    "            if done:\n",
    "                self.steps_per_ep_list.append(t)\n",
    "                break\n",
    "\n",
    "        self.scores_deque.append(sum(self.rewards))\n",
    "        self.scores_list.append(sum(self.rewards))\n",
    "\n",
    "    def compute_loss(self):\n",
    "\n",
    "        R=0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0,R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1)\n",
    "\n",
    "        for log_prob, R in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        self.loss = torch.cat(policy_unsqueezed).sum()\n",
    "        self.loss_list.append(self.loss)\n",
    "\n",
    "    def compute_gradients(self):\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()    \n",
    "        self.optimizer.step()\n",
    "        self.gradient_list.append(self.pqc.get_gradients())\n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        for i in range(1,self.n_episodes):\n",
    "            start_time = time.time()\n",
    "            self.get_trajectory()\n",
    "            self.compute_loss()\n",
    "            self.compute_gradients()\n",
    "            end_time = time.time()\n",
    "            runtime = end_time-start_time\n",
    "            self.runtime_list.append(runtime)\n",
    "            self.parameters_list.append(self.pqc.get_parameters())\n",
    "            if np.mean(self.scores_deque) == 500:\n",
    "                print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i,np.mean(self.scores_deque)))\n",
    "                break\n",
    "            elif i % self.print_every == 0 and self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tLast 10 Episodes average reward: {:.2f}\\tRuntime: {:.2f}\\t'.format(i, self.scores_deque[-1], np.mean(self.scores_deque), runtime))\n",
    "\n",
    "    def save_data(self,file_path,file_name):\n",
    "        \n",
    "        file_path = os.path.join(file_path, file_name)\n",
    "        \n",
    "        data = {\n",
    "            \"scores_list\": self.scores_list,\n",
    "            \"runtime_list\": self.runtime_list,\n",
    "            \"gradient_list\": self.gradient_list,\n",
    "            \"loss_list\": self.loss_list,\n",
    "            \"parameters_list\": self.parameters_list,\n",
    "            \"steps_per_ep_list\": self.steps_per_ep_list\n",
    "        }\n",
    "        \n",
    "        # Save data to pickle file\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "\n",
    "def create_zz_operator(measure_qubits):\n",
    "    ZZ = qml.PauliZ(0)\n",
    "    for i in range(1, measure_qubits):\n",
    "        ZZ = qml.operation.Tensor(ZZ, qml.PauliZ(i))\n",
    "    return ZZ\n",
    "\n",
    "def measure_selection(measure_type, observables, measure_qubits):\n",
    "    if measure_type == 'probs':\n",
    "        if observables is None:\n",
    "            return qml.probs(wires=range(measure_qubits))\n",
    "        else:\n",
    "            return qml.probs(op=observables, wires=range(measure_qubits))\n",
    "        \n",
    "    elif measure_type == 'expval':\n",
    "        op = observables if observables is not None else create_zz_operator(measure_qubits)\n",
    "        return qml.expval(op=op) \n",
    "    \n",
    "def create_optimizer_with_lr(params, lr_list):\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': params, 'lr': lr} for params, lr in zip(params, lr_list)\n",
    "    ])\n",
    "    return optimizer\n",
    "\n",
    "def jerbi_circuit(n_qubits, n_layers, shots, input_scaling, data_reuploading, diff_method, weight_init, input_init, measure_type, observables, measure_qubits):\n",
    "\n",
    "    dev = qml.device(\"default.qubit\", wires=n_qubits, shots=shots)\n",
    "\n",
    "    observables = observables if observables is not None else None\n",
    "    weight_shapes = {\"params\": (n_layers + 1, n_qubits, 2),\n",
    "                    \"input_params\": (n_layers + 1, n_qubits, 2)}\n",
    "    init_method   = {\"params\": weight_init,\n",
    "                    \"input_params\": input_init}\n",
    "\n",
    "    @qml.qnode(dev, interface='torch', diff_method=diff_method)\n",
    "    def qnode(inputs, params, input_params):\n",
    "\n",
    "    #in case n_qubits != input length\n",
    "        if n_qubits > len(inputs) and n_qubits % len(inputs) == 0:\n",
    "            multiplier = n_qubits // len(inputs)\n",
    "            inputs = torch.cat([inputs] * multiplier)\n",
    "        elif n_qubits > len(inputs) and n_qubits % len(inputs) != 0:\n",
    "            return 'Number of qubits cannot be divided by input lenght'\n",
    "        \n",
    "    #hadamard\n",
    "        qml.broadcast(qml.Hadamard, wires=range(n_qubits), pattern=\"single\")\n",
    "        \n",
    "    #in case data_reuploading is True\n",
    "        if data_reuploading:\n",
    "        #iterate for each layer\n",
    "            for layer in range(n_layers):\n",
    "                for wire in range(n_qubits):\n",
    "                    qml.RZ(params[layer][wire][0], wires=wire)\n",
    "                    qml.RY(params[layer][wire][1], wires=wire)\n",
    "\n",
    "                qml.broadcast(qml.CNOT, wires=range(n_qubits), pattern=\"chain\")\n",
    "\n",
    "            #in case input_scaling is True\n",
    "                if input_scaling:\n",
    "                    for wire in range(n_qubits):\n",
    "                        qml.RY(input_params[layer][wire][0] * inputs[wire], wires=wire)\n",
    "                        qml.RZ(input_params[layer][wire][1] * inputs[wire], wires=wire)\n",
    "            #in case input_scaling is False\n",
    "                else:\n",
    "                    for wire in range(n_qubits):\n",
    "                        qml.RY(inputs[wire], wires=wire)\n",
    "                        qml.RZ(inputs[wire], wires=wire)\n",
    "\n",
    "        #last parameterized block\n",
    "            for wire in range(n_qubits):\n",
    "                qml.RZ(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "\n",
    "    #in case data_reuploading is False  \n",
    "        else:\n",
    "            for wire in range(n_qubits):\n",
    "                qml.RZ(params[-1][wire][0], wires=wire)\n",
    "                qml.RY(params[-1][wire][1], wires=wire)\n",
    "\n",
    "        #in case input_scaling is True\n",
    "            if input_scaling:\n",
    "                for wire in range(n_qubits):\n",
    "                    qml.RZ(input_params[-1][wire][0]* inputs[wire], wires=wire)\n",
    "                    qml.RY(input_params[-1][wire][1]* inputs[wire], wires=wire)\n",
    "\n",
    "        #in case input_scaling is False            \n",
    "            else:\n",
    "                for wire in range(n_qubits):\n",
    "                    qml.RZ(input_params[-1][wire][0], wires=wire)\n",
    "                    qml.RY(input_params[-1][wire][1], wires=wire)\n",
    "        \n",
    "        return measure_selection(measure_type, observables, measure_qubits)\n",
    "\n",
    "    model = qml.qnn.TorchLayer(qnode, weight_shapes=weight_shapes, init_method=init_method)\n",
    "\n",
    "    return model\n",
    "\n",
    "def S2D(n_qubits, n_layers, shots, input_scaling, diff_method, weight_init, input_init, measure_type, observables, measure_qubits):\n",
    "\n",
    "    dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "    observables = observables if observables is not None else None\n",
    "    \n",
    "    shapes = qml.SimplifiedTwoDesign.shape(n_layers=n_layers, n_wires=n_qubits)\n",
    "\n",
    "    weight_shapes = {\"params\": shapes[1],\n",
    "                     \"input_params\": shapes[0]}\n",
    "    \n",
    "    init_method   = {\"params\": weight_init,\n",
    "                     \"input_params\": input_init}\n",
    "\n",
    "    @qml.qnode(dev, interface='torch', diff_method='parameter-shift')\n",
    "    def qnode(inputs, params, input_params):\n",
    "\n",
    "        return measure_selection(measure_type, observables, measure_qubits)\n",
    "\n",
    "    model = qml.qnn.TorchLayer(qnode, weight_shapes=weight_shapes, init_method=init_method)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, n_qubits, n_layers,  shots= None, input_scaling=True, data_reuploading=True, design='jerbi_circuit', diff_method = 'backprop', weight_init=torch.nn.init.uniform_, input_init = torch.nn.init.ones_, measure_type = 'probs', observables = None, measure_qubits = None):\n",
    "        super(CircuitGenerator, self).__init__()\n",
    "        self.n_qubits = n_qubits                        #number of qubits\n",
    "        self.n_layers = n_layers                        #number of layers\n",
    "        self.shots = shots                              #number of shots\n",
    "        self.input_scaling = input_scaling              #input scaling - True or False\n",
    "        self.data_reuploading = data_reuploading        #data reuploading - True or False\n",
    "        self.design = design                            #circuit design\n",
    "        self.diff_method = diff_method                  #differentiator method \n",
    "        self.measure_type = measure_type                #measure type - 'probs' or 'expval'\n",
    "        self.observables = observables                  #observables\n",
    "        self.weight_init = weight_init                  #weight initialization - torch.nn.init\n",
    "        self.input_init = input_init                    #input weight initialization - torch.nn.init.\n",
    "        if measure_qubits is None:\n",
    "            self.measure_qubits = n_qubits\n",
    "        else:\n",
    "            self.measure_qubits = measure_qubits\n",
    "\n",
    "        if self.design == 'jerbi_circuit':\n",
    "            self.circuit = jerbi_circuit(n_qubits = self.n_qubits,\n",
    "                                        n_layers = self.n_layers,\n",
    "                                        shots = self.shots,\n",
    "                                        input_scaling = self.input_scaling,\n",
    "                                        data_reuploading = self.data_reuploading,\n",
    "                                        diff_method = self.diff_method,\n",
    "                                        weight_init = self.weight_init,\n",
    "                                        input_init = self.input_init,\n",
    "                                        measure_type = self.measure_type,\n",
    "                                        observables = self.observables,\n",
    "                                        measure_qubits = self.measure_qubits)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported circuit type\")\n",
    "\n",
    "    def input(self,inputs):\n",
    "        outputs = self.circuit(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyType(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_qubits, n_actions, post_processing = 'raw_contiguous'):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_qubits = n_qubits\n",
    "        self.post_processing = post_processing\n",
    "\n",
    "\n",
    "    def input(self,probs):\n",
    "\n",
    "        if self.post_processing == 'raw_contiguous':\n",
    "            policy = self.raw_contiguous(probs)\n",
    "        elif self.post_processing == 'raw_parity':\n",
    "            policy = self.raw_parity(probs)\n",
    "        elif self.post_processing == 'softmax':\n",
    "            policy = self.softmax(probs)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid post-processing method specified.\")\n",
    "\n",
    "        return policy\n",
    "\n",
    "\n",
    "    def raw_contiguous(self,probs):\n",
    "\n",
    "        probs_flatten = probs.flatten()\n",
    "        chunk_size = len(probs_flatten) // self.n_actions\n",
    "        remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "        policy = []\n",
    "\n",
    "        for i in range(self.n_actions):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size\n",
    "\n",
    "            if i < remainder:\n",
    "                end += 1\n",
    "\n",
    "            # Update the original policy list instead of creating a new one\n",
    "            policy.append(sum(probs_flatten[start:end]))\n",
    "\n",
    "        policy_tensor = torch.stack(policy)\n",
    "\n",
    "        return policy_tensor\n",
    "    \n",
    "\n",
    "    def raw_parity(self,probs):\n",
    "\n",
    "        if self.n_actions % 2 != 0:\n",
    "            return('For parity-like policy, n_actions must be an even number')\n",
    "        \n",
    "        probs_flatten = probs.flatten()\n",
    "        policy = torch.zeros(self.n_actions)\n",
    "        counter = 0\n",
    "        for prob in probs_flatten:\n",
    "            policy[counter] += prob\n",
    "            counter += 1\n",
    "            if counter == self.n_actions:\n",
    "                counter = 0\n",
    "        \n",
    "        policy_tensor = torch.stack(policy)\n",
    "\n",
    "        return policy_tensor\n",
    "    \n",
    "\n",
    "    def softmax(self, probs, beta=1):\n",
    "        if len(probs) == self.n_actions:\n",
    "            scaled_output = probs * beta\n",
    "            softmax_output = F.softmax(scaled_output, dim=0)\n",
    "            return softmax_output\n",
    "\n",
    "        else:\n",
    "            probs_flatten = probs.flatten()\n",
    "            chunk_size = len(probs_flatten) // self.n_actions\n",
    "            remainder = len(probs_flatten) % self.n_actions\n",
    "\n",
    "            policy = [0] * self.n_actions\n",
    "\n",
    "            for i in range(self.n_actions):\n",
    "                start = i * chunk_size\n",
    "                end = (i + 1) * chunk_size\n",
    "\n",
    "                if i < remainder:\n",
    "                    end += 1\n",
    "\n",
    "                policy[i] = sum(probs_flatten[start:end])\n",
    "\n",
    "            policy_tensor = torch.tensor(policy, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "            scaled_output = policy_tensor / beta\n",
    "            softmax_output = F.softmax(scaled_output, dim=0)\n",
    "            return softmax_output.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumPolicyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, circuit, policy):\n",
    "        super(QuantumPolicyModel, self).__init__()\n",
    "        self.circuit = circuit\n",
    "        self.policy = policy\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        probs = self.circuit.input(inputs)\n",
    "        probs_processed = self.policy.input(probs)\n",
    "        return probs_processed\n",
    "    \n",
    "    def sample(self, inputs):\n",
    "\n",
    "        policy = self.forward(inputs)\n",
    "        dist = torch.distributions.Categorical(policy)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), policy\n",
    "    \n",
    "    def get_gradients(self):\n",
    "\n",
    "        gradients = []\n",
    "        for name, param in self.circuit.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                gradients.append(param.grad.clone().detach())  # Clone and detach gradient to avoid modifying computation graph\n",
    "            else:\n",
    "                gradients.append(torch.zeros_like(param))  # Append zero tensor if gradient is None\n",
    "        return gradients\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        parameter_values = []\n",
    "        for param in self.circuit.parameters():\n",
    "            parameter_values.append(param.clone().detach().numpy())\n",
    "        return parameter_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceUpdate():\n",
    "\n",
    "    def __init__(self, pqc, optimizer, env, n_episodes, max_t, gamma, print_every, verbose, file_path, file_name):\n",
    "        \n",
    "        self.pqc = pqc\n",
    "        self.optimizer = optimizer\n",
    "        self.env = env\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_t = max_t\n",
    "        self.gamma = gamma\n",
    "        self.scores_deque = deque(maxlen=10)\n",
    "        self.print_every = print_every\n",
    "        self.verbose = verbose\n",
    "        self.folder_path = file_path\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def get_trajectory(self):\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        state = self.env.reset()\n",
    "        for t in range(self.max_t):\n",
    "            # Sample the action from current policy\n",
    "            if t == 0:\n",
    "                state_tensor = torch.tensor(state[0]).float()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state).float()\n",
    "            action, log_prob, _, = self.pqc.sample(state_tensor)\n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            self.rewards.append(reward)\n",
    "            if done:\n",
    "                self.scores_deque.append(sum(self.rewards))\n",
    "                break\n",
    "\n",
    "    def compute_loss(self):\n",
    "\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1)\n",
    "\n",
    "        for log_prob, R in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        policy_unsqueezed = [torch.unsqueeze(loss, 0) for loss in policy_loss]\n",
    "        self.loss = torch.cat(policy_unsqueezed).sum()\n",
    "\n",
    "    def compute_gradients(self):\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()    \n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def save_episode_data(self):\n",
    "        if self.folder_path is not None:\n",
    "            file_path = os.path.join(self.folder_path, f\"{self.file_name}.json\")\n",
    "            episode_data = {\n",
    "                \"episode_reward\": self.scores_deque[-1],\n",
    "                \"episode_length\": len(self.rewards),\n",
    "                \"runtime\": self.runtime,\n",
    "                \"loss\": self.loss.item(),\n",
    "                \"gradient\": self.tensor_to_list(self.pqc.get_gradients()),\n",
    "                \"parameters\": self.tensor_to_list(self.pqc.get_parameters())\n",
    "            }\n",
    "\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, 'r') as f:\n",
    "                    existing_data = json.load(f)\n",
    "                existing_data.append(episode_data)\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump(existing_data, f, indent=4)\n",
    "            else:\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump([episode_data], f, indent=4)\n",
    "\n",
    "    def tensor_to_list(self, tensor):\n",
    "        \"\"\"\n",
    "        Convert a tensor or numpy array to a nested list.\n",
    "        \"\"\"\n",
    "        if isinstance(tensor, list):\n",
    "            return [self.tensor_to_list(t) for t in tensor]\n",
    "        elif isinstance(tensor, dict):\n",
    "            return {key: self.tensor_to_list(value) for key, value in tensor.items()}\n",
    "        elif isinstance(tensor, np.ndarray):\n",
    "            return tensor.tolist()  # Convert numpy array to list\n",
    "        elif isinstance(tensor, torch.Tensor):\n",
    "            return tensor.tolist()  # Convert torch tensor to list\n",
    "        else:\n",
    "            return tensor\n",
    "                    \n",
    "    def train(self):\n",
    "        \n",
    "        for i in range(1, self.n_episodes):\n",
    "            start_time = time.time()\n",
    "            self.get_trajectory()\n",
    "            self.compute_loss()\n",
    "            self.compute_gradients()\n",
    "            end_time = time.time()\n",
    "            self.runtime = end_time - start_time\n",
    "            if np.mean(self.scores_deque) == 500:\n",
    "                print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i, np.mean(self.scores_deque)))\n",
    "                break\n",
    "            elif i % self.print_every == 0 and self.verbose == 1:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tLast 10 Episodes average reward: {:.2f}\\tRuntime: {:.2f}\\t'.format(i, self.scores_deque[-1], np.mean(self.scores_deque), self.runtime))\n",
    "\n",
    "            self.save_episode_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analysis():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "def train_agent(agent_id, env_name, pqc, optimizers, env, n_episodes, max_t, gamma, print_every, verbose):\n",
    "    folder_name = f\"{n_layers}_{n_qubits}_{env_name}\"\n",
    "    folder_path = os.path.join(current_directory, folder_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    file_name = f\"{n_layers}_{n_qubits}_{env_name}_{agent_id}\"\n",
    "    \n",
    "    reinforce_update = ReinforceUpdate(pqc, optimizers, env, n_episodes, max_t, gamma, print_every, verbose, folder_path, file_name)\n",
    "    reinforce_update.train()\n",
    "\n",
    "    return f\"Agent {agent_id}: Training completed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_qubits = 4\n",
    "    n_layers = 4        #set to 0 if data_reuploading = False\n",
    "    n_actions = 2\n",
    "\n",
    "    circuit = CircuitGenerator(n_qubits, n_layers)\n",
    "    policy_type = PolicyType(n_qubits, n_actions)\n",
    "    pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "    lr_list= [0.01,0.1]\n",
    "    params= circuit.parameters()\n",
    "\n",
    "    pqc = pqc\n",
    "    optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    n_episodes = 500\n",
    "    max_t = 500\n",
    "    gamma = 1\n",
    "    print_every = 10\n",
    "    verbose = 1\n",
    "    n_qubits = n_qubits\n",
    "    n_layers = n_layers\n",
    "    \n",
    "    num_agents = 10\n",
    "\n",
    "    results = Parallel(n_jobs=num_agents)(delayed(train_agent)(i, env_name, pqc, optimizers, env, n_episodes, max_t, gamma, print_every, verbose,) for i in range(num_agents))\n",
    "\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'path/to/json/files'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     34\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath/to/json/files\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 35\u001b[0m all_data \u001b[38;5;241m=\u001b[39m load_episode_data(directory)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m all_data:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data)\n",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m, in \u001b[0;36mload_episode_data\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mLoad episode data from JSON files in a directory.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m- all_data (list): A list containing data from all JSON files in the directory.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m all_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(directory):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     28\u001b[0m         json_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, filename)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'path/to/json/files'"
     ]
    }
   ],
   "source": [
    "def load_json_data(file_path):\n",
    "    \"\"\"\n",
    "    Load JSON data from a file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    - data: The loaded JSON data.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def load_episode_data(directory):\n",
    "    \"\"\"\n",
    "    Load episode data from JSON files in a directory.\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): The directory containing the JSON files.\n",
    "\n",
    "    Returns:\n",
    "    - all_data (list): A list containing data from all JSON files in the directory.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            json_file_path = os.path.join(directory, filename)\n",
    "            data = load_json_data(json_file_path)\n",
    "            all_data.append(data)\n",
    "    return all_data\n",
    "\n",
    "# Example usage:\n",
    "directory = \"path/to/json/files\"\n",
    "all_data = load_episode_data(directory)\n",
    "for data in all_data:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 4        #set to 0 if data_reuploading = False\n",
    "n_actions = 2\n",
    "\n",
    "circuit = CircuitGenerator(n_qubits, n_layers)\n",
    "policy_type = PolicyType(n_qubits, n_actions)\n",
    "pqc = QuantumPolicyModel(circuit,policy_type)\n",
    "\n",
    "lr_list= [0.01,0.1]\n",
    "params= circuit.parameters() \n",
    "optimizers= create_optimizer_with_lr(params,lr_list)\n",
    "env = gym.make('CartPole-v1')\n",
    "n_episodes = 1000\n",
    "max_t = 500\n",
    "gamma = 1\n",
    "print_every = 10\n",
    "verbose = 1\n",
    "\n",
    "reinforce_update = ReinforceUpdate(pqc, optimizers, env, n_episodes, max_t, gamma, print_every, verbose)\n",
    "reinforce_update.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.title('Score per episode')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(average_scores)+1), average_scores)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Average score of the previous 10 episodes')\n",
    "plt.xlabel('Average of previous 10 #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1_gradients = gradient_list[::2]\n",
    "param2_gradients = gradient_list[1::2]\n",
    "\n",
    "# Plotting gradients for parameters 1\n",
    "plt.plot(param1_gradients, label='Parameters')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Gradient Mean')\n",
    "plt.title('Gradients of Parameters')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting gradients for parameters 2\n",
    "plt.plot(param2_gradients, label='Parameters')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Gradient Mean')\n",
    "plt.title('Gradients of Input Parameters')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_runtimes = np.cumsum([runtime / 60 for runtime in runtimes])\n",
    "\n",
    "plt.plot(cumulative_runtimes)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Cumulative Runtime (Minutes)')\n",
    "plt.title('Cumulative Runtimes')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import os\n",
    "import inspect\n",
    "import gym\n",
    "import glob\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "from IPython.display import HTML, display\n",
    "import torch\n",
    "\n",
    "def show_video_of_model(policy, env_name):\n",
    "    current_dir = os.path.abspath('')\n",
    "    video_dir = os.path.join(current_dir, 'video')\n",
    "    os.makedirs(video_dir, exist_ok=True)  # Ensure video directory exists\n",
    "\n",
    "    # Create the Gym environment with render mode\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    video_path = os.path.join(video_dir, '{}.mp4'.format(env_name))\n",
    "    vid = video_recorder.VideoRecorder(env, path=video_path)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    for t in range(1000):\n",
    "        # Convert state to tensor if needed\n",
    "        state_tensor = torch.tensor(state[0]).float() if t == 0 else torch.tensor(state).float()\n",
    "        \n",
    "        # Capture frame\n",
    "        vid.capture_frame()\n",
    "        \n",
    "        # Sample action from policy\n",
    "        action, log_prob, _, = policy.sample(state_tensor)\n",
    "        \n",
    "        # Take action in the environment\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Close video recorder and environment\n",
    "    vid.close()\n",
    "    env.close()\n",
    "    \n",
    "    print(\"Video saved at:\", video_path)\n",
    "\n",
    "def show_video(env_name):\n",
    "    current_dir = os.path.abspath('')\n",
    "    video_dir = os.path.join(current_dir, 'video')\n",
    "    mp4list = glob.glob(os.path.join(video_dir, '*.mp4'))\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = os.path.join(video_dir, '{}.mp4'.format(env_name))\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "\n",
    "# Example usage\n",
    "# show_video_of_model(pqc, 'CartPole-v1')\n",
    "# show_video('CartPole-v1')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
